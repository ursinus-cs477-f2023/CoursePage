<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Week 6: Hidden Markov Models / Bayes Filtering / The Viterbi Algorithm</h2>
                                        <h3><a href = "http://www.ctralie.com">Chris Tralie</a></h3>
									</header>

									<div id="page-content">

									<h2>Markov Chains As Bayesian Networks</h2>

									<p>
										We've already seen <a href = "../Week4_Markov/">Markov chains</a>, where the probability of transitioning from one state to the next is completely independent of the previous states.  This is known as the <b>Markov state assumption</b>.  If we let the variable <code>X<sub>i</sub></code> represent the state that we're in at step <code>i</code>, then we can depict this as the following state diagram: 
									</p>

									<img src = "MarkovStateDiagram.svg" width=500>

									<p>
										This is a specific instance of a more general state diagram known as a "<a href = "https://towardsdatascience.com/introduction-to-bayesian-networks-81031eeed94e">Bayesian Network</a>."  The arrows indicate conditional dependencies.  What this diagram is saying specifically is that the "joint probability" (AND) of all states in a Markov sequence can be written as follows:
									</p>

									<h3>
										\[ p(x_1, x_2, ..., x_n) = p(x_1) \times p(x_2 | x_1) \times p(x_3 | x_2) \times ... \times p(x_n | x_{n-1})\]
									</h3>

									<p>
										Notice how we only condition on the states that are at the tail of arrows, which represents the Markov assumption graphically.
									</p>

									<h2><a name = "hmss">Hidden Markov Models</a></h2>
									<p>
										Markov models are applicable to time sequence data that we can observe directly, like the text strings in <a href = "../../Assignments/HW3_Markov/">assignment 3</a>.  However, there are plenty of time sequence scenarios where we can't observe what we actually want to know directly.  For example, in a <a href = "https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf">speech to text system</a>, we observe noisy audio data over time, but we actually want to know the text string that someone is saying.  Below is a table of some more examples of time sequences that have both a <b>hidden state</b> and an <b>observation</b>
									</p>

									<table style="width:800px;">
										<tr><td><h4>Problem</h4></td><td><h4>Hidden State</h4></td><td><h4>Observation</h4></td></tr>
										<tr>
											<td><a href = "https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf">Speech To Text</a></td>
											<td>Text String</td>
											<td>Noisy audio samples</td>
										</tr>
										<tr>
											<td><a href = "https://aclanthology.org/J90-2002.pdf">Machine translation</a> <p>(e.g. German to English)</p></td>
											<td>Target Language (e.g. English)</td>
											<td>Source Language (e.g. German)</td>
										</tr>
										<tr>
											<td>Handwriting Recognition</td>
											<td>Text string</td>
											<td>Images of characters</td>
										</tr>
										<tr>
											<td><a href = "https://ismir2006.ismir.net/PAPERS/ISMIR06152_Paper.pdf">Transcribing sheet music</a></td>
											<td>Symbolic representation of a musical score </td>
											<td>Images of sheet music (e.g. a <a href = "https://musiclab.chromeexperiments.com/Piano-Roll/">piano roll</a></td>
										</tr>
										<tr>
											<td>
												<a href = "https://ursinus-cs472a-s2021.github.io/CoursePage/Assignments/HW4_RhythmAnalysis/#beattrack">Audio beat tracking</a>
											</td>
											<td>
												Time series indicating when to tap the foot
											</td>
											<td>
												Audio data of music
											</td>
										</tr>
										<tr>
											<td>
												<a href = "robot.html">Robot Localization</a>
											</td>
											<td>
												Position of robot
											</td>
											<td>
												Sensor scan of environment (e.g. video stream, laser range scanner)
											</td>
										</tr>
									</table>

									<p>
										We can model problems with a hidden state and an observation using a <b>Hidden Markov Model (HMM)</b>.  Now, in addition to our sequence <b>x<SUB>1</SUB>, x<SUB>2</SUB>, ..., x<SUB>n</SUB></b>, which is what we want to know, we also have a sequence <b>y<SUB>1</SUB>, y<SUB>2</SUB>, ..., y<SUB>n</SUB></b>, in which we observe something at each state.  The Bayes net for a hidden markov model looks like this:
									</p>

									<img src = "HiddenMarkovStateDiagram.svg" width=600>
									
									<p>
										In addition to the Markov state assumption, we also have the <b>Markov observation assumption</b>; that is, each observation <b>y<SUB>i</SUB></b> only depends on the state <b>x<SUB>i</SUB></b> and is independent of all other states and all other observations.
									</p>

									<h2><a name = "bayesfilter">Bayes Filtering</a></h2>
									

									<p>
										Now that we have the HMM framework, let's start trying to do some inference on it given some measurements over time.  The first problem we'll solve is known as <b>filtering</b>, and its goal is to estimate over time a probability distribution over all of the hidden states.  As more measurements come in, we update the distribution.  We can estimate where we think we are as the state which has the maximum probability given the measurements up to a particular point in time.  Because we update as things are streaming in, this is referred to as an <b>online method</b>.
									</p>

									<p>
										Before we dive into a formal specification of this process, let's sketch out an example for intuition.  Let's suppose we have a robot on a grid as in <a href = "../../Assignments/HW4_RobotLocalization/">assignment 4</a> that can move left/right/up/down or stay still at any time.  Let's also suppose that we know with perfect certainty that it starts at the center of a square map.  Below is how we would forecast the probability of being in future positions on that map if we knew nothing else, assuming that we're equally likely to stay still or move left/right/up/down:
									</p>

									<img src = "BayesMarkovOnly.gif">



									<p>
										Since we're equally likely to stay still or move left/right/up/down, we have to hedge our bets in all directions at each step.  The result is that the once very certain probability "smears out" and distributes in all directions.  This is pure Markov, and is referred to as the <b>state transition step</b> for the hidden states.  However, we have also have measurements at each timestep that help us to narrow down the possible states.  Let's suppose we receive noisy measurements that indicate that the robot is moving to the right.  Then we can do a two step process at each timestep where we diffuse the probability in the state transitions, but then we update our forecast based on measurement.  Here's how that might look in this example:
									</p>

									<img src = "BayesFullFilter.gif">

									<p>
										Notice how the probability density still diffuses at each transition, but the measurement helps it to sharpen around the true location.  This is the power of Bayes filtering in the HMM framework: it takes into consideration both <b>transitions</b> of the hidden states, as well as <b>measurement probabilities</b> to help narrow down the states.
									</p>

									<p>
										Now let's put on some formal notation and define exactly what it is we're looking for.  Given the first <b>n</b> measurements, we want to figure out the probability that the hidden state <b>x<SUB>n</SUB></b> is a particular state <b>k</b> at time <b>n</b>.  We'll refer to this as <b>b<SUB>n</SUB>(k)</b>, or the <b>belief</b> that <b>x<SUB>n</SUB> = k</b>
									</p>

									<h3>
										\[ b_n(k) =  p(x_n = k | y_1, y_2, ..., y_n) \]
									</h3>

									<p>
										Let's consider what happens when we apply Bayes' rule within the space of intersection of all measurements that have happened before this step only: <b>y<SUB>1</SUB>, y<SUB>2</SUB>, ..., y<SUB>n-1</SUB></b>.  By Bayes' rule, we can rewrite our expression for the belief as 
									</p>

									<h3>
										\[ b_n(k) = p(\boldsymbol{x_n = k | y_n}, y_1, y_2, ..., y_{n-1}) = \frac{p(\boldsymbol{y_n | x_n = k}, y_1, y_2, ..., y_{n-1}) p(\boldsymbol{x_n = k |} y_1, y_2, ..., y_{n-1})}{p(y_1, y_2, ..., y_n)} \]
									</h3>

									<p>
										By the <b>Markov measurement assumption</b>, measurement <b>y<SUB>n</SUB></b> only depends on state <b>x<SUB>n</SUB></b> and is independent of all other measurements, so we can simplify this to 
									</p>

									<h3>
										\[ b_n(k) = \frac{p(y_n | x_n = k) p(x_n = k | y_1, y_2, ..., y_{n-1})}{p(y_1, y_2, ..., y_n)} \]
									</h3>

									<p>
										Now we have to tackle the second term in the numerator, <b>p(x<SUB>n</SUB> | y<SUB>1</SUB>, y<SUB>2</SUB>, ..., y<SUB>n-1</SUB>)</b>.  Actually, if we had solved something similar for the belief at the last measurement and kept track of it:
									</p>

									<h3>
										\[ b_{n-1}(j) = p(x_{n-1} = j | y_1, y_2, ..., y_{n-1}) \]
									</h3>

									<p>
										then we can make some headway here by coming up with an expression in terms of this.  The key insight is that in the absence of a new measurement, <b>p(x<SUB>n</SUB> | y<SUB>1</SUB>, y<SUB>2</SUB>, ..., y<SUB>n-1</SUB>)</b> is simply like transitioning from <b>x<SUB>n-1</SUB></b> to <b>x<SUB>n</SUB></b> by following an ordinary Markov state transition, since, by the <b>Markov state property</b>, the state <b>x<SUB>n</SUB></b> only depends on <b>x<SUB>n-1</SUB></b>.  So we'll consider what happens of we transitioned from every possible state at <b>x<SUB>n-1</SUB></b> to <b>x<SUB>n</SUB></b>.  The picture below shows this
									</p>

									<img src = "jtok.svg" width=200>
									<p></p>
									<p>
										Since all of the possible <b>j</b> values at state <b>x<SUB>n-1</SUB></b> are disjoint, we can sum up the probabilities over all <b>j</b>.  Let's suppose we have <b>K</b> unique states.  Then the expression is as follows
									</p>

									<h3>
										\[ p(x_n = k | y_1, y_2, ..., y_{n-1}) = \sum_{j=1}^K  b_{n-1}(j) p(x_n = k | x_{n-1} = j)\]
									</h3>

									<p>
										Plugging this back into our original expression for the belief <b>b<SUB>n</SUB>(k)</b>, we have
									</p>

									<h3>
										\[ b_n(k) = \frac{p(y_n | x_n = k) \left( \sum_{j=1}^K  b_{n-1}(j) p(x_n = k | x_{n-1} = j) \right)}{p(y_1, y_2, ..., y_n)}  \]
									</h3>

									<p>
										This is very neat, because now we almost have a recursive expression for <b>b<SUB>n</SUB></b> in terms of <b>b<SUB>n-1</SUB></b>.  The only thing that remains is the denominator.  Actually, because <b>x<SUB>n</SUB> = k</b> is disjoint over all <b>k</b>, we can write the denominator as the sum of all possible numerators.
									</p>

									<h3>
										\[ b_n(k) = \frac{p(y_n | x_n = k) \left( \sum_{j=1}^K  b_{n-1}(j) p(x_n = k | x_{n-1} = j) \right)}{\sum_{k = 1}^K p(y_n | x_n = k) \left( \sum_{j=1}^K  b_{n-1}(j) p(x_n = k | x_{n-1} = j) \right)}  \]
									</h3>

									<p>
										This is analogous to how we <a href = "../Week5_NaiveBayes/index.html#covid">computed the overall probability of testing positive for covid</a> as 
									</p>

									<h3>
										\[ p(\text{positive}) = p(\text{positive} | \text{covid}) + p(\text{positive} | \text{not covid}) \]
									</h3>

									<p>
										So now, we have a complete expression for <b>b<SUB>n</SUB></b> in terms of known <b>measurement probabilities</b> <b>p(y<SUB>n</SUB> | x<SUB>n</SUB>)</b> and <b>transition probabilities</b> <b>p(x<SUB>n</SUB> = k | x<SUB>n-1</SUB> = j)</b>, which we assume we know.  This means we can continue to update <b>a single array of beliefs over all states as we get a new measurement</b>.  The complexity of such an update is <b>O(K)</b> space and time, since there are <b>K</b> states.  The pseudocode below summarizes the algorithm
									</p>

									<div style="padding:15px; background-color: blanchedalmond;">
										<h3><a name = "bayespseudocode">Online Bayes Filtering</a></h3>
										Given <b>K</b> states and <b>T</b> measurements, do the following steps
										<ol>
											<li>
												Initialize an array <b>b</b> of length <b>K</b>
											</li>
											<li>
												Let <b>b[k]</b> be the prior probability of starting in state <b>k</b>
											</li>
											<li>
												for each new measurement <b>n = 1</b> to <b>T</b>
												<ol>
													<li>Initialize an array <b>g</b> of length <b>K</b></li>
													<li>For each state <b>k = 1</b> to <b>K</b>
														We need to compute the probability that any sequence of states ends on state <b>k</b> at time <b>n</b>.  This is the product of the measurement probability at time <b>n</b> and the probability that we transition to <b>k</b> from any of the states at time <b>n-1</b>.  We show how to compute both of these below
														<ul>
															<li>
																Let <b>meas</b> be the measurement probability at time <b>n</b> given that <b>x<SUB>n</SUB> = k</b>, or <b>p(y<SUB>n</SUB> | x<SUB>n</SUB> = k)</b>
															</li>
															<li>
																Set <b>trans = 0</b> (this will hold the Markov transition probabilities from the previous state, accumulated over all previous states)
															</li>
															<li>
																For each prior state <b>j = 1</b> to <b>K</b>
																<ul>
																	<b>trans = trans + f[j] * p(x<SUB>n</SUB> = k | x<SUB>n-1</SUB> = j)</b>
																</ul>
															</li>
															<li>
																Let <b>g[k] = meas * trans</b> (this is the "measurement correction" step that updates the probability based on the measurement)
															</li>
														</ul>
													</li>
													<li>
														Let <b>&sigma;</b> be the sum of all elements in <b>g</b>. Set <b>b[k]</b> to be <b>g[k]/&sigma;</b> 
													</li>
												</ol>
											</li>
										</ol>
									</div>

									<p></p>
									<h4><a name = "bayesfiltercomplexity">A Note on Complexity</a></h4>
									<p>
										Over all timesteps, the above algorithm uses <b>O(K)</b> memory to store <b>f</b> and the intermediate <b>g</b> array.  On the other hand, it takes <b>O(TK<SUP>2</SUP>)</b> computation time in the worst case.  This is due to the triply nested for loop; for each timestep (of which there are <b>T</b>) for each state (of which there are <b>K</b>), we have to look back at <b>K</b> states at the previous timestep.  However, if we can limit the previous states we have to look at to a small constant, then this turns into <b>O(KT)</b> complexity.  For instance, in the <a href = "../../Assignments/HW4_RobotLocalization/index.html#filtering">robot localization</a> problem, we only have to consider the states that neighbor on the map and the state itself, which is at most 5.
									</p>

									<p></p>

									<h2><a name = "viterbi">The Viterbi Algorithm: Maximizing Joint Posterior Likelihood</a></h2>
									<p>
										<b>NOTE: </b> In the discussion below, <b>argmax<SUB>j</SUB>(arr)</b> means to find the index <b>j</b> in the array <b>arr</b> that maximizes <b>arr[j]</b>.  For instance, if we were 0-indexing an array, <b>argmax<SUB>j</SUB> [5, 8, -4, 3]</b> would be 1
									</p>

									<p>
										We can also use the <a href = "../Week5_BagOfWords/#theory">Bayesian classification framework</a> to figure out the <b>joint likelihood over all of the hidden states given the observations</b>.  This will help us to avoid making mistakes in individual steps since all of the steps have to be coherent across the entire sequence.  This is the reason that speech to text works really well even when peoples' speech is noisy and garbled.  Since we have to have the entire sequence of measurements ahead of time, though, this is referred to as an <b>offline method</b>.
										
										
									</p>
									
									<p>
									Let's begin by looking at the <b>posterior likelihood</b> of the hidden states given all observations, supposing that we have <b>T</b> observations total that are zero-indexed
									</p>

									<h3>
										\[ \ell(x_0, x_1, ... x_{T-1} | y_0, y_1, ..., y_{T-1}) = p(y_0, y_1, ..., y_{T-1} | x_0, x_1, ..., x_{T-1}) p(x_0, x_1, ..., x_{T-1}) \]
									</h3>

									<p>
										Because we still have the Markov state assumption, the second term of the joint probability of the state sequence can be written as 
									</p>

									<h3>
										\[ p(x_0, x_1, ... x_{T-1} ) = p(x_0) \times p(x_1 | x_0) \times p(x_2 | x_1) \times ... \times p(x_{T-1} | x_{T-2})\]
									</h3>

									<p>
										just as before.  But now we have another multiplicative term of the observations given the states.  Because of the Markov observation assumption, we can write this second probability as
									</p>

									<h3>
										\[ p(y_0, y_1, ..., y_{T-1}) = p(y_0 | x_0) \times p(y_1 | x_1) \times ... \times ... \times p(y_{T-1} | x_{T-1})\]
									</h3>

									<p>
										Putting this all together, we get the following expression for the posterior likelihood, which includes all of the same probabilities, as well as the probabilities for observations
									</p>

									<h3>
										\[ \ell(x_0, x_1, ... x_{T-1} | y_0, y_1, ..., y_{T-1}) = \left( p(x_0) \times p(x_1 | x_0) \times p(x_2 | x_1) \times ... \times p(x_{T-1} | x_{T-2}) \right) \left( p(y_0 | x_0) \times p(y_1 | x_0) \times ... \times ... \times p(y_{T-1} | x_{T-1}) \right) \]
									</h3>

									<p>
										We can also write this in log form as a sum.  This important, <i>the probability of an individual sequence will be very small, and we want to avoid numerical underflow</i>.  Abbreviating the full state sequences as <b>x<SUB>0:T</SUB></b> and the full observation sequence as <b>y<SUB>0:T</SUB></b>, we have
									</p>

									<h3>
										\[ \log \ell(x_{0:T} | y_{0:T}) = \log(p(x_0)) + \sum_{i = 1}^{T-1} \log(p(x_i | x_{i-1})) + \sum_{i=0}^{T-1} \log(p(y_i | x_i)) \]
									</h3>

									<p>
										Finding the most likely sequence of hidden states is akin to finding the <b>x<SUB>0:T</SUB></b> that maximizes the above expression.  This is also referred to as the <b>maximum a posteriori estimate</b>.  Formally, it is written as 
									</p>

									<h3>
										\[ \text{argmax}_{x_{0:T}} \log \ell(x_{0:T} | y_{0:T}) \]
									</h3>

									<p>
										If we had a brute force approach to solving this, how many combinations of states would we have to check?  Let's suppose that we had <b>K</b> states and <b>T</b> timesteps.  Then, unfortunately, we'd have to check <b>K<SUP>T</SUP></b> different combinations of states.  For example, suppose our state space was <b>{a, b, c}</b> and we had 4 timesteps.  Then we'd have to consider the <b>3<SUP>4</SUP> = 81</b> sequences: 
									</p>
									<p>
										['aaaa', 'aaab', 'aaac', 'aaba', 'aabb', 'aabc', 'aaca', 'aacb', 'aacc', 'abaa', 'abab', 'abac', 'abba', 'abbb', 'abbc', 'abca', 'abcb', 'abcc', 'acaa', 'acab', 'acac', 'acba', 'acbb', 'acbc', 'acca', 'accb', 'accc', 'baaa', 'baab', 'baac', 'baba', 'babb', 'babc', 'baca', 'bacb', 'bacc', 'bbaa', 'bbab', 'bbac', 'bbba', 'bbbb', 'bbbc', 'bbca', 'bbcb', 'bbcc', 'bcaa', 'bcab', 'bcac', 'bcba', 'bcbb', 'bcbc', 'bcca', 'bccb', 'bccc', 'caaa', 'caab', 'caac', 'caba', 'cabb', 'cabc', 'caca', 'cacb', 'cacc', 'cbaa', 'cbab', 'cbac', 'cbba', 'cbbb', 'cbbc', 'cbca', 'cbcb', 'cbcc', 'ccaa', 'ccab', 'ccac', 'ccba', 'ccbb', 'ccbc', 'ccca', 'cccb', 'cccc']
									</p>
									<p>
										This quickly grows intractable for any reasonable problem.  For instance, if we're localizing a robot on a 100x100 grid, then <b>K = 10000</b>.  Let's suppose we're measuring at 30 frames per second for 10 seconds, so <b>T = 300</b>.  Then we'd need to check <b>10000<SUP>300</SUP></b> combinations of states.  This is <i>beyond astronomical</i> in size!  So we're going to need a smarter approach.
									</p>

									<p>
										Actually, we can do something kind of similar to Bayes filtering, except we're going to need to store a bit more information.  Let's let <b>L<SUB>j, t</SUB></b> be the maximum <b>log likelihood</b> of the joint distribution over the first <b>t</b> states that end at state <b>x<sub>t</sub> = j</b>.  We can think of <b>L</b> as holding optimal cumulative probabilities of traveling up to a certain point in time.  As with Bayes filtering, we can write <b>L<SUB>j, t</SUB></b> in terms of maximum likelihoods one step prior in time, since every step before that is independent, as well as the measurement at time <b>t</b>.  In particular, we have the following recurrence:
									</p>

									<h3>
										\[ L_{k, t} = \log(p(y_t | x_t = j) + \max_{k} \left( L_{k, t-1} + \log(p(x_t = j | x_{t-1} = k)) \right) \]
									</h3>

									<p>
										Or, in other words, accumulating the next step in the joint probability adds on a term for the measurement probability <b>log(p(y<SUB>n</SUB> | x<SUB>n</SUB> = k))</b>, as well as the transition probability <b>log(p(x<SUB>n</SUB> | x<SUB>n-1</SUB> = j))</b>.  So we pay a cost for how good or bad the transition is and how well the new state matches the next measurement we got.  Such a recursive specification lends itself well to a bottom up <b><a href = "https://www.youtube.com/watch?v=2k5v_nQTkhc">dynamic programming</a> solution</b>, where we fill in the first column of <b>L</b> and then fill in each column one at a time successively.  We'll also store a matrix <b>B</b> which remembers which state <b>j</b> maximized <b>L<SUB>k, t</SUB></b>
									</p>

									<p>
										We start with the first column of <b>L</b> holding the prior probabilities and measurement probabilities only (since no measurements have happened yet).
									</p>


									<div style="padding:15px; background-color: blanchedalmond;">
									<h3><a name = "viterbipseudocode">Offline Viterbi Optimal Sequence Extraction</a></h3>
									Given <b>K</b> states and <b>T</b> measurements, do the following steps
									<ol>
										<li>
											Initialize a <b>K x T</b> 2D array <b>L</b> and a <b>K x T</b> 2D array <b>B</b>
										</li>
										<li>
											For each state <b>j = 0</b> to <b>K-1</b>
											<ul>
										<li>											Let 
												<div style="width:200px;">
												\[ L_{j, 0} = \log(p(x_0 = j)) + \log(p(y_0 | x_0 = j)) \]
												</div>
												That is, we start off with the prior probabilities in each state, also integrating in the measurement probabilities at the first timestep.
										</li>
										<li>
											Let
											<div style="width:70px;">
												\[ B_{j, -1} = -1 \]
											</div>
											We'll use this array <b>B</b> to remember transitions from state to state
											<p></p>
										</li>
										</ul>



										</li>
										<li>
											for each <b>t = 1</b> to <b>T-1</b>
											<ul>
												<li>For each state <b>j = 0</b> to <b>K-1</b>
													<ul>
														<li>
															Let <b>maxj = -1</b>.  This will store the index of previous state which maximizes the cumulative probability plus the next transition probability
														</li>

														<li>
															Let <b>maxval = -infinity</b>.  This will store the maximum cumulative probability plus the next transition probability
														</li>
														<li>
															For each state <b>k = 0</b> to <b>K-1</b>						<ul>
																<li>
																	<div style="width:100px;">
																		\[ val = L_{k, t-1} + \log(p(x_n = j | x_{n-1} = k))  \]

								
																	</div>										<p>
																			In other words, the probability of coming from <b>j</b> at the last step to <b>k</b> in this step, without knowledge of a new measurement, is the log probability of being at <b>j</b> in the last step, plus the log transition probability from state <b>j</b> to state <b>k</b>.
																		</p>
								
												
																</li>
																<li>

																	If val > maxval:  (We just found a more probable transition than the best we've found so far)
																	<ul>
																		<li>
																			maxval = val
																		</li>
																		<li>
																			maxj = j
																		</li>
																	</ul>
																</li>
															</ul>
														</li>
														<li>
															<div style="width:100px;">
																\[ L_{j, t} =  \log(p(y_t | x_t = j)) + \text{maxval} \]
															</div>
															In other words, once we have looked over all possible transitions, we pick the one with the highest probability, and we also integrate the new measurement at this step
														</li>
														<li>
															<div style="width:100px;">
																\[ B_{j, t} =  \text{maxj} \]
															</div>
															In other words, we remember which <b>j</b> gave us the highest probability.  We'll use this later when backtracing to reconstruct a full path from start to finish.
														</li>
													</ul>
												</li>
											</ul>
										</li>
										<li>
											Now find index of the maximum element in the last column of <b>L</b> and backtrace through <b>B</b> to extract the optimal path in  (Note that this is similar to the backtracing that we used in our <a href = "../Week2_BasicSearch/">search algorithms</a> from before).

											<ul>
												<li>Let <b>state</b> = argmax<SUB>j</SUB> <b>L<SUB>j, T-1</SUB></b>.  In other words, find the state that ends up with the maximum probability at the end.  We will backtrace this now to the beginning of time.</li>
												<li>Let <b>states</b> = []</li>
												<li>Let <b>t = T-1</b></li>
												<li>while <b>t > 0</b>
													<ul>
														<li><b>states</b>.push(<b>state</b>)</li>
														<li>state = <b>B<SUB>[state, t]</SUB></b></li>
														<li><b>t = t - 1</b></li>
													</ul>
												</li>
												<li>
													<b>states</b>.reverse()
												</li>
											</ul>
										</li>
									</ol>
								</div>
								<p></p>
								<p></p>
								<h4><a name = "viterbicomplexity">A Note on Complexity</a></h4>
								<p>
									Like the <a href = "#bayesfiltercomplexity">bayes filter</a>, this algorithm takes <b>O(TK<SUP>2</SUP>)</b> computation time in the worst case, but this can be limited to <b>O(KT)</b> complexity if there are a small number of possible states that can be reached from any given state.  However, the memory requirements are worse at <b>O(KT)</b>, and there's no way around this, because we need to remember enough information to backtrace a full optimal trajectory at the end.  In practice, we could just store the <b>B</b> array and the most recently filled in column of <b>L</b>m but it's still <b>O(KT)</b> to store the entire <b>B</b> array.
								</p>
								
								
								<p></p><p></p>
								<h2>Going Further: Continuous State Spaces</h2>
								<p>
									We've covered a lot of ground through some really important algorithms here.  But these algorithms will completely fail if the state space is continuous, since there are an infinite number of possible states.  We can choose to discretize the space; for instance, if we were trying to do GPS localization, we could round every latitude and longitude to the 3rd decimal place.  But this may not be fine enough, and we will still have tons of states, grinding our computation to a halt and blowing up memory.  
								</p>
								<p>
									To deal with large or possibly infinite state space, there are two common approaches in CS.  One is to maintain a <b>continuous distribution</b> on all measurment probabilities, transition probabilties, and filtered probabilities.  If the choice of distribution is a <a href = "../Week6_GradSchoolAdmissions/">Gaussian</a>, then this leads to something known as a <a href = "https://www.kalmanfilter.net/background.html">Kalman filter</a>.  This is widely used in many localization problems (apparently my dad used to use this when working on the <a href = "https://www.kalmanfilter.net/kalman1d.html">Landsat 7 satellite</a>).  
									
								</p>
								<p>
									The problem with the Gaussian assumption, though, is that it only works for data that is "unimodal"; that is, there is only one peak in the probability distribution.  In other words, there is one state which is much more likely than all of the others given a particular sequence of measurements.  This doesn't work, for example, in our <a href = "../../Assignments/HW4_RobotLocalization/">robot localization</a> problem when the robot is in symmetric hallways, because there are many possible hypotheses.  There is an alternative known as a <b>particle filter</b> that can handle such cases by approximating a continuous distribution by a collection of discrete samples.  A particle filter is an instance of what's known as a "genetic algorithm."  You can <a href = "https://towardsdatascience.com/particle-filter-a-hero-in-the-world-of-non-linearity-and-non-gaussian-6d8947f4a3dc">click here</a> to read more about it.  I wish we had time to do it in this class, because it's really cool!
								</p>

                                    
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#debugging">Debugging Principles</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading / Deadlines Policy</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "../../../Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
												</li>
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<li>
													<a href = "../../Assignments/HW4_FundamentalFreq">HW4: Fundamental Frequency Tracking And Pitch-Based Audio Effects</a>
													<ul>
														<li>
															<a href = "../../Assignments/HW4_FundamentalFreq/statements.html">Musical statements</a>
														</li>
													</ul>
												</li>
												<li>
													<a href = "../../Assignments/HW5_LogisticRegression">HW5: Logistic Regression on Movie Reviews</a>
												</li>
												<li>
													<a href = "../../Assignments/HW6_BYOMLP">HW6: Build Your Own Multilayer Perceptron (BYOMLP)</a>
												</li>
												<!--
												<li>
													<a href = "../../Assignments/HW7_DeepLearning">HW7: (Deep) Neural Networks on Images</a>
												</li>
												!-->
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_Bandit/index.html">Week 1: Introduction To Reinforcement Learning</a>
													<ul>
														<li>
															<a href = "../../ClassExercises/Week1_Bandit/jsbandit/index.html">The Multi-Armed Bandit Game</a>
														</li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../Materials/MazeExplorer">Week 2: Maze Searching Game</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/assignments/186957">Week 3: An Admissible But Not Consistent Heuristic</a>
												</li>
												<li>
													<a href = "../../../Modules/Module2/Video1">Week 4: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_MarkovText">Week 4: Markov Chains for Document Representations</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/quizzes/24127">Week 4: Bayes Rule Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_NaiveBayes">Week 5: Bayes Rule And Naive Bayes Classifiers</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Naive Bayes Exercise</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 5/6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_RobotLocalization">Week 5/6: Robot Localization</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2023.github.io/Modules/HMM/Video1">Week 6: HMM Module</a>
												</li>
												<li>
													<a href = "https://github.com/ursinus-cs477-f2023/Week6_MDP">Week 6: Markov Decision Processes And Pong AI</a>
												</li>
												<li>
													<a href = "../../../Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN/index.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_KMeans">Week 7: KMeans Clustering And Visual Bag of Words</a>
												</li>
												<li>
													<a href = "../../../Modules/MatrixModule/Video1">Week 7: Matrix Module</a>
												</li>
												<li>
													<a href = "ClassExercises/Week7_PCA/Week7_PCA/UnsupervisedDigits.html">Week 7: PCA on MNIST Digits</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_LogisticRegression/index.html">Week 8: Logistic Regression And Gradient Descent</a>
												</li>
												<li>
													<a href = "../../../Modules/LogisticRegression/Video1">Week 8: Neural Networks Module 1</a>
												</li>
												<li>
													<a href = "../../../Modules/Softmax/Video1">Week 9: Softmax Module</a>
												</li>
												<li><a href = "../../../Modules/NeuralNets/Video0">Week 9/10: Multi-Class Logistic Regression And Feedforward Neural Nets Module</a></li>
												<li>
													<a href = "../../ClassExercises/Week10_Backprop/index.html">Week 10: Backpropagation on Multilayer Perceptrons</a>
												</li>
												<li>
													<a href = "../../../Modules/Backprop/Video1">Week 10: Backpropagation Module</a>
												</li>
												<li>
													<a href = "https://github.com/ursinus-cs477-f2023/CoursePage/blob/main/ClassExercises/Week11_CNNs/CatDog_Train_Better.ipynb">Week 11: Convolutional Neural Network with Data Augmentation for Cats vs Dogs</a>
												</li>
												<li>
													<a href = "../../../Modules/VAEGAN/Video1">Week 13/14: Variational Autoencoder / GAN Module</a>
												</li>
												
												
											</ul>
										</li>
										<li><a href = "../../Ethics/index.html">Ethics Reading / Discussions</a></li>
										<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
