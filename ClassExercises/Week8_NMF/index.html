<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Week 8: Nonnegative Matrix Factorization</h2>
                                        <h3><a href = "http://www.ctralie.com">Chris Tralie</a></h3>
									</header>

									<div id="page-content">
										<h3>Building Up Data from Primitive Components</h3>
										<p>
											We saw with the <a href = "../Week7_PCA/">PCA examples</a> that it's possible to decompose data into different components that sum up to make a more complex whole.  This is sometimes referred to as <a href = "https://team.inria.fr/panama/projects/please/dictionary-learning/">dictionary learning</a>.  We'll now consider a different way of learning such a dictionary.  But first, we need to get more comfortable with matrix multiplication.  
										</p>

										<p>
											<a href = "https://github.com/Ursinus-CS477-F2021/NMFBuildAFace/archive/refs/heads/main.zip">Click here</a> to download an image processing example to warm us up.  In this example, we have the following primitive 9 primitive 500x500 images, which we flatten and stack up into a <b>250,000 x 9</b> matrix <b>W</b>
										</p>

										<img src = "FaceColumns.png">

										<p>
											Your challenge is to find a 9-element column matrix that, when multiplied on the left by <b>W</b>, gives a smiling, winking man with a handlebar moustache.
										</p>

										<h3>Nonnegative Matrix Factorization Theory</h3>
										<p>
											The goal of nonnegative matrix factorization is to approximate a matrix 𝑉 as the product of two other matrices 𝑊 and 𝐻; that is to get as close as possible to the matrix equation 𝑉 =  𝑊𝐻 being true.  Solving this problem will allow us to <b>learn</b> the kinds of templates we saw above in data.  The question itself, though, should already seem a little weird, because even if 𝑊, 𝐻, and 𝑉 are 1x1 matrices (i.e. ordinary nonnegative numbers), there can be infinitely many solutions.  For instance, if I ask you how you could multiply two numbers together to get the number 12, you could do the following:
											<ul>
												<li>1 x 12 = 12</li>
												<li>2 x 6 = 12</li>
												<li>3 x 4 = 12</li>
												<li>2.4 x 5 = 12</li>
												<li>...</li>

											</ul>
											
											
										<p>
											But surprisingly, when we do this with matrices, sometimes just finding one factorization will be quite useful.  Crucially, though, we assume that all of the entries of 𝑊, 𝐻, and 𝑉 are <b>nonnegative</b>, which ends up allowing us to devise some simple <a href = "https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf">iterative algorithms to solve this numerically in practice</a>.  There is some great theory in the above paper to show that these algorithms converge and are related to something else we'll be talking about: gradient descent.  There is also other theory that characterizes probability distributions over the entries of the matrices we expect to get based on the different algorithms we choose (<a href = "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.184.7402&rep=rep1&type=pdf">click here</a> to see a paper about this).  This is how we can characterize the solutions that we're more likely to get over a possibly infinite set of solutions.
										</p>
										<p>
											The goal of today's exercise is to go through these algorithms in numpy.  We'll start with some synthetic examples, but this will set the stage for some interesting applications.  Even with the nonnegative constraint, we have plenty of applications.  For instance, 𝑉 could represent a data matrix of 2D images as in the <a href = "../Week7_PCA/UnsupervisedDigits.html">collection of MNIST digits</a>.  Or 𝑉 could represent something called a "spectrogram" of an audio signal.  You'll explore these two applications in the next homework.
										</p>
										
										<HR>
										<h4><a name = "euclidean">Formulation 1: Euclidean NMF</a></h4>
										<p>
											We seek a  𝑊  and an  𝐻  so that we minimize the <b>loss function</b> (or what you may have called the "objective" function in Math 111)
										</p>


										<div style="width:250px;">
										
											<h3> \[ L(W, H) = \sum_{i, j} (V[i, j]-WH[i, j])^2 \]</h3>
										</div>

										<ol>
											<li>
												First, randomly initialize  𝑊  and  𝐻  with nonnegative numbers
											</li>
											<li>
												If  𝑉  is a  𝑀𝑥𝑁  matrix, and we choose a number  𝐾  which is the "rank" of our approximate factorization
												<ul>
													<li>𝑊  is a  𝑀𝑥𝐾  matrix</li>
													<li>𝐻  is a  𝐾𝑥𝑁  matrix</li>
													<li>𝑊<SUP>𝑇</SUP>  is called the "transpose" of  𝑊 ;it's simply switching the rows for the columns. It is a  𝐾𝑥𝑀  matrix</li>
													<li>𝐻<SUP>𝑇</SUP>  would be a  𝑁𝑥𝐾</li>
													
												</ul>
											</li>
										</ol>

										

										


										<p>
											To solve this, you'll follow the <b>multiplicative update rules</b> below in sequence; that is, swap back and forth between the first step and the second step in a loop.
										</p>
										<div style="width:300px;">
											<uo>
												<li>
													<h3>
														\[ H[i, j] = H[i, j] \frac{ (W^TV)[i, j] }{(W^TWH)[i, j]} \]
													</h3>
												</li>
												<li>
													<h3>
														\[ W[i, j] = W[i, j] \frac{ (VH^T)[i, j] }{(WHH^T)[i, j]} \]
													</h3>
												</li>
											</ol>
										</div>
											
										<p>
											The <a href = "https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf">original paper</a> that presented this algorithm proved that the loss function only decreases at every iteration.
										</p>

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											import numpy as np
											M = 100
											N = 1000
											K = 10
											np.random.seed(0) # For repeatable results
											V = np.random.rand(M, N)
											# Initialize two random matrices to start with
											W = np.random.rand(M, K)
											H = np.random.rand(K, N)
										</script>  

										
										<b>Your Task in Class:</b>
										<p>
											
											Implement the above rules in a loop to solve for 𝑊 and 𝐻.  Keep track of the loss function at every iteration.  When you plot it at the end, you should see that the loss is going down.  <i>How does K affect the error of what the algorithm converges to?</i>
										</p>

										<b>Your Next Task:</b>
										<p>
											Go back and load in all of the digits.  Make the matrix <b>V</b> be the transpose of the <b>1-X</b> for the data matrix, so that each column holds a unique digit and the digits are drawn in white on a black background instead of vice versa.  Perform NMF on this matrix, and plot the columns of <b>W</b> as reshaped images.  What do they look like?  How do they change if you vary <b>K</b>?  What if <b>K = 10</b>?
										</p>

										<HR>
											<h4><a name = "kl">Formulation 2: Kullback-Liebler Divergence NMF</a></h4>

											<p>
												There are other loss functions we can use other than the sum of the squared differences of the entries of <b>V</b> and <b>WxH</b>.  One of them is called the "Kullback-Liebler" loss, and it's defined as follows
											</p>

											<div style="width:200px">

												<h3>
													\[  \sum_{i, j} \left( V[i, j] \left( \log \frac{V[i, j]}{WH[i, j]} \right) - V[i, j] + WH[i, j] \right) \]
												</h3>

											</div>


											<p>
												Notice how, like the Euclidean loss, this loss is 0 if <b>V = WxH</b>.  As it turns out, this loss will decrease under the following update rules
											</p>

											<div style="width:200px">
											
											<h3>
												<ul>
													<li>
														\[ V_L[i, j] = V[i, j] / (WH)[i, j] \]
													</li>
													<li>
														\[ H[i, j] = H[i, j] \left( \frac{  (W^T V_L)[i, j] }{{\sum_a}W[a, j]} \right) \]
													</li>
													<li>
														\[ V_L[i, j] = V[i, j] / (WH)[i, j] \]
													</li>
													<li>
														\[  W[i, j] = W[i, j] \left( \frac{  (V_L H^T)[i, j] }{{\sum_b}H[i, b]} \right) \]
													</li>
												</ul>

												
											</h3>
										</div>

										<p>
											You will implement this on <a href = "../../Assignments/HW5b_Unsupervised/">assignment 5</a>
										</p>




                                    
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#debugging">Debugging Principles</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading / Deadlines Policy</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "../../../Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
												</li>
												<!--
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<li>
													<a href = "../../Assignments/HW4_RobotLocalization">HW4: Bayesian Robot Localization</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5a_3DShapeCluster">HW5a: 3D Shape Clustering</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5b_Unsupervised">HW5b: NMF for Music Component Separation</a>
												</li>
												<li>
													<a href = "../../Assignments/HW6_LogisticRegression">HW6: Logistic Regression on Movie Reviews</a>
												</li>
												<li>
													<a href = "../../Assignments/HW7_DeepLearning">HW7: (Deep) Neural Networks on Images</a>
												</li>
												!-->
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_Bandit/index.html">Week 1: Introduction To Reinforcement Learning</a>
													<ul>
														<li>
															<a href = "../../ClassExercises/Week1_Bandit/jsbandit/index.html">The Multi-Armed Bandit Game</a>
														</li>
													</ul>
												</li>

												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../Materials/MazeExplorer">Week 2: Maze Searching Game</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "../../../Modules/Module2/Video1">Week 5: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Exercise / Theory of Bayesian Classifiers</a>
													<ul>
														<li><a href = "../../ClassExercises/Week5_BagOfWords#exercise">Text Classification Exercise</a></li>
														<li><a href = "../../ClassExercises/Week5_BagOfWords#theory">Naive Bayes Theory</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../../Modules/Module3/Exercise0">Week 5: Bayes Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "../../../Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "../../../Modules/MatrixModule/Video1">Week 8: Matrix Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_LogisticRegression/index.html">Week 10/11: Logistic Regression And Gradient Descent</a>
												</li>
												
											</ul>
										</li>
										<li><a href = "../../Ethics/index.html">Ethics Reading / Discussions</a></li>
										<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
