<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Backpropagation for Multilayer Perceptrons</h2>
										<h3>Chris Tralie</h3>
									</header>
									
									<p>
										<a href = "../../../Modules/Backprop/Video1">Click here</a> to view the companion module for these notes
									</p>

									<h3>
										Table of Contents
										<ul>
											<li>
												<a href = "#leakyrelu">Leaky ReLU</a>
											</li>
											<li>
												<a href = "#forward">Forward Pass via Matrix Operations</a>
											</li>
											<li>
												<a href = "#backward">Backward Pass via Matrix Operations</a>
											</li>
										</ul>

									</h3>

									<p>
										In <a href = "../HW5_LogisticRegression/">homework 5</a>, you implemented gradient descent to perform logistic regression.  This can be thought of as learning a neural network with a single neuron ("the perceptron"), but the best we can do in this case is to learn a separating hyperplane.  As we discussed in class, though, when we put a bunch of neurons together, we can learn arbitrarily complicated functions.  So now we're going to take gradient descent to the next level to learn how to solve arbitrary fully connected feed forward networks, also known as <b>multilayer perceptrons (MLPs)</b>, by using an algorithm called <b>backpropagation</b> as a subroutine.  Actually, as you will see, this is just a fancy name for the chain rule of calculus, specialized to neural networks.
									</p>

									<p>
										At this point, we could just turn to one of the myriad libraries out there like pytorch to optimize neural network models for us, but I want you to see if we use the right definitions, then creating a vanilla neural network solver really isn't that much code in numpy.  It's also important that know how everything works under the hood when you run pytorch (see <a href = "https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b">this Medium article by Andrej Karpathy</a> on why it's important to understand these details).  You will implement the algorithms I describe below in <a href = "../../Assignments/HW6_BYOMLP/">homework 6</a>.
									</p>

									<h3>
										<a name = "leakyrelu">Leaky ReLU</a>
									</h3>
									<p>
										Before I dive into the backpropagation algorithm, I first want to define a new type of activation function that will come in handy.
									</p>
									<p>
										One of the issues with the logistic function is that it suffers from the problem of "vanishing gradients"; if the input to the logistic function is far from zero, then it is nearly flat, as shown below:
									</p>

									<img src = "Logistic.svg" width=60%>
									
									<p>
										This makes it slow to learn, as the step sizes for internal weights in the network in these regimes will be very small.  To address this, there's another activation function that's very popular known as the <b>Leaky Rectified Linear Unit (Leaky ReLU)</b>, which can be defined as the following piecewise function 
									</p>

									<div style="width:200px">
									<h3>
										\[ f(u) = \left\{  \begin{array}{cc}  u & u > 0 \\ 0.01 u & u \leq 0  \end{array}  \right\} \]
									</h3>
									</div>

									<p>
										The derivative of this function is then 
									</p>

									<div style="width:200px">
										<h3>
											\[ f'(u) = \left\{  \begin{array}{cc}  1 & u > 0 \\ 0.01 & u \leq 0  \end{array}  \right\} \]
										</h3>
									</div>

									<p>
										These functions are plotted below
									</p>

									<img src = "LeakyReLU.svg" width=60%>

									<p>
										We've lost continuity of the derivative at the origin, but otherwise, it's great numerically and it never saturates, and you'll find that learning can happen much faster.
									</p>

									<h3><a name = "forward">Forward Pass via Matrix Operations</a></h3>
									
									<p>
										A multilayer perceptron is a sequence of layers of perceptrons.  The output of each perceptron in each layer is fed as an input to <b>every perceptron</b> the next layer of perceptrons.  This means that we need to define a weight between every pair of perceptrons between two adjacent layers.  The image below shows all such weights between two layers with 3 and 4 perceptrons, respectively
									</p>

									<img src = "NetworkSnapshot.svg" width=25%>


									<p>
										At the end of the day, though, an MLP is just a function, and we want to know what it should output given a particular input <b>x</b> fed into the first layer.  The key trick to make this easier for multilayer perceptrons is to recast their operation as a sequence of matrix multiplications and element-wise applications of activation functions<SUP><a href = "#goodfellow">[1]</a></SUP>.  Let's take the following piece of a network below, for instance, where the blue nodes show inputs to the orange neurons:
									</p>

									<p>
										<a name = "goodfellow"><SUP>[1]</SUP></a> In what follows, I'll be mostly following notational conventions from <a href = "https://www.deeplearningbook.org/contents/mlp.html">Ch. 6.5 of <i>Deep Learning</i> by Goodfellow, Bengio, and Courville</a>
									</p>


									


									<p>
										Let's also assume that the <b>i<SUP>th</SUP></b> orange node has a bias <b>b<SUB>i</SUB></b>.  Assuming that the activation function for the orange neurons is a function <b>f(u)</b>, then we could write the output of the <b>i<SUP>th</SUP> neuron as</b>
									</p>

									<div style="width:100px;">
									<h3>
										\[ a[i] = w_{0i} x_0 + w_{1i} x_1 + w_{2i} x_2 + w_{3i} x_3 + b_i \]
										\[ h[i] = f(a[i])  \]
									</h3>
									</div>

									<p>
										In other words, the vector <b>a</b> is a dot product of the weights and the input vector <b>x</b>.  But there is a much more elegant way to write transformations for <b>all</b> inputs if we reformulate it as a matrix expression, and this will be much easier to implement in code.  In particular, define the following matrices 
									</p>

									<div style="width:100px;">
										<h3>
											\[ x = \left[ \begin{array}{c} x_0 \\ x_1 \\ x_2 \\ x_3  \end{array} \right], W = \left[ \begin{array}{cccc}w_{00}&w_{10}&w_{20}&w_{30}\\w_{01}&w_{11}&w_{21}&w_{31}\\w_{02}&w_{12}&w_{22}&w_{32}\\w_{03}&w_{13}&w_{23}&w_{33}\\w_{04}&w_{14}&w_{24}&w_{34}\end{array}
											\right], b = \left[ \begin{array}{c} b_0 \\ b_1 \\ b_2 \\ b_3 \\ b_4 \end{array} \right] \]
										</h3>
									</div>

									<p>
										Then the output of a layer in the network can be defined in two stages
									</p>

									<div style="width:50px;">
										<h3>
										 \[a = Wx + b \text{, which is a linear operation }\]
										\[ h = f(a)  \text{, which is a nonlinear operation applied element-wise to } a\]
										</h3>
									</div>

									<p>
										In general, the parameters to map from the output of a layer with <b>N</b> neurons to the input of a layer with <b>M</b> neurons can be described by an <b>M x N</b> weight matrix <b>W</b> and an <b>Mx1</b> bias column vector <b>b</b>.  To propagate information through the whole network, we continually apply this sequence of linear and nonlinear operators in matrix form.  We just have to store the matrix <b>W</b>, the vector <b>b</b>, and the nonlinear function <b>f</b> that we're using at each layer.  And that's it!
									</p>

									<p>
										In sum, below is pseudocode that describes how to do a forward pass to transform the input from each layer to the next through the entire network
									</p>

									<div style="background-color: blanchedalmond; padding:20px;">
									<h3><a name = "forwardalg">Algorithm 1: Forward Propagation Through A Fully Connected Feedforward Neural Network</a></h3>
									<h4>def forward(x)</h4>
									<ul>
										<li>Let <b>L</b> be the number of layers</li>
										<li>Let <b>h<SUB>0</SUB> = x</b></li>
										<li>
											for <b>k = 1, 2, ..., L</b>
											<ul>
												<li>
													Let <b>a<SUB>k</SUB> = W<SUB>k</SUB> h<SUB>k-1</SUB> + b<SUB>k</SUB></b> // Linear step. The input to this layer, h<SUB>k-1</SUB>, is the output of the last layer
												</li>
												<li>
													Let <b>h<SUB>k</SUB> = f<SUB>k</SUB>(a<SUB>k</SUB>)</b> // Nonlinear step
												</li>
											</ul>
										</li>
										<li>
											<b>y<SUB>est</SUB> = h<SUB>L</SUB></b> // The output of the last layer is the output of our network
										</li>
									</ul>
									</div>

									<p></p>
									


									<h3><a name = "backward">Backward Pass via Matrix Operations</a></h3>

									<p>
										<a href = "../../../Modules/Backprop/Video3">Click here</a> to view a derivation in the companion module for these notes, which explains how I got the equations in the pseudocode below
									</p>

									<p>
										We're now ready to express the equations to compute the gradient over all parameters in the network; in other words, we will now figure out the effect of our parameters on the network downstream, so that we know how to change the parameters to improve classification.  This will again boil down to some matrix equations that should be fairly painless to implement in code.  Though a full derivation of these equations is beyond the scope of this writeup, I will give some intuition by looking at the at a simple network with 3 neurons, each with one input and one output (so that we can avoid doing matrix derivatives for the moment).  In particular, let's consider the following function:
									</p>
									
									<div style="width:100px;">
									<h3>
										\[ g(x) = f_3(w_3 f_2(w_2 f_1(w_1x + b_1) + b_2) + b_3) \]
									</h3>
									</div>

									<p>
										In order to do updates, we need derivatives with respect to our weights <b>w<SUB>1</SUB></b>, <b>w<SUB>2</SUB></b>, <b>w<SUB>3</SUB></b> and our biases  <b>b<SUB>1</SUB></b>, <b>b<SUB>2</SUB></b>, <b>b<SUB>3</SUB></b>.  Let's use the notation we established in the forward pass to define the following variables 
										<div style="width:50px;">
											<ul>
												<li>
													\[ a_1 = w_1x + b_1 \]
												</li>
												<li>
													\[ h_1 = f_1(a_1) \]
												</li>
												<li>
													\[ a_2 =  w_2 h_1 + b_2 \]
												</li>
												<li>
													\[ h_2 =  f_2(a_2) \]
												</li>
												<li>
													\[ a_3 = w_3 h_2 + b_3 \]
												</li>
												<li>
													\[ g(x) = f_3(a_3) \]
												</li>
											</ul>
										</div>

										<p>
											Then we can compute the following derivatives from the outside of the expression inwards, using the chain rule (recall that a partial derivative holds all of the variables fixed as constants except for the one we're taking the derivative with respect to)
										</p>
										
										<div style="width:50px;">
											<ol>
												<li>
													\[ \frac{\partial g}{\partial w_3} =  f_3'(a_3) \times  h_2 \]
												</li>
												<li>
													\[ \frac{\partial g}{\partial b_3} =  f_3'(a_3)  \]
												</li>
												<li>
													\[ \frac{\partial g}{\partial w_2} =  f_3'(a_3) \times w_3 \times f_2'(a_2) \times h_1 \]
												</li>
												<li>
													\[ \frac{\partial g}{\partial b_2} =  f_3'(a_3) \times w_3 \times f_2'(a_2) \]
												</li>
												<li>
													\[ \frac{\partial g}{\partial w_1} =  f_3'(a_3) \times w_3 \times f_2'(a_2) \times w_2 \times f_1'(a_1) \times x \]
												</li>
												<li>
													\[ \frac{\partial g}{\partial b_1} =  f_3'(a_3) \times w_3 \times f_2'(a_2) \times w_2 \times f_1'(a_1) \]
												</li>
											</ol>
										</div>

									</p>

									<p>
										A pattern is starting to emerge here.  In particular, notice how equation 2 is contained in part of equations 3 and 4 and how equation 4 is contained in part of equations 5 and 6.  So this means we'll be able to define some recursive substitutions from layer to layer as we go along, just like we remembered outputs of layers from one to the next as we went along during the forward pass.
									</p>
									<p>
										From the point of view of the network, the first derivatives we're able to compute are with respect to parameters at the end of the network.  We can then use these expressions to substitute in for parameters in layers that precede them.  We can avoid recomputing things by remembering some of the products we computed along the way.  This leads to an efficient dynamic programming algorithm known as <b>backpropagation</b>.  It earns its name since, by contrast to evaluating moving forward layer to layer when evaluating an input, we actually start with the output of the network and compute the gradients backwards layer by layer.
									</p>

									<p>
										There's one more thing I omitted, which is that we also define a loss function over the output, and we're really looking for the gradient with respect to the loss function.  But the above gives a flavor for the patterns that emerge.
									</p>

									<p>
										Below is the pseudocode for the matrix form of backpropagation for general feedforward networks of any shape.  It may look daunting at first, but each bullet point should be a single line of code since we set up things in such an organized way with matrices.
									</p>

									<p>
										If you look closely, you can match these steps up with the simple 1D example I gave above and see where the substitutions happen from one layer to the next.  In a nutshell, what I've done is rearrange some of the results from the <a href = "../../../Modules/Backprop/Video3">derivations I did in the module</a> to be more convenient in code.
									</p>


									<div style="background-color: blanchedalmond; padding:20px;">
										<h3><a name = "backpropalg">Algorithm 2: Backpropagation Through A Fully Connected Feedforward Neural Network</a></h3>
										<p>
											<b>NOTE: </b> Below I refer to derivatives <b>f'</b> as <b>f_deriv</b> so that the <b>'</b> doesn't get lost in the shuffle
										</p>

										<h4><b>def backprop(x, y)</b>, where x is input and y is ground truth label</h4>
											<p>
												After this iterates, the lists <b>W<SUB>derivs</SUB></b> and <b>b<SUB>derivs</SUB></b> will be populated with <b>matrices</b> that hold the derivatives of all weights and biases, respectively
											</p>
										<ul>
											<li>Let <b>L</b> be the number of layers</li>
											<li>Call <b>forward(x)</b> to compute a's and h's at each layer</li>
											<li>Let <b>y<SUB>est</SUB> = h<SUB>L</b></SUB></li>
											<li>Let <b>g = est_lossderiv(y<SUB>est</SUB>, y)</b> // This is the derivative of the loss function with respect to the inputs of the last layer </li>
											<li>
												for <b>k = L, L-1, ..., 1</b>
												<ul>
													<li>
														// Step 1: Propagate gradient backwards through the nonlinear output <b>f<SUB>k</SUB></b> of this layer<BR>
														if <b>k &lt; L</b> 
														<ul>
															<li>
																<b>g = g*f_deriv<SUB>k</SUB>(a<SUB>k</SUB>)</b> // This is element-wise multiplication of g and f_deriv<SUB>k</SUB>(a<SUB>k</SUB>), which are parallel arrays
															</li>
														</ul>
													</li>
													<li>
														// Step 2: Compute the gradients of the weights and biases at this layer 
														<Ul>
															<li>
																Let <b>b_derivs<SUB>k</SUB> = g</b> // We now have the gradient for biases in this level
															</li>
															<li>
																Let <b>W_derivs<SUB>k</SUB> = g h<SUB>k-1</SUB><SUP>T</SUP></b> // This is a matrix multiplication.  Treating <b>h<SUB>k-1</SUB></b> and <b>g</b> as column matrices, this performs their <a href = "https://en.wikipedia.org/wiki/Outer_product">outer product</a> to get the gradient for each weight at this layer 

																<p>
																	// As a sanity dimension check, note that <b>h<SUB>k-1</SUB></b> is the output of the layer before, which is the input to this layer, and <b>g</b> is the gradient of the output of this layer.  So if <b>h<SUB>k-1</SUB></b> is an <b>N x 1</b> matrix and <b>g</b> is an <b>M x 1</b> matrix, then <b>g h<SUB>k-1</SUB><SUP>T</SUP></b> will be an <b>M x N</b> matrix, which matches the dimensions of <b>W<SUB>k</SUB></b>.  So each element of <b>g h<SUB>k-1</SUB><SUP>T</SUP></b> will hold the derivative of <b>W<SUB>k</SUB></b>'s corresponding weight.
																</p>
																<p>
																	<b>NOTE:</b> When coding this in numpy, you can take advantage of <a href = "https://numpy.org/doc/stable/reference/generated/numpy.outer.html"><code>np.outer</code></a>
																</p>
																<p>
																	<b>NOTE Also:</b> When we're at the first layer (<b>k=1</b>), then <b>h<SUB>k-1</SUB></b> is actually <b>x</b>, the input to the whole network. 
																</p>
															</li>
														</Ul>
													</li>
													<li>
														// Step 3: Propagate the gradient backwards through the linear part of this layer to be used at the next layer back 
														<ul>
															<li>
																<b>g = W<SUB>k</SUB><SUP>T</SUP> g</b> // This is a matrix multiplication
															</li>
														</ul>
													</li>
												</ul>
											</li>
										</ul>
									</div>
									<p></p>

						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#debugging">Debugging Principles</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading / Deadlines Policy</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "../../../Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
												</li>
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<li>
													<a href = "../../Assignments/HW4_FundamentalFreq">HW4: Fundamental Frequency Tracking And Pitch-Based Audio Effects</a>
													<ul>
														<li>
															<a href = "../../Assignments/HW4_FundamentalFreq/statements.html">Musical statements</a>
														</li>
													</ul>
												</li>
												<li>
													<a href = "../../Assignments/HW5_LogisticRegression">HW5: Logistic Regression on Movie Reviews</a>
												</li>
												<li>
													<a href = "../../Assignments/HW6_BYOMLP">HW6: Build Your Own Multilayer Perceptron (BYOMLP)</a>
												</li>
												<!--
												<li>
													<a href = "../../Assignments/HW7_DeepLearning">HW7: (Deep) Neural Networks on Images</a>
												</li>
												!-->
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_Bandit/index.html">Week 1: Introduction To Reinforcement Learning</a>
													<ul>
														<li>
															<a href = "../../ClassExercises/Week1_Bandit/jsbandit/index.html">The Multi-Armed Bandit Game</a>
														</li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../Materials/MazeExplorer">Week 2: Maze Searching Game</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/assignments/186957">Week 3: An Admissible But Not Consistent Heuristic</a>
												</li>
												<li>
													<a href = "../../../Modules/Module2/Video1">Week 4: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_MarkovText">Week 4: Markov Chains for Document Representations</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/quizzes/24127">Week 4: Bayes Rule Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_NaiveBayes">Week 5: Bayes Rule And Naive Bayes Classifiers</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Naive Bayes Exercise</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 5/6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_RobotLocalization">Week 5/6: Robot Localization</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2023.github.io/Modules/HMM/Video1">Week 6: HMM Module</a>
												</li>
												<li>
													<a href = "https://github.com/ursinus-cs477-f2023/Week6_MDP">Week 6: Markov Decision Processes And Pong AI</a>
												</li>
												<li>
													<a href = "../../../Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN/index.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_KMeans">Week 7: KMeans Clustering And Visual Bag of Words</a>
												</li>
												<li>
													<a href = "../../../Modules/MatrixModule/Video1">Week 7: Matrix Module</a>
												</li>
												<li>
													<a href = "ClassExercises/Week7_PCA/Week7_PCA/UnsupervisedDigits.html">Week 7: PCA on MNIST Digits</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_LogisticRegression/index.html">Week 8: Logistic Regression And Gradient Descent</a>
												</li>
												<li>
													<a href = "../../../Modules/LogisticRegression/Video1">Week 8: Neural Networks Module 1</a>
												</li>
												<li>
													<a href = "../../../Modules/Softmax/Video1">Week 9: Softmax Module</a>
												</li>
												<li><a href = "../../../Modules/NeuralNets/Video0">Week 9/10: Multi-Class Logistic Regression And Feedforward Neural Nets Module</a></li>
												<li>
													<a href = "../../ClassExercises/Week10_Backprop/index.html">Week 10: Backpropagation on Multilayer Perceptrons</a>
												</li>
												<li>
													<a href = "../../../Modules/Backprop/Video1">Week 10: Backpropagation Module</a>
												</li>
												<li>
													<a href = "https://github.com/ursinus-cs477-f2023/CoursePage/blob/main/ClassExercises/Week11_CNNs/CatDog_Train_Better.ipynb">Week 11: Convolutional Neural Network with Data Augmentation for Cats vs Dogs</a>
												</li>
												<li>
													<a href = "../../../Modules/VAEGAN/Video1">Week 13/14: Variational Autoencoder / GAN Module</a>
												</li>
												
												
											</ul>
										</li>
										<li>
											<span class="opener">Ethics Reading / Discussions</span>
											<ul>
												<li><a href = "../../Ethics/index.html#intro">Bias, Social Media, Current vs Future Harms</a></li>
												<li><a href = "../../Ethics/index.html#corporatecapture">Corporate Capture And Colonial Practices</a></li>
												<li><a href = "../../Ethics/index.html#music">The Ethics of AI in Art / Music</a></li>
												<li><a href = "../../Ethics/index.html#climate">AI And The Climate Crisis</a></li>
												<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
											</ul>
										</li>
										
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
