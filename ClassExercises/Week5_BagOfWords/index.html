<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Week 5: Bag of Words Naive Bayes Exercise</h2>
                                        <h3><a href = "http://www.ctralie.com">Chris Tralie</a></h3>
									</header>

									<div id="page-content">
										<p>
											In <a href = "../../Assignments/HW3_Markov/">homework 3</a>, you've been modeling text as a sequence using Markov chains.  We're going to do something seemingly much dumber and completely forget the sequence of words and focus just on how often words occur across entire documents.  For example, from the first 2016 debate transcript on the homework, you get the following "bag of words" for Clinton (courtesy of <a href = "https://www.wordclouds.com/">wordclouds.com</a>)
										</p>

										<img src = "clinton.png" width=500>

										<p>and the following for Trump</p>

										<img src = "trump.png" width=500>

										<p>
											We can estimate the probability of a single word as simply the number of times that word was said divided by the total number of words, which will be proportional to the size of the words in the above word clouds.  For example, Clinton said 6257 words total, and she said "Donald" 20 times, so her probabability of saying Donald would be estimated as 
										</p>

										<h3>
											\[ 20/6257 \]
										</h3>
										
										<p>by contrast, Trump only says "Donald" once out of 8367 words, so his probability would be </p>

										<h3>
											\[ 1/8367 \]
										</h3>
										

										<p>
											Your task will be to estimate the probabilities for all words from data and assume complete independence, as per the <a href = "../Week5_NaiveBayes/index.html#naivebayes">naive bayes</a> assumption, and multiply them together to get a total probability for both Trump and Clinton of each phrase, for each model.  For example, if you got the text "Donald job job Donald Donald", and given that Trump said "job" 6/8367 words, we'd compute the likelihood of the Trump model as 

										</p>

										<h3>
											\[ (1/8367) \times (6/8367) \times (6/8367) \times (1/8367) \times (1/8367) \]
										</h3>

										<p>
											You'd then also want to compute such a probability using the Clinton model, and choose the model with the highest probability as your estimate for who said what.  You should try to classify all of the Trump and Clinton quotes from the third debate this way and record how many of your guesses are correct.  As you go along, we will talk about some pitfalls, and after class, we will dig into some more theory about why this is actually correct and what the "Naive Bayes Model" is precisely
										</p>
											

										<h3>Tweaks To The Technique</h3>
										<p>
											Here are a few things to think about as you go:
										</p>
										<ul>
											<li>What's the problem with multiplying a lot of probabilities together? how would you fix this?</li>
											<li>What happens if we never saw a particular word in our "training data"?  What should we do about that?</li>
										</ul>

										<h3>Programming tips</h3>
										
											<p>
												Here's some code to load the debate text for Clinton
											</p>

											<script type="syntaxhighlighter" class="brush: python"><![CDATA[
												fin = open("text/2016Debates/clinton1.txt")
												text = fin.read()
												fin.close()	
											</script>  

											<p>
												Here's a slick way to loop through all of the test data (the third debate) for Clinton
											</p>

											<script type="syntaxhighlighter" class="brush: python"><![CDATA[
												import glob
												for f in glob.glob("text/2016Debates/clinton3*.txt"):
													fin = open(f)
													test = fin.read()
													fin.close()
													## TODO: Do bag of words on test and compute probabilities under different models
											</script>  

											

										<p>
											To get a list of words in a string, use the <code>split()</code> method of the string class.  For example,
										</p>

										
										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											s = "Hello CS 477 this is Dr. Tralie speaking\nWe're doing some cool stuff!"
											s.split()</script>  

										<p>
											would yield
										</p>

										
										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											['Hello',
											'CS',
											'477',
											'this',
											'is',
											'Dr.',
											'Tralie',
											'speaking',
											"We're",
											'doing',
											'some',
											'cool',
											'stuff!']</script>  

										
										
										<h2><a name = "solution">Solution</a></h2>
										<p>
											Below is some code I wrote to implement this on the 2016 debate text.  There were two problems that needed to be addressed which people pointed out in class:
											
											<ol>
												<li>
													To prevent numerical underflow by multiplying tons of small numbers, I summed the logs of probabilities instead of multiplying them directly.  The log is a monotonic function, so it preserves maxes, and this won't change which model wins.
												</li>
												<li>
													If we see a word when we're classifying a new document that wasn't in the training model, it will have a probability of 0 by default.  But this will completely kill off the probability of the whole thing when multiplied through.  So we need to do something more intelligent.  We opt to do something called <a href = "http://static.hlt.bme.hu/semantics/external/pages/n-gram/en.wikipedia.org/wiki/Additive_smoothing.html">additive smoothing</a>, in which each unique word has an extra count added to it.  This means we also have to add to the denominator the unique number of words so that the probabilities still sum to 1.  If we see a word that wasn't in the training data, we simply assign a <b>1/denominator</b> to it
												</li>
											</ol>
										</p>

										<p>
											Below is the code.  If you run it, you'll see that it's lightning fast!  This is one of the advantages of Naive Bayes, even in situations where it doesn't work as well as other supervised learning techniques
										</p>

										<iframe src="Solution.html" width=100% height=1600>

										</iframe>
										

											






                                    
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#debugging">Debugging Principles</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading / Deadlines Policy</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "../../../Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
												</li>
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<!--
												<li>
													<a href = "../../Assignments/HW4_RobotLocalization">HW4: Bayesian Robot Localization</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5a_3DShapeCluster">HW5a: 3D Shape Clustering</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5b_Unsupervised">HW5b: NMF for Music Component Separation</a>
												</li>
												<li>
													<a href = "../../Assignments/HW6_LogisticRegression">HW6: Logistic Regression on Movie Reviews</a>
												</li>
												<li>
													<a href = "../../Assignments/HW7_DeepLearning">HW7: (Deep) Neural Networks on Images</a>
												</li>
												!-->
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_Bandit/index.html">Week 1: Introduction To Reinforcement Learning</a>
													<ul>
														<li>
															<a href = "../../ClassExercises/Week1_Bandit/jsbandit/index.html">The Multi-Armed Bandit Game</a>
														</li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../Materials/MazeExplorer">Week 2: Maze Searching Game</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/assignments/186957">Week 3: An Admissible But Not Consistent Heuristic</a>
												</li>
												<li>
													<a href = "../../../Modules/Module2/Video1">Week 4: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_MarkovText">Week 4: Markov Chains for Document Representations</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/quizzes/24127">Week 4: Bayes Rule Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_NaiveBayes">Week 5: Bayes Rule And Naive Bayes Classifiers</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Naive Bayes Exercise</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_KMeans">Week 5: KMeans Clustering And Visual Bag of Words</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 5/6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "../../../Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "../../../Modules/MatrixModule/Video1">Week 8: Matrix Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_LogisticRegression/index.html">Week 10/11: Logistic Regression And Gradient Descent</a>
												</li>
												
											</ul>
										</li>
										<li><a href = "../../Ethics/index.html">Ethics Reading / Discussions</a></li>
										<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
