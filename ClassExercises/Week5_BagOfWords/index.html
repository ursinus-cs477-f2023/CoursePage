<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2021</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2021</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h1>Week 5: Bag of Words Exercise / Theory of Bayesian Classifiers</h1>
                                        <h3><a href = "http://www.ctralie.com">Chris Tralie</a></h3>
									</header>

									<div id="page-content">
                                        <HR>
										<h1><a name = "exercise">Bag of Words Exercise</a></h1>
										<p>
											In <a href = "../../Assignments/HW3_Markov/">homework 3</a>, you've been modeling text as a sequence using Markov chains.  We're going to do something seemingly much dumber and completely forget the sequence of words and focus just on how often words occur across entire documents.  For example, from the first 2016 debate transcript on the homework, you get the following "bag of words" for Clinton (courtesy of <a href = "https://www.wordclouds.com/">wordclouds.com</a>)
										</p>

										<img src = "clinton.png" width=500>

										<p>and the following for Trump</p>

										<img src = "trump.png" width=500>

										<p>
											We can estimate the probability of a single word as simply the number of times that word was said divided by the total number of words, which will be proportional to the size of the words in the above word clouds.  For example, Clinton said 6257 words total, and she said "Donald" 20 times, so her probabability of saying Donald would be estimated as 
										</p>

										<h3>
											\[ 20/6257 \]
										</h3>
										
										<p>by contrast, Trump only says "Donald" once out of 8367 words, so his probability would be </p>

										<h3>
											\[ 1/8367 \]
										</h3>
										

										<p>
											Your task will be to estimate the probabilities for all words from data and assume complete independence (the "naive bayes" assumption), and multiply them together to get a total probability for both Trump and Clinton of each phrase, for each model.  For example, if you got the text "Donald job job Donald Donald", and given that Trump said "job" 6/8367 words, we'd compute the likelihood of the Trump model as 

										</p>

										<h3>
											\[ (1/8367) \times (6/8367) \times (6/8367) \times (1/8367) \times (1/8367) = (1/8367)^3 \times (6/8367)^2 \]
										</h3>

										<p>
											You'd then also want to compute such a probability using the Clinton model, and choose the model with the highest probability as your estimate for who said what.  You should try to classify all of the Trump and Clinton quotes from the third debate this way and record how many of your guesses are correct.  As you go along, we will talk about some pitfalls, and after class, we will dig into some more theory about why this is actually correct and what the "Naive Bayes Model" is precisely
										</p>
											

										<h3>Tweaks To The Technique</h3>
										<p>
											Here are a few things to think about as you go:
										</p>
										<ul>
											<li>What's the problem with multiplying a lot of probabilities together? how would you fix this?</li>
											<li>What happens if we never saw a particular word in our "training data"?  What should we do about that?</li>
										</ul>

										<h3>Programming tips</h3>
										
											<p>
												Here's some code to load the debate text for Clinton
											</p>

											<script type="syntaxhighlighter" class="brush: python"><![CDATA[
												fin = open("text/2016Debates/clinton1.txt")
												text = fin.read()
												fin.close()	
											</script>  

											<p>
												Here's a slick way to loop through all of the test data (the third debate) for Clinton
											</p>

											<script type="syntaxhighlighter" class="brush: python"><![CDATA[
												import glob
												for f in glob.glob("text/2016Debates/clinton3*.txt"):
													fin = open(f)
													test = fin.read()
													fin.close()
													## TODO: Do bag of words on test and compute probabilities under different models
											</script>  

											

										<p>
											To get a list of words in a string, use the <code>split()</code> method of the string class.  For example,
										</p>

										
										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											s = "Hello CS 477 this is Dr. Tralie speaking\nWe're doing some cool stuff!"
											s.split()</script>  

										<p>
											would yield
										</p>

										
										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											['Hello',
											'CS',
											'477',
											'this',
											'is',
											'Dr.',
											'Tralie',
											'speaking',
											"We're",
											'doing',
											'some',
											'cool',
											'stuff!']</script>  


										<h2><a name = "solution">Solution</a></h2>
										<p>
											Below is some code I wrote to implement this on the 2016 debate text.  There were two problems that needed to be addressed which people pointed out in class:
											
											<ol>
												<li>
													To prevent numerical underflow by multiplying tons of small numbers, I summed the logs of probabilities instead of multiplying them directly.  The log is a monotonic function, so it preserves maxes, and this won't change which model wins.
												</li>
												<li>
													If we see a word when we're classifying a new document that wasn't in the training model, it will have a probability of 0 by default.  But this will completely kill off the probability of the whole thing when multiplied through.  So we need to do something more intelligent.  We opt to do something called <a href = "http://static.hlt.bme.hu/semantics/external/pages/n-gram/en.wikipedia.org/wiki/Additive_smoothing.html">additive smoothing</a>, in which each unique word has an extra count added to it.  This means we also have to add to the denominator the unique number of words so that the probabilities still sum to 1.  If we see a word that wasn't in the training data, we simply assign a <b>1/denominator</b> to it
												</li>
											</ol>
										</p>

										<p>
											Below is the code.  If you run it, you'll see that it's lightning fast!  This is one of the advantages of Naive Bayes, even in situations where it doesn't work as well
										</p>

										<iframe src="Solution.html" width=800 height=800>

										</iframe>


										

										<HR>
											<h1><a name = "theory">Theory of Bayesian Classifiers</a></h1>
											
											
											<p>
												Below is some theory on why the above actually works.  
											</p>

											<h2><a name = "bayesclassifier">The Bayesian Classifier Framework</a></h2>

											<p>
												First, let's define a Bayesian framework for classifying data based on models:
											</p>

											<img src = "BayesBreakdown.svg" width=600>

											<p>
												In other words, we want to know how likely a class is given a particular observation, but we're usually only able to model things the other way around, as how likely we are to have made an observation given that we're in a class.  In this context, a "class" is a different type of event.  In the above example, it would either be that Trump is speaking or that Clinton is speaking.
											</p>

											<p>
												Let's look at the above formula applied to two different classes <b>C<SUB>1</SUB></b> and <b>C<SUB>2</SUB></b>
											</p>
											<div style="width:400px;">
												<h3>
													\[ p(C_1 | X) = P(X | C_1) P(C_1) / P(X) \]
												</h3>

												<h3>
													\[ p(C_2 | X) = P(X | C_2) P(C_2) / P(X) \]
												</h3>
											</div>

											<p>
												We notice that while the likelihoods and prior probabilities are different, they have a common factor of "evidence."  The expression for evidence is often nasty, as it involves sums of probabilities over many disjoint events.  Since it's a common factor, though, it won't change the relative order of the probabilities of different classes, so we can omit it and define something instead called a "posterior likelihood," which is proportional to the probability
											</p>

											<div style="width:400px;">
												<h3>
													Def. Posterior Likelihood
												</h3>

												<h3>
													\[ \ell(C | X) = P(X | C) P(C) \propto P(C | X) \]
												</h3>
											</div>

											<p>
												Another neat thing we can do is take the log of this to separate out the likelihood and prior terms into a sum instead of a product.  We also saw in the above exercise that taking the log prevents the numerical problem of <a href = "https://en.wikipedia.org/wiki/Arithmetic_underflow">underflow</a>
											</p>

											<div style="width:400px;">
												<h3>
													Def. Log Posterior Likelihood
												</h3>

												<h3>
													\[ \log \ell(C | X) = \log (P(X | C)) + \log(P(C)) \]
												</h3>
											</div>


											<h3>Def. Classify</h3>
											<p>
												When we say we want to "classify" something in the Bayesian framework, it means we want to find the class that maximizes the (log) posterior likelihood.  In computer science terms, it's the "argmax" of the (log) likelihood over all classes.  Whichever class maximizes this value is referred to as the <b>maximum a posteriori estimate</b>, and it's the label we will guess that our data is.
											</p>

											<p></p><p></p><p></p>

											<h2><a name = "bowtheory">Bag of Words / Naive Bayes</a></h2>

											<p>
												We can be a little more specific about if we link it to the bag of words example.  Bag of words is an example of something called "Naive Bayes."  The assumption behind Naive Bayes is that all of the components of our observation are independent.  In the context of bag of words, this means that we draw words completely at random out of our bag of words in our model, and the probability of the next word we draw is the same regardless of all of the other words we've drawn.
											</p>

											<p>
												Let's suppose we have the following setup with a set of different classifier.
												<ol>
													<li>
														A bag of words model with <b>M</b> words in it, with <b>p<SUB>ij</SUB></b> being the probability of the <b>j<SUP>th</SUP></b> word under class <b>i</b>
													</li>
													<li>
														A new string <b>s</b> that we want to classify, which has <b>N</b> words in it.  Let <b>k<SUB>j</SUB></b> be the number of times the <b>j<SUP>th</SUP></b> word in the dictionary occurs.  Since we have <b>N</b> words total, then 
														<div style="width:100px;">
															\[  k_1 + k_2 + ... + k_M = N \]
														</div>
														
													</li>
												</ol>
											</p>

											<p>
												If <b>x_j</b> are the counts we actually witness in a string, we seek the following posterior likelihoods
											</p>

											<div>
												<h3>
													 \[ \ell(C_i | X) = p(x_1 = k_1, x_2 = k_2, ..., x_M = k_M | C_i) P(C_i)\]
												</h3>
											</div>


											<p>
												As an example, suppose we had a bag of words model with only two words: {Computer, Science}.  Let's suppose we make an observation that has 5 words, with 2 counts of "computer" and 3 counts of "science."  For example, the following sentence would match the above counts
											</p>

											<h4>s = "Science Computer Science Computer Computer"</h4>

											<p>
												Let <b>p<SUB>c</SUB></b> be the probability of drawing a "computer" and <b>p<SUB>s</SUB></b> be the probability of drawing a "science."  Since the events are assumed to be independent in Naive Bayes, we can simply multiply the probabilities together, yielding the following probability 
											</p>

											<div>
												<h3>
													\[ P(s | C) = ps \times pc \times ps \times pc \times pc =  p_c^3 ps^2 \]
												</h3>
											</div>

											<p>
												Notice, though, that there are many strings that would satisfy these probabilities; for instance,
											</p>

											<h4>s = "Science Science Computer Computer Computer"</h4>

											<p>
												Each one of these strings is a <b>disjoint event</b>, so we can <b>add up their probabilities</b>.  In fact, there are 10 possible 5-word sentences that have 3 "computer" strings and 2 "science" strings, so, letting <b>k1</b> count the "computer" strings and <b>k2</b> count the "science" string
											</p>


											<div>
												<h3>
													\[ P(k_1=3, k_2=2 | C) = 10 p_c^3 p_s^2  \]
												</h3>
											</div>
											
											<p>
												In general, the counts can be obatined from <a href = "https://en.wikipedia.org/wiki/Multinomial_theorem#Multinomial_coefficients">multinomial coefficients</a>.  The formula turns out like this
											</p>

											<h3>
												\[  \ell (x_1 = k_1, x_2 = k_2, ... , x_M = k_M | C_i) = \left( \left( \frac{N!}{k_1! k_2! ... k_M!}  \right) p_{i1}^{k_1} p_{i2}^{k2} ... p_{iM}^{kM} \right)  p(C_i) \]
											</h3>

											<p>
												Here's an example where taking the log posterior likelihood really helps us, because we have a ton of things multiplied together which we can split into sums
											</p>

											<h3>
												\[  \log \ell (X | C_i) = \log \left( \frac{N!}{k_1! k_2! ... k_M!}  \right) + \log \left(p_{i1}^{k_1} p_{i2}^{k2} ... p_{iM}^{kM} \right)  + \log( p(C_i))  \]
											</h3>

											<p>
												In fact, we can break this apart even further by remembering the rules lob(a+b) = log(a) + log(b) and log(a<SUP>b</SUP>) = b log(a)
											</p>

											<h3>
												\[ \log \ell (X | C_i) = \log \left( \frac{N!}{k_1! k_2! ... k_M!}  \right) + k_1 \log(p_{i_1}) + k_2 \log(p_{i_2}) + ... + k_M \log(p_{iM}) + \log( p(C_i)) \]
											</h3>

											<p>
												If we're simply trying to find the max of the log posterior likelihood, then the ugly first term doesn't actually affect us at all, and we can ignore it!  So we can define a new quantity
											</p>

											<h3>
												\[ h_i(X) = k_1 \log(p_{i_1}) + k_2 \log(p_{i_2}) + ... + k_M \log(p_{iM}) + \log( p(C_i)) \]
											</h3>

											<p>
												This is nearly exactly what we actually did in our code in the bag of words exercise.  The only extra term is <b>log(p(C<SUB>i</SUB>))</b>.  If a class has a small prior, it is inherently less likely, so we can see this as a penalty or promoter for how likely the class is in the absence of any other evidence (for instance, knowing nothing else, your guess that my CS 477 lectures will be awesome is more likely than not 
												ðŸ˜‰).  Since we ignored this in the <a href = "#solution">code above</a>, we were assuming that Trump and Hillary were equally likely options.
											</p>

											






                                    
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#readings">Readings</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
													<ul>
														<li><a href = "../../Assignments/HW2_RushHour/competition.html">Competition Results</a> </li>
													</ul>
												</li>
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<li>
													<a href = "../../Assignments/HW4_RobotLocalization">HW4: Bayesian Robot Localization</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5a_3DShapeCluster">HW5a: 3D Shape Clustering</a>
												</li>
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_WhatIsAI">Week 1: What Is AI?</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_COVID">Week 1: Monte Carlo COVID Simulation</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_COVID/solution.html">Solution</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/Module2/Video1">Week 5: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Exercise / Theory of Bayesian Classifiers</a>
													<ul>
														<li><a href = "../../ClassExercises/Week5_BagOfWords#exercise">Text Classification Exercise</a></li>
														<li><a href = "../../ClassExercises/Week5_BagOfWords#theory">Naive Bayes Theory</a></li>
													</ul>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/Module3/Exercise0">Week 5: Bayes Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_GradSchoolAdmissions">Week 6: Gaussian Naive Bayes And Grad School Admissions</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/MatrixModule/Video1">Week 8: Matrix Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week8_NMF">Week 8: Nonnegative Matrix Factorization</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week9_KMeans3DShapes">Week 9/10: KMeans Clustering, Applications To Image Processing</a>
												</li>
											</ul>
										</li>
										<li><a href = "../../Ethics/index.html">Ethics Reding / Discussions</a></li>
										<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
