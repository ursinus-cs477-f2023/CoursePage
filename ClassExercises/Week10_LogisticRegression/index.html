<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2021</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2021</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Week 10/11: Logistic Regression</h2>
                                        <h3><a href = "http://www.ctralie.com">Chris Tralie</a></h3>
									</header>

									<div id="page-content">

										<h2>Step Function Regression for 1D Data And Grid Search</h2>
										<p>
											Let's consider the following supervised learning problem.  We have data with a single coordinate and two classes we want to sort it into: blue dots (0) or orange stars (1), as shown below:
										</p>

										<img src = "1DMotivation.svg" width=700>

										<p>
											For our model, let's try to create a function of a single variable <b>x</b> that returns a 0 for some ranges of <b>x</b> where we find the blue dots, and a 1 for other ranges of <b>x</b> where we find the orange dots.  The simplest possible function we could define is a step function
 										</p>

										<div style="width:200px;">
											<h3>
												\[ s_a(x) = \left\{  \begin{array}{cc} 1 & x > a \\ 0 & x \leq a \end{array} \right\} \]
											</h3>
										</div>

										<p>
											In other words, we want to find a single threshold <b>a</b> above which we believe we have an orange star, and below which we've decided we have a blue dot.  We shouldn't expect to get this 100% correct on the above example since they're mixed together a bit, but this is not a bad model since most of the orange stars are to the right of most of the blue dots.
										</p>

										<p>
											To score how well different <b>a</b> values work across all of our data points <b>(x<SUB>i</SUB>, y<SUB>i</SUB>)</b> (where <b>y<SUB>i</SUB></b> is the label, either 0 or 1, that <b>x<SUB>i</SUB></b> has), we can use a <b>squared loss loss function</b> which is the sum of the squared distances between <b>f<SUB>a</SUB>(x<SUB>i</SUB>)</b> and the true values of the dots.  This loss would look like this 
										</p>

										<div style="width:200px;">
											<h3>
												\[ L(a) = \sum_{i = 1}^N (s_a(x_i) - y_i)^2 \]
											</h3>
										</div>

										<p>
											In the case of the step function on the above data, this means we add a penalty of 1 for every point that is misclassified.  If we think back to calc 1, the way that we usually minimize functions is by finding critical points.  Most of the critical points are found by setting the derivative equal to 0 and solving.  The problem with the step function, though, is that it's non-differentiable, so this becomes a pain.  We can instead use something called <b>grid search</b> where we try a whole bunch of values evenly spaced over an interval and see which one leads to a min (an even smarter thing to do in 1D is to use <a href = "https://en.wikipedia.org/wiki/Golden-section_search#Iterative_algorithm">golden section search</a>).  Here's some code that does grid search with 1000 points over the interval [-1, 8] 
										</p>

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											a_s = np.linspace(-2, 8, 1000)
											min_loss = np.inf
											min_a = np.inf
											for a in np.linspace(-1, 8, 1000):
												loss_a = np.sum(np.abs(0.5*(1+np.sign(x-a))-y))
												if loss_a < min_loss:
													min_loss = loss_a
													min_a = a
										</script>  



										<p>
											If we do this on the above example and plot the loss function at each <b>a</b> we try, we get this:
										</p>

										<img src = "StepLoss.svg" width=400>

										<p>
											There are a few global mins at a loss of 6, one of which occurs at around <b>a = 2.7</b>.  As shown below, if we plot the value of the step function versus the ground truth value of our data (class 0 or class 1), we can see that 3 orange points got misclassified and 3 blue points got misclassified.
										</p>

										<img src = "StepFit.svg" width=500>

										<p>
											This seems like it worked out pretty well!  The problem is that it doesn't scale if we're trying to fit a model that has more than one parameter.  For example, if we have two parameters and we likewise divvy up the range into 1000 evenly spaced regions for each parameter, we have to try a million parameter combinations.  If we have 3 parameters, we have to try a billion.  And in general, if we take <b>N</b> samples of each parameter and we have <b>P</b> parameters, we will have to try <b>N<SUP>P</SUP></b> parameter combinations!  This is known as the <b>curse of dimensionality</b>.  To give you an idea of how bad this is, when we go to classify images, we will have on parameter for pixel.  In a fairly small 100x100 image, this is 10,000 pixels.  If we try even a paltry 10 unique parameter values in grid search, we get <b>10<SUP>10,000</SUP></b> different combinations, which is <b>beyond astronomical</b>.  We're definitely going to need a more sophisticated technique.
										</p>


										<HR>
										<h2>1D Logistic Regression</h2>
										<p>
											Let's now try to come up with something that acts like the step function but which is differentiable.  We'll use something called the <b>logistic function</b>, which is defined as 
										</p>

										<div style="width:200px;">
										<h3>
											\[ f(u) = \frac{1}{1+e^{-u}} \]
										</h3>
										</div>

										<p>
											If you plot it, you'll see it's like a step function that makes a smooth transition from 0 to 1
										</p>

										<img src = "Logisticfn.svg" width=500>

										<p>
											This means it acts like the function we were looking for that chooses a boundary between the objects in class 0 and class 1, but it's differentiable.  If we use the chain rule, we can compute its derivative as 
										</p>

										<div style="width:200px;">
											<h3>
												\[ \frac{d f(u)}{du} = \frac{1}{1+e^{-u}} = \frac{e^{-u}}{(1+e^{-u})^2} \]
											</h3>
										</div>

										<p>
											Actually, there's a really slick way to rewrite this as just the following (which you can check with some algebraic manipulations)
										</p>

										<div style="width:200px;">
											<h3>
												\[ \frac{d f(u)}{du} = f(u)(1-f(u)) \]
											</h3>
										</div>

										<p>
											Now let's think about how we might fit the logistic function to our data.  Actually, we'll allow ourselves to have two parameters now: one called <b>a</b> which controls the sharpness of the increase from 0 to 1, and one called <b>b</b> which controls a horizontal shift (at this point you may want to stop and review how <a href = "https://www.geogebra.org/m/yxa2dw5x">to stretch out and horizontally shift functions</a>). In particular, we'll consider how the equation
										</p>

										<div style="width:200px;">
											<h3>
												\[ f(ax + b) =  \frac{1}{1+e^{-ax - b}} \]
											</h3>
										</div>

										<p>
											best fits our data.  Then we can define our <b>squared loss</b> loss function as follows
										</p>

										<div style="width:200px;">
											<h3>
												\[ L(a, b) = \sum_{i=1}^N \left( \frac{1}{1+e^{-ax_i - b}} - y_i \right)^2 \]
											</h3>
										</div>

										<p>
											We want to minimize this function.  As an example of what this looks like, let's fix <b>a = 1</b> and look at grid search again over the horizontal shift parameter <b>b</b>
											
										</p>
										<img src = "LogisticLossGrid.svg">

										<p>
											But let's see what we can do towards optimizing both parameters jointly with a more sophisticated technique. In class, we saw how instead of doing a full on grid search, we could look in a small neighborhood around each point and walk in the direction of where the function seems to be decreasing, and then to repeat this process.  One fundamental optimization technique for doing mathematically is <b>gradient descent</b>, whereby we compute the derivative of our loss with respect to each parameter, treating everything else as a constant.  This is referred to as the <b>partial derivative  &part;</b> with respect to each parameter.  For example, let's consider the partial derivative of our loss with respect to the parameter <b>a</b>.  We can obtain this with multiple applications of the chain rule, noting that everything other than <b>a</b> is a constant
										</p>

										<div style="width:200px;">
											<h3>
												\[ \frac{\partial L(a, b)}{\partial a} = \sum_{i = 1}^N \frac{\partial (f(ax_i+b) - y_i)^2}{\partial a} = \sum_{i=1}^N 2(f(ax_i+b) - y_i) \frac{\partial f(ax_i+b)}{\partial a} \]
											</h3>
											<h3>
												\[ \frac{\partial L(a, b)}{\partial a} = 2(f(ax_i+b) - y_i) f(ax_i+b)(1-f(ax_i+b)) \frac{\partial ax_i+b}{\partial a}\]
											</h3>
											<p>
												and finally
											</p>
											<h3>
												\[  \frac{\partial L(a, b)}{\partial a} = \sum_{i=1}^N 2x_i (f(ax_i+b) - y_i) f(ax_i+b)(1-f(ax_i+b)) \]
											</h3>
										</div>

										<p>
											By a similar procedure, we find that the partial derivative with respect to <b>b</b> is 
										</p>

										<div style="width:200px;">
											<h3>
												\[  \frac{\partial L(a, b)}{\partial b} = \sum_{i=1}^N 2(f(ax_i+b) - y_i) f(ax_i+b)(1-f(ax_i+b)) \]
											</h3>
										</div>

										<p>
											Since the derivative tells us how the function is increasing, we want to walk along the <b>opposite direction</b>.  Therefore, we will update our parameters <b>a</b> and <b>b</b> by <i>subtracting</i> some nonnegative constant <b>&mu;</b> times the derivative.  This parameter <b>&mu;</b> is sometimes referred to as the <b>learning rate</b>
										</p>

										<div style="width:200px;">
											<ul>
												<li>
													\[ a \gets  a - \mu \frac{\partial L(a, b)}{\partial a}\]
												</li>
												<li>
													\[ b \gets  b - \mu \frac{\partial L(a, b)}{\partial b}\]
												</li>
											</ul>
										</div>


										<p>
											Here's what gradient descent looks like after modifying the <a href = "https://github.com/Ursinus-CS477-F2021/Week10_LogisticRegression/blob/main/3-Logistic1DGradient.ipynb">starter code here</a>, with a step <b>&mu; = 0.1</b>
										</p>

										<img src = "Step0.1.gif">

										<p>
											This step size is a bit large and causes it to zig zag.  Here is a smaller step size <b>&mu; = 0.01</b>, which moves more slowly but which does not cause the same zig zagging:
										</p>

										<img src = "Step0.01.gif">

										<p>
											We should be mindful of this tradeoff as we choose our learning rate.  
										</p>


										<HR>
											<h2>Separating Lines in 2D / Higher Dimensional Logistic Regression</h2>

											<p>
												We can extend these ideas to higher dimensions.  The very next dimension up leads us to a procedure for separating lines in 2D.  Here, the boundary decision boundary is described with the equation <b>ax + by + c = 0</b>, so there are three parameters: <b>a</b>, <b>b</b>, and <b>c</b>.  Let's now suppose the class labels of our planar data points <b>(x<SUB>i</SUB>, y<SUB>i</SUB>)</b> are called <b>z<SUB>i</SUB></b> to avoid a variable name collision.  This leads us towards minimizing the loss function 
											</p>

											<div style="width:200px;">
												<h3>
													\[ L(a, b, c) = \sum_{i=1}^N \left( \frac{1}{1+e^{-ax_i - by_i - c}} - z_i \right)^2 \]
												</h3>
											</div>

											<p>
												We can work through similar math to find the following update rules for gradient descent
											</p>

											<div style="width:200px;">
												<ul>
													<li>
														\[ a \gets  a - \mu \frac{\partial L(a, b, c)}{\partial a}\]
													</li>
													<li>
														\[ b \gets  b - \mu \frac{\partial L(a, b, c)}{\partial b}\]
													</li>
													<li>
														\[ c \gets  c - \mu \frac{\partial L(a, b, c)}{\partial c}\]
													</li>
												</ul>
											</div>

											<p>
												where
											</p>

											<div style="width:200px;">
												<h3>
													\[  \frac{\partial L(a, b, c)}{\partial a} = \sum_{i=1}^N 2x_i (f(ax_i+by_i+c) - z_i) f(ax_i+by_i+c)(1-f(ax_i+by_i+c)) \]
												</h3>
												<h3>
													\[  \frac{\partial L(a, b, c)}{\partial b} = \sum_{i=1}^N 2y_i (f(ax_i+by_i+c) - z_i) f(ax_i+by_i+c)(1-f(ax_i+by_i+c)) \]
												</h3>
												<h3>
													\[  \frac{\partial L(a, b, c)}{\partial c} = \sum_{i=1}^N 2 (f(ax_i+by_i+c) - z_i) f(ax_i+by_i+c)(1-f(ax_i+by_i+c)) \]
												</h3>
											</div>
											



											<p>
												Here's an example of a working implementation of gradient descent on 2D data starting from <a href = "https://github.com/Ursinus-CS477-F2021/Week10_LogisticRegression/blob/main/4-Logistic2DGradient.ipynb">this file</a>
											</p>

											<img src = "2DExample1.gif">

											<p>
												Here's another example where the line started off in the backwards orientation
											</p>

											<img src = "2DExample2.gif">

											<p>
												Here's an example where the data isn't separable
											</p>
											<img src = "2DExample3.gif">

											<h3>Higher Dimensions</h3>
											<p>
												In general, we can run logistic regression on <b>d</b> independent variables <b>x<SUB>1</SUB></b>, <b>x<SUB>2</SUB></b>, ... <b>x<SUB>d</SUB></b> by fitting with the logistic function composed with 
											</p>
											<div style="width:200px;">
												\[ w_1x_1 + w_2x_2 + ... + w_dx_d + c \]
											</div>

											<p>In that case, we can update all of the <b>w</b>'s as </p>
										

											<h3>
												\[  \frac{\partial L(w_1, w_2, ..., w_d, c)}{\partial w_k} = \sum_{i=1}^N 2 x_k( f(\vec{w}, c)  - z_i) f(\vec{w}, c)(1-f(\vec{w}, c)) \]
											</h3>

											<h3>
												\[  \frac{\partial L(w_1, w_2, ..., w_d, c)}{\partial c} = \sum_{i=1}^N 2 ( f(\vec{w}, c)  - z_i) f(\vec{w}, c)(1-f(\vec{w}, c)) \]
											</h3>

											<p>
												where 
											</p>

											<h3>
												\[  f(\vec{w}, c) = f( w_1x_1 + w_2x_2 + ... + w_dx_d + c ) \]
											</h3>

											<p>
												This is what you'll be doing with the movie reviews on homework 6
											</p>


                                    
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#readings">Readings</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
													<ul>
														<li><a href = "../../Assignments/HW2_RushHour/competition.html">Competition Results</a> </li>
													</ul>
												</li>
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<li>
													<a href = "../../Assignments/HW4_RobotLocalization">HW4: Bayesian Robot Localization</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5a_3DShapeCluster">HW5a: 3D Shape Clustering</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5b_Unsupervised">HW5b: NMF for Music Component Separation</a>
												</li>
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_WhatIsAI">Week 1: What Is AI?</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_COVID">Week 1: Monte Carlo COVID Simulation</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_COVID/solution.html">Solution</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/Module2/Video1">Week 5: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Exercise / Theory of Bayesian Classifiers</a>
													<ul>
														<li><a href = "../../ClassExercises/Week5_BagOfWords#exercise">Text Classification Exercise</a></li>
														<li><a href = "../../ClassExercises/Week5_BagOfWords#theory">Naive Bayes Theory</a></li>
													</ul>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/Module3/Exercise0">Week 5: Bayes Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_GradSchoolAdmissions">Week 6: Gaussian Naive Bayes And Grad School Admissions</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/MatrixModule/Video1">Week 8: Matrix Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week8_NMF">Week 8: Nonnegative Matrix Factorization</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week9_KMeans3DShapes">Week 9/10: KMeans Clustering, Applications To Image Processing</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_VisualBOW/index.html">Week 10: Visual Bag of Words</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_LogisticRegression/index.html">Week 10/11: Logistic Regression And Gradient Descent</a>
												</li>
											</ul>
										</li>
										<li><a href = "../../Ethics/index.html">Ethics Reading / Discussions</a></li>
										<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
