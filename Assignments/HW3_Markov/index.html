<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
										<h2>Homework 3: Markov Chains for Text Processing</h2>
                                        <h3><a href = "http://www.ctralie.com">Chris Tralie</a></h3>
										
										<ul>
											<li><a href = "#overview">Overview/Logistics</a>
												<ul>
													<li><a href = "#objectives">Learning Objectives</a></li>
													<li><a href = "#readme">What To Submit</a></li>
												</ul>
											</li>
											<li><a href = "#markov">Background: Markov Chains for Text Representation</a>
												<ul>
													<li><a href = "#zeroorder">Zero-Order Statistical Modeling</a></li>
													<li><a href = "#kprefix">K-Prefixes</a></li>
													<li><a href = "#probability">The Probability of A Sequence</a></li>
												</ul>
											</li>
											<li>
												<a href = "#tasks">Assignment Tasks</a>
												<ul>
													<li><a href = "#startercode">Starter Code</a></li>
													<li>
														<a href = "#addingtext">Data Structures / Adding Text (10 Points)</a>
													</li>
													<li>
														<a href = "#computingprobabilities">Computing Probabilities (10 Pts)</a>
													</li>
													<li>
														<a href = "#debates">Experiment 1: 2016 US Presidental Debate (10 Pts)</a>
													</li>
													<li>
														<a href = "#spamham">Experiment 2: Spam vs Ham (5 Pts)</a>
													</li>
													<li>
														<a href = "#moviereview">Experiment 3: Movie Reviews (5 Points)</a>
													</li>
													<li>
														<a href = "$synthesizing">Synthesizing Text (10 Pts)</a>
													</li>
													<li>
														<a name = "#poeticstatement">Poetic Statement (5 Pts)</a>
													</li>
													<li>
														<a name = "#ethics">Ethics Reflection (5 Pts)</a>
													</li>
												</ul>
											</li>
										</ul>

									</header>

									<div id="page-content">

                                        <h2><a name = "overview">Overview / Logistics</a></h2>
										<p>
                                            The purpose of this assignment is to introduce you to natural language processing via <a href = "https://en.wikipedia.org/wiki/Markov_chain">Markov chains</a>, which are incredibly powerful tools for representing sequences mathematically.  First, you will use this tool to classify text, seeing how it performs at telling whether Clinton or Trump was speaking during the 2016 presidential debates, whether a text is spam or ham, or whether a movie review is positive or negative.  You will also use this tool to synthesize new text that mimics the style of some given text.  For instance, you'll be able to synthesize new, previously unheard Spongebob quotes.  You can even mix styles together (such as my <a href = "#ursinussith">Ursinus sith lord</a>).  We will have a class gallery of such "Markov hybrids" that students devise.
										</p>

                                        <p>
                                            <h3><a name = "objectives">Learning Objectives</a></h3>
                                            <ul>
												<li>Implement a statistical technique for analysis and synthesis of natural language.</li>
												<li>Follow the train set / test set paradigm for evaluating the performance of an algorithm, and learn how to "wrangle datasets" (i.e. load files and prepare them for testing).</li>
												<li>Search for optimal model parameters, and explore how these parameters vary across datasets.</li>
												<li>Use jupyter notebooks to organize experiments into a <a href = "https://writings.stephenwolfram.com/2017/11/what-is-a-computational-essay/">computational essay</a>.</li>
												<li>Reflect on the potential harms of technology that you've created, and what can be done to mitigate ethical threats.</li>
                                            </ul>
                                        </p>

                                        <h3><a name = "readme">What to submit</a></h3>
										<p>
                                            
											Finally, submit a comment on canvas with the answer to the following questions:
										
										<ol>
											<li>
												Your markov.py file and the jupyter notebooks you created in the experimental section.
											</li>
											<li>
												Your <a href = "#poeticstatment">poetic statement</a>, a title that should go along with it, and what name/pseudonym you would like to use? (results will be displayed on the class web site).  Also submit the text files that you trained it on.
											</li>
											<li>
												Your <a href = "#ethics">ethics reflection</a>.
											</li>
											<li>
												Did you work with a <a href = "../../index.html#buddy">buddy</a>?  If so, who?
											</li>
											<li>
												How long did this take you? I'm simply using this to gauge if things are too easy, too hard, or just right. 
											</li>
											<li>
												Any feedback to help improve the assignment for future students? Alternatively, if there's nothing you would change, let me know what specifically worked for you about it so I know to keep doing things like that. 
											</li>
											<li>
												Any other concerns that you have. For instance, if you have a bug that you were unable to solve but you made progress, write that here. The more you articulate the problem the more partial credit you will receive (fine to leave this blank)
											</li>
										</ol>
									</p>


									<h2><a name = "markov">Background: Markov Chains for Text Representation</a></h2>

									<p>
										Human languages, often referred to as ``natural languages,'' are rich and full of meaning.  Compared to computer languages, they are much harder to automatically interpret and process.  However, as you will see in this assignment, we can make some progress at representing natural languages using a few simple statistical ideas.
									</p>

									<p>
										In this assignment, our focus will be on creating an algorithm to capture the <i>style</i> of some given text from a set of examples.  Then, we will be able to classify documents or "synthesize" (create new text) in this style.
									</p>

									<p>
										In the discussion below, we refer to documents that we use to build a statistical model as <b>training data</b>.
									</p>

									<h3><a name = "zeroorder">Zero-Order Statistical Modeling</a></h3>

									<p>
										We'll start by discussing synthesis.  The simplest possible thing we could do to model text statistically is try to match the frequency of occurrence of the characters in a document.  For example, if we use a collection of <a href = "HW3_Markov/text/spongebobquotes.txt">spongebob quotes</a>, we find that we have the following character counts across all quotes:
									</p>

									<p>
										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										{' ': 869, '!': 35, ''': 62, ',': 79, '-': 4, '.': 90, '0': 3, 
										'1': 2, '2': 4, '4': 1, '5': 1, '?': 21, 'A': 8, 'B': 4, 'C': 8, 'D': 8, 
										'E': 6, 'F': 2, 'G': 5, 'H': 8, 'I': 55, 'K': 3, 'L': 3, 'M': 3, 'N': 7, 
										'O': 5, 'P': 11, 'R': 5, 'S': 22, 'T': 13, 'U': 2, 'V': 1, 'W': 14, 
										'Y': 9, 'a': 305, 'b': 91, 'c': 83, 'd': 133, 'e': 453, 'f': 70, 'g': 85, 
										'h': 163, 'i': 239, 'j': 6, 'k': 50, 'l': 170, 'm': 126, 'n': 241, 
										'o': 350, 'p': 63, 'q': 7, 'r': 233, 's': 242, 't': 314, 'u': 157, 
										'v': 36, 'w': 89, 'x': 4, 'y': 139, 'z': 5}</script>   

									</p>

									<p>
										Then, we can then try drawing characters according to these counts.  For example, we'd be more likely to draw an 'i' than we would to draw a 'z', and we're more likely to get a space ' ' than any other character.  If we draw 100 characters independently with these probabilities, we get text that looks like this:
										<ul>
											<li>onc 5donps fwyce  agoti'tm  ne  edoi a e Iueogd ei IralcrltItmo.g mimyheat tgesue  nwierayekra fP he</li>
											<li>l   rOxttsu Iogrmetucafo ewa khtois!e bttcatnht,r Cyfhr  Pngkswhnwl oiet lyoatrl atumr e lenriadb Ie</li>
											<li>Gi dyuh   b .di Po mmceooet'd'nne'n gdo dkimeo aanti is0o i 'uttj'Sstopfsasep!.  mosltayaaSso?lraV l</li>
										</ul>
									</p>
									<p>
										Interestingly, the size of the "words" looks about right since we are more likely to choose a space than any other character, but they are total gibberish. This is because the process has no memory; each character is chosen completely independently from the character preceding it.    

									<h3><a name = "kprefix">K-Prefixes</a></h3>
									<p>
										As it turns out, if we have just a very simple memory of a few characters, we can do a much better job at synthesizing sequences.  Surprisingly, we can shoehorn the "memoryless" Markov chain framework from our <a href = "../../ClassExercises/Week4_Markov/">class exercise</a> into having a small memory if we change our notion of what a "state" is.  In particular, a "state" will now be a small sequence of characters of length <b>k</b> instead of just a single character.  This is referred to as a <b>k-prefix</b>.
									</p>

									<p>
										As an example, let's consider that we had the string <b>aaabaaacaabbcabcc</b>.  If we took all of the 2-prefixes of this string, we would have the following counts
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										{'aa': 5, 'ab': 3, 'ba': 1, 'ac': 1, 'ca': 2, 'bb': 1, 'bc': 2}</script>   

									<p>
										In the Markov framework, we also have transitions that occur every time a new character is seen.  On the above example, they would look like this:
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										{
											'aa': {'a': 2, 'b': 2, 'c': 1}, 
											'ab': {'a': 1, 'b': 1, 'c': 1}, 
											'ba': {'a': 1}, 
											'ac': {'a': 1}, 
											'ca': {'a': 1, 'b': 1}, 
											'bb': {'c': 1}, 
											'bc': {'a': 1, 'c': 1}
										}</script>   

									<p>
										Let's think about what these mean as transitions in the Markov chain.  If we're at <b>aa</b> and we get an <b>a</b>, then we stay at <b>aa</b>.  If we get a <b>b</b>, then we chop the <b>a</b> off of the left and move to <b>ab</b>.  If we're at <b>ca</b> and we see a <b>b</b>, we chop the <b>c</b> off of the left and move to <b>ab</b>.  And so on.  Drawn as a state diagram, all such transitions look like this:
									</p>

									<img src = "Example1.png">

									<p>
										One issue with this is that it runs into a dead end at <b>cc</b>.  This is fine if we're analyzing <a href = "#probability">probabilities</a>, but if we're doing a random walk through our model to synthesize text, we'll run into a dead end there.  To make it so we can keep going, we'll loop the text around by padding the end of the string with the first <b>k</b> characters.  In this example, this means padding the original string with the first two characters, so we create the prefixes of the string <b>aaabaaacaabbcabcc<span style="color: red">aa</span></b>.  That leads to the following counts for the prefixes and transitions
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										{'aa': 5, 'ab': 3, 'ba': 1, 'ac': 1, 'ca': 3, 'bb': 1, 'bc': 2, 'cc': 1}
									</script>   

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										{
											'aa': {'a': 2, 'b': 2, 'c': 1}, 
											'ab': {'a': 1, 'b': 1, 'c': 1}, 
											'ba': {'a': 1}, 
											'ac': {'a': 1}, 
											'ca': {'a': 2, 'b': 1}, 
											'bb': {'c': 1}, 
											'bc': {'a': 1, 'c': 1}, 
											'cc': {'a': 1}
										}</script>   
									<p>
										Notice how the count of <code>ca->a</code> has gone up by 1, and now there is a transition from <code>cc->a</code>.  Now there are no dead ends in the state transitions, and we can walk through this model randomly forever to generate random strings.
									</p>

									<img src = "Example1_Loop.png">

										


									

									<p>Below is a more in-depth example with some real text from the <a href = "HW3_Markov/text/spongebobquotes.txt">spongebob quotes text file</a> provided with the assignment</p>
									<p></p><p></p>

									<button type="button" onclick="showSpongebob()">Show Spongebob Example</button> 
									<button type="button" onclick="hideSpongebob()">Hide Spongebob Example</button> 
									<div id="spongebobtext" style="background-color: blanchedalmond; padding: 20px;">

										<h4><a name = "spongebobexample">Spongebob Example</a></h4>
										<p>
											As an example here are all of the counts of the 4-prefix <b>'you '</b> in the Spongebob text:
										</p>

										<table>
											<tr><td>New Sentence</td><td>Character Counts for 'you ' After Sentence</td></tr>
											<tr><td>Gary, go away, can't <u>you </u><b>s</b>ee I'm trying to forget you?</td><td>{"s":1}</td></tr>
											<tr><td>Why are <u>you </u><b>m</b>ad? Because I can't see my forehead!</td><td>{"s":1, "m":1}</td></tr>
											<tr><td>Can <u>you </u><b>t</b>ake the crust off my Krabby Patty?</td><td>{"s":1, "t":1, "m":1}</td></tr>
											<tr><td>Did <u>you </u><b>s</b>ee my underwear?</td><td>{"s":2, "t":1, "m":1}</td></tr>
											<tr><td>Well, it makes <u>you </u><b>l</b>ook like a girl!</td><td>{"s":2, "t":1, "l":1, "m":1}</td></tr>
											<tr><td>East, oh I thought <u>you </u><b>s</b>aid Weest!</td><td>{"s":3, "t":1, "l":1, "m":1}</td></tr>
											<tr><td>That's it, mister, <u>you </u><b>j</b>ust lost your brain priveleges!</td><td>{"s":3, "t":1, "j":1, "l":1, "m":1}</td></tr>
											<tr><td>I wumbo, <u>you </u><b>w</b>umbo, he she we, wumbo, wumboing, wumbology, the study of wumbo? </td><td>{"s":3, "t":1, "w":1, "j":1, "l":1, "m":1}</td></tr>
											<tr><td>It's not <u>you </u><b>t</b>hat's got me... it's me that's got me!</td><td>{"s":3, "t":2, "w":1, "j":1, "l":1, "m":1}</td></tr>
											<tr><td>Why don't <u>you </u><b>a</b>sk CowBob RanchPants and his friend Sir Eats-a-lot?</td><td>{"a":1, "s":3, "t":2, "w":1, "j":1, "l":1, "m":1}</td></tr>
											<tr><td>Krusty Krab Pizza, it's the pizza for <u>you </u><b>a</b>nd meeeee!</td><td>{"a":2, "s":3, "t":2, "w":1, "j":1, "l":1, "m":1}</td></tr>
											<tr><td>If <u>you </u><b>b</b>elieve in yourself, with a tiny pinch of magic all your dreams can come true!</td><td>{"a":2, "b":1, "s":3, "t":2, "w":1, "j":1, "l":1, "m":1}</td></tr>
											<tr><td>Goodbye everyone, I'll remember <u>you </u><b>a</b>ll in therapy.</td><td>{"a":3, "b":1, "s":3, "t":2, "w":1, "j":1, "l":1, "m":1}</td></tr>
											<tr><td>Don't <u>you </u><b>h</b>ave to be stupid somewhere else?</td><td>{"a":3, "b":1, "s":3, "t":2, "w":1, "h":1, "j":1, "l":1, "m":1}</td></tr>
											<tr><td>Squidward, <u>you </u><b>c</b>an't eat all those patties at one time!</td><td>{"a":3, "b":1, "s":3, "c":1, "t":2, "w":1, "h":1, "j":1, "l":1, "m":1}</td></tr>
											<tr><td>I'll have <u>you </u><b>k</b>now, I stubbed my toe last week, while watering my spice garden, and I only cried for 20 minutes.</td><td>{"a":3, "b":1, "s":3, "c":1, "t":2, "w":1, "h":1, "j":1, "k":1, "l":1, "m":1}</td></tr>
											<tr><td>Squidward, <u>you </u><b>a</b>nd your nose will definitely not fit in.</td><td>{"a":4, "b":1, "s":3, "c":1, "t":2, "w":1, "h":1, "j":1, "k":1, "l":1, "m":1}</td></tr>
											<tr><td>Who <u>you </u><b>c</b>allin' pinhead?</td><td>{"a":4, "b":1, "s":3, "c":2, "t":2, "w":1, "h":1, "j":1, "k":1, "l":1, "m":1}</td></tr>
											<tr><td>Gee Patrick, I didn't know <u>you </u><b>c</b>ould speak bird.</td><td>{"a":4, "b":1, "s":3, "c":3, "t":2, "w":1, "h":1, "j":1, "k":1, "l":1, "m":1}</td></tr>
											<tr><td>Any particular reason <u>you </u><b>t</b>ook your pants off.</td><td>{"a":4, "b":1, "s":3, "c":3, "t":3, "w":1, "h":1, "j":1, "k":1, "l":1, "m":1}</td></tr>
											<tr><td>Let me get this straight, <u>you </u><b>t</b>wo ordered a giant screen television just so you could play in the box?</td><td>{"a":4, "b":1, "s":3, "c":4, "t":4, "w":1, "h":1, "j":1, "k":1, "l":1, "m":1}</td></tr>
											<tr><td>I'd hate <u>you </u><b>e</b>ven if I didn't hate you.</td><td>{"a":4, "b":1, "s":3, "c":4, "t":4, "e":1, "w":1, "h":1, "j":1, "k":1, "l":1, "m":1}</td></tr>
											<tr><td>You're a man now, Spongebob, and it's time <u>you </u><b>s</b>tarted acting like one.</td><td>{"a":4, "b":1, "s":4, "c":4, "t":4, "e":1, "w":1, "h":1, "j":1, "k":1, "l":1, "m":1}</td></tr>
											<tr><td>Can <u>you </u><b>g</b>ive Spongebob his brain back, I had to borrow it for a week.</td><td>{"a":4, "b":1, "c":4, "e":1, "g":1, "h":1, "j":1, "k":1, "l":1, "m":1, "s":4, "t":4, "w":1}</td></tr>
											
										</table>

										<h4>Synthesizing Text</h4>Now, let's say we do the following steps, starting with the prefix <b>'you '</b>:
										<ol>
											<li>We randomly choose one of the characters that's to follow, and we choose a <b>'c'</b></li>
											<li>We then slide over by one character move onto the next prefix, which is <b>'ou c'</b>.  We then see the character counts {"a":2, "o":2} for that prefix.</li>
											<li>We make another random choice at this point, and we draw the character <b>'a'</b>. So then we slide onto the prefix <b>'u ca'</b>, and we see the counts {"l":1, "n":1} for that prefix.</li>
											<li>We now make a random choice and draw the character <b>'n'</b>.  We then slide over to the prefix <b>' can'</b>, and we see the counts {" ":3, "'":4}</li>
											<li>We now make a random choice and draw a space, so we slide over to the prefix <b>'can '</b>, and we see the counts {"c":1, "h":1, "I":1}</li>
											<li>
												We now make a random choice of an <b>h</b>, moving us to the prefix <b>'an h'</b>, and so on and so forth
											</li>
										</ol>


										So far in this example, we have synthesized the text <b>"you can h"</b>, and we could keep going.  Here are a few different outcomes if we keep following this process:
										<ul>
											<li>you can have facial hair!Now than 24, 25.You don't need a new I stupid</li>
											<li>you can have to die right.You know the true!If your secrets is hot burns down</li>
											<li>you can have feet?Since who can't you.I'd hate you and I'm absorbing</li>
											<li>you can have to be alright.If I wish is nothere ther.No, Patty?</li>
										</ul>

										<p>
											As you can see, this text is starting to make a lot more sense than choosing each character independently, even with a very short memory.
										</p>

									</div>

									<script>
										let spongebob = document.getElementById("spongebobtext");
										function showSpongebob() {
											spongebob.style.display="block";
										}
										function hideSpongebob() {
											spongebob.style.display = "none";
										}
										hideSpongebob();
									</script>


									<p></p><p></p>

									<h3><a name = "probability">The Probability of A Sequence</a></h3>
									<p>
										In addition to synthesizing text in a model trained on k-prefixes from a particular set of documents, we can assess how likely a different document is to be in the style that we've modeled with our Markov chains.  To do this, we will compute the probability of a particular sequence given a model.  Markov chains have a simplifying assumption of <a href = "https://en.wikipedia.org/wiki/Independence_(probability_theory)">independence</a> that will help make this easier to compute.  In particular, the next character is chosen only based on the current prefix, and none of the previous prefixes influence this decision.
									<p>
										Independent events are nice to work with because the probability of independent events occurring is a simple multiplication.  For example, it's reasonable to assume that the type of weather in Beijing on a particular day is independent of the weather in Collegeville.  So if the probability it rains in Collegeville is 0.4 and the probability it rains in Beijing is 0.6, then the joint probability of both occurring is <code>0.4 x 0.6 = 0.24</code>
									</p>

									<p>
										To compute the probability of a particular sequence of characters, we must first compute the probability that each character was drawn under our model, and then we may compute the joint probability of the entire sequence by multiplying them together.  The probability of a particular character <code>c</code> preceded by the prefix <code>p</code> is given as 

										\[ p(c) = \frac{N(p.c) + 1}{N(p)+S} \]

										where 
										
										<ul>
											<li>
												<code>N(p)</code> is the number of times the prefix occurs in the model (which can be 0)
											</li>
											<li>
												<code>N(p.c)</code> is the number of times the character <code>c</code> follows prefix <code>p</code> (which can be 0, and which is automatically 0 if the prefix doesn't exist in the model)
											</li>
											<li>
												<code>S</code> is the size of the alphabet in the model (i.e. the number of unique characters across all prefixes)
											</li>
										</ul>
										So the joint probability of all characters is obtained by multiplying a bunch of these together.  Note that this gracefully handles the case where we never saw a particular substring in any of the training data; in this case <code>N(p.c)</code> and <code>N(p)</code> are 0, and the probability is <code>1/S</code>
									</p>

									<p>
										There is a slight numerical issue with the above scheme, however.  Since there are many characters in most sequences of interest, and since the probabilities of each character are small, we can run into <a href = "https://en.wikipedia.org/wiki/Arithmetic_underflow">arithmetic underflow</a> where the multiplication of many small numbers ends up just bottoming out at zero numerically.  To get around this, we can instead compute the ``log probability''.  Since 
										
										\[ \log(AB) = \log(A) + \log(B) \]

										We can compute the log of the product probabilities as the sum of the log of each probability.  So simply modify the formula as 

										\[ \log \left( p(c) \right) = \log \left( \frac{N(p.c) + 1}{N(p)+S} \right) \]

										and then sum all of these up for each character to get the final log probability.
									</p>

									<h4><a name = "probexample">Simple Example</a></h4>
									<p>
										Let's consider a fully fledge simple example.  Let's suppose that we added the string <b>aaacbaaaacabacbbcabcccbccaaac</b> to the model and we considered all 3-prefixes, so that we have these counts in the model (considering the wrap-around)
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										{'aaa': 4, 'aac': 3, 'acb': 2, 'cba': 1, 'baa': 1,
										 'aca': 2, 'cab': 2, 'aba': 1, 'bac': 1, 'cbb': 1, 
										 'bbc': 1, 'bca': 1, 'abc': 1, 'bcc': 2, 'ccc': 1, 
										 'ccb': 1, 'cbc': 1, 'cca': 1, 'caa': 2}
									
										{
											aaa : {'c': 3, 'a': 1}
											aac : {'b': 1, 'a': 2}
											acb : {'a': 1, 'b': 1}
											cba : {'a': 1}
											baa : {'a': 1}
											aca : {'b': 1, 'a': 1}
											cab : {'a': 1, 'c': 1}
											aba : {'c': 1}
											bac : {'b': 1}
											cbb : {'c': 1}
											bbc : {'a': 1}
											bca : {'b': 1}
											abc : {'c': 1}
											bcc : {'c': 1, 'a': 1}
											ccc : {'b': 1}
											ccb : {'c': 1}
											cbc : {'c': 1}
											cca : {'a': 1}
											caa : {'a': 2}
										}
											
											
									</script>  

									<p>
										Now let's say we wanted to compute the probability of the string <b>aacabaa</b> given the the model.  There are three unique characters in the alphabet here: a, b, and c, so <b>S = <span style="color:darkgreen">3</span></b>.  Then, we compute the probability of each chunk <b>aac, aca, cab, aba</b> (all substrings of length 3 with at least one character following them)
									</p>

									<table>
										<tr>
											<td>p</td><td>N(p)</td><td>c</td><td>N(p.c)</td><td> \[  \log \left( \frac{N(p.c) + 1}{N(p)+S} \right) \]</td>
										</tr>
										<tr>
											<td>
												aac
											</td>
											<td>
												<span style="color:crimson">3</span>
											</td>
											<td>
												a
											</td>
											<td>
												<span style="color:darkblue">2</span>
											</td>
											<td>
												 = log(<span style="color:darkblue">2</span> + 1) / (<span style="color:crimson">3</span> + <span style="color:darkgreen">3</span>) = -0.6931
											</td>
										</tr>
										<tr>
											<td>
												aca
											</td>
											<td>
												<span style="color:crimson">2</span>
											</td>
											<td>
												b
											</td>
											<td>
												<span style="color:darkblue">1</span>
											</td>
											<td>
												 = log(<span style="color:darkblue">1</span> + 1) / (<span style="color:crimson">2</span> + <span style="color:darkgreen">3</span>) = -0.9163
											</td>
										</tr>
										<tr>
											<td>
												cab
											</td>
											<td>
												<span style="color:crimson">2</span>
											</td>
											<td>
												a
											</td>
											<td>
												<span style="color:darkblue">1</span>
											</td>
											<td>
												 = log(<span style="color:darkblue">1</span> + 1) / (<span style="color:crimson">2</span> + <span style="color:darkgreen">3</span>) = -0.9163
											</td>
										</tr>
										<tr>
											<td>
												aba
											</td>
											<td>
												<span style="color:crimson">1</span>
											</td>
											<td>
												a
											</td>
											<td>
												<span style="color:darkblue">0</span>
											</td>
											<td>
												 = log(<span style="color:darkblue">0</span> + 1) / (<span style="color:crimson">1</span> + <span style="color:darkgreen">3</span>) = -1.386
											</td>
										</tr>
									</table>

									<p>
										Summing all of these up, we get <b>-3.91</b>
									</p>





									<h2><a name = "tasks">Assignment Tasks</a></h2>

									<h3><a name = "startercode">Starter Code</a></h3>
									<p>
										<a href = "https://github.com/Ursinus-CS477-F2021/HW3_Markov/archive/refs/heads/main.zip">Click here</a> to download the starter code for this assignment.  The main file you will be editing is <code>markov.py</code>.  This contains bare bones code for a <code>Markov</code> class for encapsulating all data in a model, and some (not necessarily all) of the instance methods that you need to write, as explained below.
									</p>

									<p>
										The folder also contains a number of text documents you will use to test your code.
									</p>

									<h3><a name = "addingtext">Data Structures / Adding Text (10 Points)</a></h3>
									<p>
										Choose appropriate data structures to represent your Markov chain (<a href = "../../ClassExercises/Week1_Adventure/">dictionaries</a> are highly recommended), and initialize them as instance variables in the constructor.  Then, fill in the method <code>add_string</code> to add prefixes and character counts to your data structures for a string that's passed in.  You should loop through all possible prefixes in the string passed as a parameter and update the counts of the prefixes (or add a new prefix if this is the first time you're seeing it).  You should also update the counts of the characters that follow each prefix.  Two things to watch out for here:
										<ul>
											<li>
												To prevent your model from running into dead ends when you go to synthesize new text, you should loop around once you run out of room to make a prefix at the end.  For example, if you had the text
												<p>
													<code>CS 477 rocks</code>
												</p>

												<p>And you chose to use prefix lengths of 5, you should have all of these prefixes</p>
<pre>
	CS 47
	S 477
	 477 
	477 r
	77 ro
	7 roc
	 rock	
</pre>


												<p>But also all of these prefixes once you start looping around</p>

<pre>
	rocks
	ocksC
	cksCS
	ksCS 
	sCS 4
</pre>

										in code, you'd construct a Markov model and add this string as follows
										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											model = MarkovModel(5)
											model.add_string("CS 477 rocks")
										</script>   

											</li>
											<li>
												<p>
													Do not add strings with a length less than the chosen prefix length.  Simply do nothing if such a string is passed.
												</p>
											</li>
										</ul>

										As an example to check to make sure this is working properly, if you add all of the lines from the the <a href = "HW3_Markov/text/spongebobquotes.txt">spongebob</a> file using 5-prefixes

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											model = MarkovModel(5)
											model.load_file_lines("text/spongebobquotes.txt")
										</script>   
										
										
										you should have <code>4032</code> unique prefixes, and the prefix counts for the string <code>" you "</code> should be as follows:
									</p>

									<p>
										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											{'s': 4, 'm': 1, 't': 4, 'l': 1, 'j': 1, 'w': 1, 
											'a': 4, 'b': 1, 'h': 1, 'c': 4, 'k': 1, 'e': 1, 'g': 1}
										</script>   
									</p>



									<h3><a name = "computingprobabilities">Computing Probabilities (10 Pts)</a></h3>
									<p>
										Given a model and some text, we can compute the likelihood that this text was drawn from the model by following the procedure <a href = "#probability">discussed in the background</a>.  Fill in the method <code>get_log_probability</code> to return the log probability of a sequence according to your model.
									</p>

									<p>
										You should first test the <a href = "#probexample">simple example</a> to make sure you're agreeing
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										m = MarkovModel(3)
										m.add_string("aaacbaaaacabacbbcabcccbccaaac")
										print(m.get_log_probability("aacabaa"))
									</script>   

									<p>
										For a longer example, if you run the following code 
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										model = MarkovModel(6)
										model.load_file_lines("text/spongebobquotes.txt")
										print(model.get_log_probability("I'm ready, I'm ready"))
									</script>   

									<p>
										You should get a log probability of <code>-40.5</code>
									</p>

									<p>
										If you then say
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										print(model.get_log_probability("Artificial intelligence"))
									</script>  

									<p>
										You should get a log probability of <code>-69.6</code>.  Note how much lower this probability is, even though the sequences are of roughly the same length.  This is our model telling is spongebob is much more likely to have said "I'm ready, I'm ready" than "Artificial intelligence".
									</p>

									<h3><a name = "debates">Experiment 1: 2016 US Presidental Debate (10 Pts)</a></h3>

									<p>
										You will now perform an experiment in the "train/test" paradigm, which is common in machine learning; that is, you will build models on some labeled data that is designated as "training data," and you will see how well your models predict new labeled examples in the "test data" set.  <b>You should perform the experiment and write your reflections all in a single Jupyter notebook file</b>
									</p>

									<p>
										The first experiment you will do is inspired by <a href = "https://www.cs.princeton.edu/courses/archive/spring08/cos226/assignments/markov.html">very similar assignment I did nearly 14 years ago</a> on figuring out if a quote during the 2004 presidential debate was said by Bush or Kerry.  To freshen this up, I have obtained the debate transcripts from the 2016 election from <a href = "https://www.politico.com/">Politico</a>.  You will train your models on the first two debates and see if you can predict who said a particular quote from the third.  In particular, you will initialize a <code>trump</code> and <code>clinton</code> model separately for some prefix length <b>K</b>
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										trump = MarkovModel(K)
										trump.load_file("text/2016Debates/trump1.txt")
										trump.load_file("text/2016Debates/trump2.txt")
										clinton = MarkovModel(K)
										clinton.load_file("text/2016Debates/clinton1.txt")
										clinton.load_file("text/2016Debates/clinton2.txt")
									</script>  

									<p>
										You will then loop through 40 different segments from the third debate and classify each one as having come from Trump or Clinton.  Here's an example of how to load all of the segments from Trump as strings
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										for i in range(40):
											fin = open("text/2016Debates/trump3-{:02d}.txt".format(i), encoding="utf8")
											s = fin.read() # This holds the text as a string
											fin.close()
											...
									</script>  

									<p>
										For each segment from both Trump and Clinton, you can compute the log probability under both models and see which one has a higher probability.  If the correct one has a higher probability, then you can consider this a correct classification.  If not, then it is a mistake.  You can report the accuracy as the total number of correct classifications over the total number of items in the test set.  For example, if I got 60 correct and there were 80 items, the accuracy would be 60/80 = 0.75.  
									</p>
										
									<p>
										Below is pseudocode on how you might check the Clinton debate examples to see which ones your models got correct.  You'd do a similar thing for the Trump debate examples.
									</p>
									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										train clinton model on clinton quotes from the first two debates
										train trump model on trump quotes from the first two debates

										for quote in third clinton debate:
											prob1 = log probability of quote given clinton model
											prob2 = log probability of quote given trump model
											if prob2 > prob1:
												correct++
											else
												incorrect++
									</script>  

									<p>
										In the end, you can think of generating something like a <a href = "https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">confusion matrix</a> as follows
									</p>

									<table style="width:600px;">
										<tr>
											<td></td><td>Guessed that Trump said it (probability of Trump is higher)</td><td>Guessed that Clinton said it (probability of Clinton is higher)</td>
										</tr>
										<tr>
											<td>Quote from Trump</td>
											<td>x</td>
											<td>y</td>
										</tr>
										<tr>
											<td>Quote from Clinton</td>
											<td>z</td>
											<td>w</td>
										</tr>
									</table>

									<p>
										The elements along the diagonals represent correct classifications, so the proportion of correctly guessed examples would be 
										<h3>
											\[ \frac{x+w}{x+y+z+w} \]
										</h3>
									</p>

									<p>
										Here are some things you should explore and show in your notebook
									</p>

									<ol>
										<li>
											
											<p>It's unclear a priori what prefix length <b>K</b> would be best for this task.  Create a loop where you report accuracies on the test data over a range of <b>K</b> values from 1 to 20, and plot the accuracy versus <b>K</b>.  You can use <a href = "https://www.w3schools.com/python/matplotlib_plotting.asp">matplotlib</a> to create such a plot.  <b>What trends do you notice in the plot?  Given what you know about these models, can you explain these trends?</b>  Answer this in a markdown cell in your notebook directly under your code.</p></li>
											

										<li>
											<p>For the best performing <b>K</b> in your experiment above, print out the text segments that the models got incorrect.  Do these make sense to you?  Write about your thoughts in the notebook</p>
										</li>
										<li>
											<p>
												For the best performing <b>K</b>, print the correctly classified sequences with the top 10 highest absolute difference in <b>normalized</b> log probability between the models; that is, divide the probability that clinton said each string <b>s</b> by <code>len(s)-K</code>, and do the same for Trump, then compute the absolute value of these differences.  This normalization controls for the length of the sequences and gives us an idea of how different the log probabilities of each prefix are on average.  The top 10 largest differences tell us which sequences the models are the most confident came from one speaker or the other.  Do these make sense to you?  Write about your thoughts in the notebook
											</p>
											<p>
												<b>Hint: </b> The numpy <a href = "https://numpy.org/doc/stable/reference/generated/numpy.argsort.html">argsort</a> method may come in handy here
											</p>
										</li>
									</ol>

									<div style="background: blanchedalmond; padding:20px;">
										<h3>Interpretable models</h3>
									<p>
										In the interest of time, I decided to cut a task that was there before about digging into the models to see why they're doing what they're doing.  One way to do this is to narrow in on which prefixes are leading to the largest disparity in probability for sequences which are said to be very different between the two models.  If you did this, you'd see prefixes involving "Hillary", "Clinton", "disaster", "invest", "i will tell you", "Sanders", "you think", "horrible", "millions of dollars", "unbelievable" were much more probable under Trump's model than Clinton's model, for example.
									</p>
									<p>
										Because we can actually dig into our model and find out meaningful things about how it's working, it's referred to as <a href = "https://towardsdatascience.com/interperable-vs-explainable-machine-learning-1fa525e12f48">interpretable</a>.  This is a nice feature to have in a model because we can explain more easily why it is or isn't working in practice.  This will not be true of all of the models we examine in this class, but it's often important for us to have this property to trust models.
									</p>
								</div>
								<p></p><p></p>




									
									<h3><a name = "spamham">Experiment 2: Spam vs Ham (5 Pts)</a></h3>

									<p>
										Now, you will try your hand at the spam classification with data from the <a href = "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection">UCI SMS Spam Collection</a>.  Given a test set of spam texts and of ham texts, you will train models to classify a test set of spam and ham texts.
									</p>


									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										spam = MarkovModel(K)
										spam.load_file_lines("text/spamham/spam_train.txt")
										ham = MarkovModel(K)
										ham.load_file_lines("text/spamham/ham_train.txt")
									
										fin = open("text/spamham/spam_test.txt", encoding="utf8")
										spam_test = fin.readlines() # This will be a list of strings
										fin.close()
										fin = open("text/spamham/ham_test.txt", encoding="utf8")
										ham_test = fin.readlines() # This will be a list of strings
										fin.close()
										
										...
									</script>  

									<p>
										As in the debate task, you should loop through a range of <b>K</b> values to see which one yields the best classification accuracy.  <b>What do you see about this plot that's different from the debate plot?  Why do you think this is?</b>  Write about this in your notebook.
									</p>



									<h3><a name = "moviereview">Experiment 3: Movie Reviews (5 Points)</a></h3>
									<p>
										Finally, you will train a model to recognize negative or positive movie reviews using the <a href = "https://www.cs.cornell.edu/people/pabo/movie%2Dreview%2Ddata/">Cornell movie sentiment analysis dataset</a>.  There are 1000 positive movie reviews in the <code>text/movies/pos</code> directory and 1000 negative movie reviews in the <code>text/movies/neg</code> directory.  Train on the first 900 of each, and test on the last 100 of each.  As in the other experiments, perform these classifications on a range of <b>K</b> values (they may take a bit longer since there is a lot more text).  <b>What do you notice about the optimal K value and the performance compared to the other two?  Why do you think this is?</b>  Write about this in your notebook.
									</p>

									<p>
										Finally, find your own positive and negative movie reviews online, and classify them in your notebook using models built on all of the movie data with the optimal <b>K</b> you found above.  Define the reviews as strings in your notebook (<a href = "https://www.w3schools.com/python/gloss_python_multi_line_strings.asp">Python multiline strings</a> may come in handy).  <b>Does your classifier get it correct?  How different are the reported probabilities?</b>  Write about this in your notebook.
									</p>



									<h3><a name = "synthesizing">Synthesizing Text (10 Pts)</a></h3>
									<p>
										Now that we're finished the analysis portion of the model, it's time to synthesize new test.  Fill in the method <code>synthesize_text</code> to create new text of a particular length drawn with probabilities from your model.
									</p>

									<p>
										For example, if I run the code 
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										model = MarkovModel(5)
										model.load_file_lines("text/ursinustweets.txt")
										print(model.synthesize_text(200))
									</script>   

									<p>
										I might get 
									</p>

									<p>
										<i><i>s birthday, to apply.The safety featured on @BartramsGarden, the event art prof. Carlita Favero. #MLKDay of single fathers participate in hanging to #Ursinus alum Phil DeSimone ’12 of @Carbon back to</i></i>
									</p>
										Of course, this is only one of many, many possibilities, but it should look roughly like an amalgamation of things that were in the training data, just spat back with no long term memory.
									</p>
									<p>
										To accomplish this, you should choose a prefix at random from the training text to start with, and then synthesize the rest of the text one character at a time, updating the current prefix you're on to be the last k characters in the string you're synthesizing as you go along.  <b>Be sure that the next character you choose respects the probability of the model.</b>  You may want to refer back to our <a href = "../../ClassExercises/Week4_Markov/">class exercise</a> to see how we were able to draw characters respecting probabilities.
									</p>

									<h3><a name = "poeticstatement">Poetic Statement (5 Pts)</a></h3>
									<p>
										Come up with your own collection of text and train a model.  Find at least 100 lines of text in a particular style, and place them all in a text file on a different line (you should choose something other than Ursinus tweets, sith lords, spongebob quotes, or Homer Simpson quotes).  Then, play around with the prefix size and see what kinds of outputs you get.  Once you are satisfied, generate <b>three or more examples</b> that you believe capture the spirit of your synthesizer, which will be displayed on a class gallery. 
									</p>

									<p>
										One thing you might try to make this really fun is something I'll call <b>Markov Hybrids</b>.  The idea is to load in two different styles in the same model and to have the synthesizer mash them up.  For example, if you load both the Sith quotes and Ursinus tweets in, you may get some results like this<a name = "ursinussith">:</a>

										<Ul>
                                                
											<li>"the cpd has yet another tip for undergraduates to explore the dark side"</li>

											<li>
												"check out this scholarship alert: before your eyes. i can drive you mad with fear, shred your sanity, and leave you a raving lunatic"
											</li>

											<li>"thomas j. watson fellowship, a yearlong grant that anyone who knows the words and scientific committee of the force"</li>

											<li>"kill the spring! 78 days until opening day!"</li>

											<li>"vanessa wilson-marshall '02 recalls the words and actions of significance is the result of conquest"</li>
										</Ul>
										
										If you choose to do this, at least one of the styles should be different from the examples I've given.
									</p>

									<h3><a name = "ethics">Ethics Reflection (5 Pts)</a></h3>
									<p>
										With great power comes great responsibility.  Even this relatively simple model can be used for nefarious purposes, and we should take care with how such code is deployed.  Reflect on this, and see if you can come up with at least three such examples.  What safeguards do we have against the examples you came up with?
									</p>

                                    
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#debugging">Debugging Principles</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading / Deadlines Policy</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "../../../Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												<!--
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
													<ul>
														<li><a href = "../../Assignments/HW2_RushHour/competition.html">Competition Results</a> </li>
													</ul>
												</li>
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<li>
													<a href = "../../Assignments/HW4_RobotLocalization">HW4: Bayesian Robot Localization</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5a_3DShapeCluster">HW5a: 3D Shape Clustering</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5b_Unsupervised">HW5b: NMF for Music Component Separation</a>
												</li>
												<li>
													<a href = "../../Assignments/HW6_LogisticRegression">HW6: Logistic Regression on Movie Reviews</a>
												</li>
												<li>
													<a href = "../../Assignments/HW7_DeepLearning">HW7: (Deep) Neural Networks on Images</a>
												</li>
												!-->
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_Bandit/jsbandit/index.html">Week 1: Multi-Armed Bandit Game</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "../../../Modules/Module2/Video1">Week 5: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Exercise / Theory of Bayesian Classifiers</a>
													<ul>
														<li><a href = "../../ClassExercises/Week5_BagOfWords#exercise">Text Classification Exercise</a></li>
														<li><a href = "../../ClassExercises/Week5_BagOfWords#theory">Naive Bayes Theory</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../../Modules/Module3/Exercise0">Week 5: Bayes Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "../../../Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "../../../Modules/MatrixModule/Video1">Week 8: Matrix Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_LogisticRegression/index.html">Week 10/11: Logistic Regression And Gradient Descent</a>
												</li>
												
											</ul>
										</li>
										<li><a href = "../../Ethics/index.html">Ethics Reading / Discussions</a></li>
										<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
	</body>