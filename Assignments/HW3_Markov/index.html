<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
										<h2>Homework 3: Markov Chains for Text Processing (46 Points)</h2>
                                        <h3><a href = "http://www.ctralie.com">Chris Tralie</a></h3>
										
										<ul>
											<li><a href = "#overview">Overview/Logistics</a>
												<ul>
													<li><a href = "#objectives">Learning Objectives</a></li>
													<li><a href = "#readme">What To Submit</a></li>
												</ul>
											</li>
											<li>
												<a href = "#tasks">Assignment Tasks</a>
												<ul>
													<li><a href = "#startercode">Starter Code</a></li>
													<li>
														<a href = "#addingtext">Data Structures / Adding Text (10 Points)</a>
													</li>
													<li>
														<a href = "#computingprobabilities">Computing Probabilities (10 Pts)</a>
													</li>
													<li>
														<a href = "#debates">2016 US Presidental Debate (16 Pts)</a>
													</li>
													<li>
														<a href = "#synthesizing">Synthesizing Text (10 Pts)</a>
													</li>
													<li>
														<a href = "#forfun">For fun</a>
													</li>
												</ul>
											</li>
										</ul>

									</header>

									<div id="page-content">

                                        <h2><a name = "overview">Overview / Logistics</a></h2>
										<p>
                                            The purpose of this assignment is to introduce you to natural language processing via <a href = "../../ClassExercises/Week4_MarkovText/index.html">Markov chains</a>, which are incredibly powerful tools for representing sequences mathematically, and which will form the foundation of several other topics we'll see in the class (hidden markov models, markov decision processes, and diffusion generative models).  
										</p>

										<p>
											First, you will use markov chains to classify text, seeing how they perform at telling whether Clinton or Trump was speaking during the 2016 presidential debates.  This is our first application of <b>supervised learning</b>, in which we <i>learn from labeled examples how to predict new, unknown data</i>.  In particular, you will learn statistics on the first two debates and predict the identity of the speaker in the third debate.
										</p>

										<p>
											Beyond learning, you will also use Markov chains as a <b>generative model</b> to synthesize new text that statistically matches a style.  For example, here is some text I synthesized based on the statistics trained on a bunch of Star Wars sith lord quotes, as well as Ursinus tweets, together:
										</p>

										<Ul>
                                                
											<li>"the cpd has yet another tip for undergraduates to explore the dark side"</li>

											<li>
												"check out this scholarship alert: before your eyes. i can drive you mad with fear, shred your sanity, and leave you a raving lunatic"
											</li>

											<li>"thomas j. watson fellowship, a yearlong grant that anyone who knows the words and scientific committee of the force"</li>

											<li>"kill the spring! 78 days until opening day!"</li>

											<li>"vanessa wilson-marshall '02 recalls the words and actions of significance is the result of conquest"</li>
										</Ul>

										<p>
											As funny as these are, I have to admit that I cherry-picked them; most of the examples I generated were garbled nonsense.  We'll learn later in the course using neural networks to create better generative models, but this is a good foundation of what's to come.
										</p>


                                        <p>
                                            <h3><a name = "objectives">Learning Objectives</a></h3>
                                            <ul>
												<li>Implement a statistical technique for supervised learning</li>
												<li>Implement a basic generative technique for natural language</li>
												<li>Follow the train set / test set paradigm for evaluating the performance of an algorithm, and learn how to "wrangle datasets" (i.e. load files and prepare them for testing).</li>
												<li>Search for optimal model parameters, and explore how these parameters vary across datasets.</li>
												<li>Use jupyter notebooks to organize experiments into a <a href = "https://writings.stephenwolfram.com/2017/11/what-is-a-computational-essay/">computational essay</a>.</li>
                                            </ul>
                                        </p>

                                        <h3><a name = "readme">What to submit</a></h3>
										<p>
                                            
											Submit on canvas your <code>markov.py</code> file and the jupyter notebook you created in the <a href = "#debates">experimental section</a>.
										</p>



									<h2><a name = "tasks">Assignment Tasks</a></h2>

									<p>
										<a href = "../../ClassExercises/Week4_MarkovText/index.html">Click here</a> to review the notes on Markov chains for document processing.  These notes will help in the tasks below.
									</p>

									<h3><a name = "startercode">Starter Code</a></h3>
									<p>
										<a href = "https://github.com/ursinus-cs477-f2023/HW3_Markov/archive/refs/heads/main.zip">Click here</a> to download the starter code for this assignment.  The main file you will be editing is <code>markov.py</code>.  This contains bare bones code for a <code>Markov</code> class for encapsulating all data in a model, and some (not necessarily all) of the instance methods that you need to write, as explained below.
									</p>

									<p>
										The folder also contains a number of text documents you will use to test your code.
									</p>

									<h3><a name = "addingtext">Data Structures / Adding Text (10 Points)</a></h3>
									<p>
										
										<H4>Your Task</H4>
										Choose appropriate data structures to represent your Markov chain (<a href = "../../ClassExercises/Week1_Adventure/">dictionaries</a> are highly recommended), and initialize them as instance variables in the constructor.  Then, fill in the method <code>add_string</code> to add prefixes and character counts to your data structures for a string that's passed in.  You should loop through all possible prefixes in the string passed as a parameter and update the counts of the prefixes (or add a new prefix if this is the first time you're seeing it).  You should also update the counts of the characters that follow each prefix.  Two things to watch out for here:
										<ul>
											<li>
												To prevent your model from running into dead ends when you go to synthesize new text, you should loop around once you run out of room to make a prefix at the end.  For example, if you had the text
												<p>
													<code>CS 477 rocks</code>
												</p>

												<p>And you chose to use prefix lengths of 5, you should have all of these prefixes</p>
<pre>
	CS 47
	S 477
	 477 
	477 r
	77 ro
	7 roc
	 rock	
</pre>


												<p>But also all of these prefixes once you start looping around</p>

<pre>
	rocks
	ocksC
	cksCS
	ksCS 
	sCS 4
</pre>

										in code, you'd construct a Markov model and add this string as follows
										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											model = MarkovModel(5)
											model.add_string("CS 477 rocks")
										</script>   

											</li>
											<li>
												<p>
													Do not add strings with a length less than the chosen prefix length.  Simply do nothing if such a string is passed.
												</p>
											</li>
										</ul>

										As an example to check to make sure this is working properly, if you add all of the lines from the the <a href = "HW3_Markov/text/spongebobquotes.txt">spongebob</a> file using 5-prefixes

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											model = MarkovModel(5)
											model.load_file_lines("text/spongebobquotes.txt")
										</script>   
										
										
										you should have <code>4032</code> unique prefixes, and the prefix counts for the string <code>" you "</code> should be as follows:
									</p>

									<p>
										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											{'s': 4, 'm': 1, 't': 4, 'l': 1, 'j': 1, 'w': 1, 
											'a': 4, 'b': 1, 'h': 1, 'c': 4, 'k': 1, 'e': 1, 'g': 1}
										</script>   
									</p>



									<h3><a name = "computingprobabilities">Computing Probabilities (10 Pts)</a></h3>
									<p>
										Given a model and some text, we can compute the likelihood that this text was drawn from the model by following the procedure <a href = "../../ClassExercises/Week4_MarkovText/index.html#probability">discussed in the notes</a>.  
									</p>
									<h4>Your Task</h4>
									<p>
										Fill in the method <code>get_log_probability</code> to return the log probability of a sequence according to your model.
									</p>

									<p>
										You should first test the <a href = "../../ClassExercises/Week4_MarkovText/index.html#probexample">simple example</a> to make sure you're agreeing
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										m = MarkovModel(3)
										m.add_string("aaacbaaaacabacbbcabcccbccaaac")
										print(m.get_log_probability("aacabaa"))
									</script>   

									<p>
										For a longer example, if you run the following code 
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										model = MarkovModel(6)
										model.load_file_lines("text/spongebobquotes.txt")
										print(model.get_log_probability("I'm ready, I'm ready"))
									</script>   

									<p>
										You should get a log probability of <code>-40.5</code>
									</p>

									<p>
										If you then say
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										print(model.get_log_probability("Artificial intelligence"))
									</script>  

									<p>
										You should get a log probability of <code>-69.6</code>.  Note how much lower this probability is, even though the sequences are of roughly the same length.  This is our model telling is spongebob is much more likely to have said "I'm ready, I'm ready" than "Artificial intelligence".
									</p>

									<HR>
									<h3><a name = "debates">2016 US Presidental Debate (10 Pts)</a></h3>

									<p>
										You will now perform an experiment in the "train/test" paradigm, which is common in machine learning; that is, you will build models on some labeled data that is designated as "training data," and you will see how well your models predict new labeled examples in the "test data" set.  <b>You should perform the experiment and write your reflections all in a single Jupyter notebook file</b>
									</p>

									<p>
										The first experiment you will do is inspired by <a href = "https://www.cs.princeton.edu/courses/archive/spring08/cos226/assignments/markov.html">very similar assignment I did over 15 years ago</a> on figuring out if a quote during the 2004 presidential debate was said by Bush or Kerry.  To freshen this up, I have obtained the debate transcripts from the 2016 election from <a href = "https://www.politico.com/">Politico</a>.  You will train your models on the first two debates and see if you can predict who said a particular quote from the third.  
									</p>

									<h4><a name = "task1">Task 1: Classification for a fixed <b>K</b> (5 Points)</a></h4>
									<p>
										
										Initialize a <code>trump</code> and <code>clinton</code> model separately for some prefix length <b>K</b>.  You can start with <b>K=5</b>
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										K = 5
										trump = MarkovModel(K)
										trump.load_file("text/2016Debates/trump1.txt")
										trump.load_file("text/2016Debates/trump2.txt")
										clinton = MarkovModel(K)
										clinton.load_file("text/2016Debates/clinton1.txt")
										clinton.load_file("text/2016Debates/clinton2.txt")
									</script>  

									<p>
										Then, loop through 40 different segments from the third debate and <b>classify</b> each one as having come from Trump or Clinton.  Here's an example of how to load all of the segments from Trump as strings
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										for i in range(40):
											fin = open("text/2016Debates/trump3-{:02d}.txt".format(i), encoding="utf8")
											s = fin.read() # This holds the text as a string
											fin.close()
											...
									</script>  

									<p>
										To classify each segment from both Trump and Clinton, compute the log probability under both models and see which one has a higher probability.
										<ul>
											<li>
												If the correct model has a higher probability, this is a correct classification.  If not, then it is a mistake.
											</li>
											<li>
												You can report the accuracy as the total number of correct classifications over the total number of items in the test set.  For example, if I got 60 correct and there were 80 items, the accuracy would be 60/80 = 0.75.
											</li>
										</ul>  
									</p>
										
									<p>
										Below is pseudocode on how you might check the Clinton debate examples to see which ones your models got correct.  You'd do a similar thing for the Trump debate examples.
									</p>
									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										train clinton model on clinton quotes from the first two debates
										train trump model on trump quotes from the first two debates

										for quote in third clinton debate:
											prob1 = log probability of quote given clinton model
											prob2 = log probability of quote given trump model
											if prob2 > prob1:
												correct++
											else
												incorrect++
									</script>  

									<p>
										To summarize the results, print out a <a href = "https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">confusion matrix</a> as follows
									</p>

									<table style="width:600px;">
										<tr>
											<td></td><td>Guessed that Trump said it (probability of Trump is higher)</td><td>Guessed that Clinton said it (probability of Clinton is higher)</td>
										</tr>
										<tr>
											<td>Quote from Trump</td>
											<td>x</td>
											<td>y</td>
										</tr>
										<tr>
											<td>Quote from Clinton</td>
											<td>z</td>
											<td>w</td>
										</tr>
									</table>

									<p>
										The elements along the diagonals represent correct classifications, so the proportion of correctly guessed examples would be 
										<h3>
											\[ \frac{x+w}{x+y+z+w} \]
										</h3>
									</p>

									<h4><a name = "task2">Task 2: Hyperparameter Optimization (3 Points)</a></h4>

											<p>
												It's unclear a priori what prefix length <b>K</b> would be best for this task.  This is what's known as a "hyperparameter," and we should experiment some to tune it properly.
											</p>
											<h4>Your Task</h4>
											<p>
												Create a loop where you report accuracies on the test data over a range of <b>K</b> values from 1 to 20, and plot the accuracy versus <b>K</b>.  You can use <a href = "https://www.w3schools.com/python/matplotlib_plotting.asp">matplotlib</a> to create such a plot.  <b>What trends do you notice in the plot?  Given what you know about these models, can you explain these trends?</b>  Answer this in a markdown cell in your notebook directly under your code.</p></li>

											<p>
												<b>NOTE:</b> Normally when we do hyperparameter optimization, we have 3 datasets we use: a training set (in this case the first two debates), a test set (in this case, the final debate), and another dataset called the <a href = "https://machinelearningmastery.com/difference-test-validation-datasets/">validation dataset</a>.  We pick the best hyperparameter based on the validation set, and then we report the results on the test set.  This is a more honest way of choosing the best parameter; as we're not tweaking it based on the final dataset on which we're reporting the results.  Also, doing things this way will lead to better "generalization" of the models.  We'll discuss this more in the next couple of units of the course, but I decided to keep things simple for now and to just use a training and test set.
											</p>
											

										<h4><a name = "task3">Task 3: Explainability (3 Points)</a></h4>
											<p>For the best performing <b>K</b> in your experiment above, print out the text segments that the models got incorrect.  Do these make sense to you?  Write about your thoughts in the notebook</p>


										<h4><a name = "task4">Task 4: Most Prominent Differences (5 Points)</a></h4>
											<p>
												For the best performing <b>K</b>, print the strings that were correctly classified that have the top 10 highest absolute difference in <b>normalized</b> log probability between the models; that is, divide the probability that clinton said each string <b>s</b> by <code>len(s)-K</code>, and do the same for Trump, then compute the absolute value of these differences.  Do these make sense to you?  Write about your thoughts in the notebook
											</p>
											<p>
												<b>Hint: </b> The numpy <a href = "https://numpy.org/doc/stable/reference/generated/numpy.argsort.html">argsort</a> method may come in handy here
											</p>
											<p>
												<b>NOTE: </b> The normalization controls for the length of the sequences and gives us an idea of how different the log probabilities of each prefix are on average.
											</p>

									<div style="background: blanchedalmond; padding:20px;">
										<h3>Interpretable models</h3>
									<p>
										In the interest of time, I decided to cut a task that was there before about digging into the models to see why they're doing what they're doing.  One way to do this is to narrow in on which prefixes are leading to the largest disparity in probability for sequences which are said to be very different between the two models.  If you did this, you'd see prefixes involving "Hillary", "Clinton", "disaster", "invest", "i will tell you", "Sanders", "you think", "horrible", "millions of dollars", "unbelievable" were much more probable under Trump's model than Clinton's model, for example.
									</p>
									<p>
										Because we can actually dig into our model and find out meaningful things about how it's working, it's referred to as <a href = "https://towardsdatascience.com/interperable-vs-explainable-machine-learning-1fa525e12f48">interpretable</a>.  This is a nice feature to have in a model because we can explain more easily why it is or isn't working in practice.  This will not be true of all of the models we examine in this class, but it's often important for us to have this property to trust models.
									</p>
								</div>
								<p></p><p></p>




									

								<HR>
									<h3><a name = "synthesizing">Synthesizing Text (10 Pts)</a></h3>
									<p>
										We will finish by doing a basic generative model for text.  To accomplish this, you should choose a prefix at random from the training text to start with, and then synthesize the rest of the text one character at a time, updating the current prefix you're on to be the last k characters in the string you're synthesizing as you go along.  <b>Be sure that the next character you choose respects the probability of the model.</b>  You may want to refer back to our <a href = "../../ClassExercises/Week4_Markov/">class exercise</a> to see how we were able to draw characters respecting probabilities.
									</p>

									<h4>Your Task</h4>

									<p>
										Fill in the method <code>synthesize_text</code> to create new text of a particular length drawn with probabilities from your model.
									</p>

									<p>
										For example, if I run the code 
									</p>

									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										model = MarkovModel(5)
										model.load_file_lines("text/ursinustweets.txt")
										print(model.synthesize_text(200))
									</script>   

									<p>
										I might get 
									</p>

									<p>
										<i><i>s birthday, to apply.The safety featured on @BartramsGarden, the event art prof. Carlita Favero. #MLKDay of single fathers participate in hanging to #Ursinus alum Phil DeSimone ’12 of @Carbon back to</i></i>
									</p>
										Of course, this is only one of many, many possibilities, but it should look roughly like an amalgamation of things that were in the training data, just spat back with no long term memory.
									</p>
										
									<h3><a name = "forfun">For fun</a></h3>
									<p>
										In addition to the presidential debates, I also provided a spam vs ham dataset and a positive and negative movie reviews dataset.  We'll return to the movie reviews dataset in a later assignment with a different technique, but you can try the Markov chain technique on both of these now if you're bored.
									</p>
									<p>
										Beyond that, you can try synthesizing "Markov hybrids" like I did with the Star Wars and Ursinus quotes.  Please share what you get on Discord with the rest of the class!
									</p>
                                    
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#debugging">Debugging Principles</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading / Deadlines Policy</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "../../../Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
												</li>
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<!--
												<li>
													<a href = "../../Assignments/HW4_RobotLocalization">HW4: Bayesian Robot Localization</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5a_3DShapeCluster">HW5a: 3D Shape Clustering</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5b_Unsupervised">HW5b: NMF for Music Component Separation</a>
												</li>
												<li>
													<a href = "../../Assignments/HW6_LogisticRegression">HW6: Logistic Regression on Movie Reviews</a>
												</li>
												<li>
													<a href = "../../Assignments/HW7_DeepLearning">HW7: (Deep) Neural Networks on Images</a>
												</li>
												!-->
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_Bandit/index.html">Week 1: Introduction To Reinforcement Learning</a>
													<ul>
														<li>
															<a href = "../../ClassExercises/Week1_Bandit/jsbandit/index.html">The Multi-Armed Bandit Game</a>
														</li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../Materials/MazeExplorer">Week 2: Maze Searching Game</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/assignments/186957">Week 3: An Admissible But Not Consistent Heuristic</a>
												</li>
												<li>
													<a href = "../../../Modules/Module2/Video1">Week 4: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_MarkovText">Week 4: Markov Chains for Document Representations</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/quizzes/24127">Week 4: Bayes Rule Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_NaiveBayes">Week 5: Bayes Rule And Naive Bayes Classifiers</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Naive Bayes Exercise</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_KMeans">Week 5: KMeans Clustering And Visual Bag of Words</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 5/6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_RobotLocalization">Week 5/6: Robot Localization</a>
												</li>
												<li>
													<a href = "../../../Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "../../../Modules/MatrixModule/Video1">Week 8: Matrix Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_LogisticRegression/index.html">Week 10/11: Logistic Regression And Gradient Descent</a>
												</li>
												
											</ul>
										</li>
										<li><a href = "../../Ethics/index.html">Ethics Reading / Discussions</a></li>
										<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
	</body>