<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2021</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2021</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Assignment 4: Bayesian Robot Localization (40 Points)</h2>
                                        <h3><a href = "http://www.ctralie.com">Chris Tralie</a></h3>
									</header>

									<div id="page-content">
										<ul>
											<li>
												<a href = "#programming">Programming Tasks</a>
												<ul>
													<li><a href = "#gettingstarted">Getting Started</a></li>
													<li><a href = "#sensormodel">Task 1: Sensor Model</a> (5 Points)</li>
													<li><a href = "#filtering">Task 2: Online Position Tracking</a> (15 Points)</li>
													<li><a href = "#viterbi">Task 3: The Viterbi Algorithm</a> (15 Points)</li>
													<li><a href = "#yourown">Task 4: Make Your Own Map</a> (5 Points)</li>
												</ul>
											
											</li>
										</ul>

                                        <p>
											The purpose of this assignment is to track a robot through an environment from noisy measurements using Bayesian algorithms from "<a href = "../../ClassExercises/Week6_HMM/">Hidden Markov Models (HMMs)</a>."  In this case, the hidden states are the <b>position of a robot in 2D</b>, and the observations are <b>omnidirectional range scans</b>.  This is an idealized model of a real "LIDAR scanner" such as the <a href = "https://hokuyo-usa.com/products/lidar-obstacle-detection/urg-04lx">Hokuyo URG-04LX laser range scanner</a>, which is able to sweep a laser in a 270 degree field of view in front of it
										</p>

										<img src = "URG-04LX.jpg" width=250>

										<p>
											For this assignment, I've created a synthetic robot that can move right/left/up/down on a 2D grid, and which is always oriented the same way.  Below is an image of this robot moving around.  The ground truth position of the robot (the hidden state) is shown on the left on a map of the environment, while the laser scan (observation) is shown in the middle.  The right shows the scan as stored in an array, which is what you'll be manipulating in code. There is an index in the array for each angle of the scan, and the value at the index corresponds to the range at that angle.  0 degrees is the first element, and 360 degrees is the last element, so you'll notice how the ranges loop around at the boundaries
										</p>

										<img src = "clean_scan.gif">

										<h4><a name = "noisemodel">Noise Model</a></h4>
										<p>
											The above is an idealized example, however, because the scan is rarely this perfect.  Usually there is noise that <i>perturbs</i> the true range measurements; that is, we end up measuring that a wall is either closer or further than it actually is at a particular angle.  We'll model the noise here as a multiplicative Gaussian; that is, if the ground truth range is <b>r</b>, then the observed range <b>m(r)</b> is 
										</p>

										<h3>
											\[ m(r) = r(1 + \alpha n)\]
										</h3>

										<p>
											where <b>n</b> is a "standard Gaussian distributed" random variable with distribution 
										</p>

										<h3>
											\[ n \sim \frac{1}{\sqrt{2 \pi}} e^{-n^2/2} \]
										</h3>

										<p>
											and <b>&alpha;</b> is some parameter set ahead of time.  In other words, the further away the measurement is, the more it can be perturbed.  Below is the code I used to sample from this noise model, taking advantage of numpy's built in <code><a href = "https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html">randn</a></code> method for sampling random variables from the standard Gaussian
										</p>

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											scan[i] = range*(1+alpha*np.random.randn())
										</script>  

										<p>
											Below is an example where <b>&alpha; = 0.1</b>
										</p>

										<img src = "noisy_scan.gif">

										<p>
											Below is an example where <b>&alpha; = 4</b>
										</p>

										<img src = "verynoisy_scan.gif">

										<p>
											At this level of noise, it seems like we're hardly getting any useful information.  However, amazingly, if you use the above sensor model and assume that the robot is equally likely to visit any of its neighbors, then you can actually recover an excellent estimate of the robot's trajectory using the Viterbi algorithm.  Below is a plot of the original trajectory next to what the algorithm recovered here (NOTE: results may vary based on the noise samples):
										</p>

										<img src = "Est_Maze1_VeryNoisy.svg">

										<p>
											The estimated trajectory is perfect in this example!  This is the power of <b>sequence modeling</b>; even if our measurements are total crap at a particular instant in time, if they have even a little bit of signal, then we can "boost" the signal strength by looking at many states in sequence.
										</p>

										<HR>
											<h2><a name = "programming">Programming Tasks</a></h2>

											<h3><a name = "gettingstarted">Getting Started</a></h3>
											<p>
												<a href = "https://github.com/Ursinus-CS477-F2021/HW4_RobotLocalization/archive/refs/heads/main.zip">Click here</a> to download the starter code for the assignment.  When you are finished, <b>submit any notebook files or code files with a clear description about what is implemented where.</b>
											</p>
											<p>
												
												Before you run any code, make sure that you have <a href = "https://scikit-image.org/">scikit-image</a> installed in your python environment by typing the following at the top of a Jupyter notebook
											</p>

											<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											import sys
											!{sys.executable} -m pip install scikit-image
											</script>  

											<p>
												Then, setup a Jupyter notebook that imports the simulation engine, which you can start off like this:
											</p>



											<script type="syntaxhighlighter" class="brush: python"><![CDATA[
												%load_ext autoreload
												%autoreload 2
												import numpy as np
												import matplotlib.pyplot as plt
												import matplotlib.cm as cm
												import matplotlib.animation as animation
												import IPython.display as ipd
												from environment import *
											</script>  

											<p>
												Below is some code you can use to initialize a world and to simulate the scans of a trajectory through that world
											</p>

											<script type="syntaxhighlighter" class="brush: python"><![CDATA[
												## Step 1: Create the world and devise robot motion
												# Load in a particular environment
												env = Environment("Maze1.png")
												# Devise a path through that environment that passes through 3 locations
												X = env.simulate_trajectory([[0, 15], [27, 40], [26, 12]])
												# Plot the environment with the path superimposed
												plt.figure()
												env.plot()
												plt.plot(X[:, 0], X[:, 1])

												## Step 2: Simulate scans at each position
												# How many angles to sample in the range scanner
												res = 50
												# The noise of the scanner (start with low noise to make the problem easier)
												alpha = 0.1
												# Make this repeatably pseudorandom by seeding so that the numbers you get match up with mine
												np.random.seed(0) 
												# Create a list of scans.  Each scan holds a list of "res" laser ranges across all angles
												observed_scans = []
												for i in range(X.shape[0]):
													observed_scans.append(env.get_range_scan(X[i, :], res, alpha))
												# NOTE: observed_scans[i][j] would be angle j at time i
												

											</script>  

											<p>
												Next, you'll need to figure out what perfect observations look like at each grid cell in the world so you can figure out the probability that you observed a scan given that you were at a particular cell.  There are <b>N</b> open grid cells in the world indexed from <b>0</b> to <b>N-1</b>, and the robot can be at any one of them at any time.  The method <code>get_state_scans(res)</code> of the <code>Environment</code> class returns an array perfect scans from each of these locations.
											</p>



											<script type="syntaxhighlighter" class="brush: python"><![CDATA[
												state_scans = env.get_state_scans(res)
											</script> 
											
											<p>
												Finally, there's a list member variable <code>env.neighbors</code>, where <code>env.neighbors[i]</code> lists the indices of the neighbors of state <code>i</code>. 
											</p>
											

										<h3><a name = "sensormodel">Task 1: Sensor Model (5 Points)</a></h3>

										<p>
											The first step in gathering all of the components that are needed for an HMM is to come up with probabilities in a measurement model.  Let's let <b>r<SUB>i</SUB></b> be the perfect range measurement of the i<SUP>th</SUP> angle at a particular location, and let <b>x<SUB>i</SUB></b> be the corresponding measurement at that angle, sampled according to the <a href = "#noisemodel">model above</a>. Then the probability density of the measured range <b>x<SUB>i</SUB></b> in terms of <b>r<SUB>i</SUB></b> is 
										</p>

										<h2>
											\[ p(x_i | r_i) = \frac{1}{\sqrt{2 \pi }(\alpha r_i + \gamma)} e^{-\frac{(x_i-r_i)^2}{2 (\alpha r_i + \gamma)^2}} \]
										</h2>

										<p>
											where <b>&gamma;</b> is a small number to prevent numerical divide by 0 for ranges that are too closes or noise that is too small.  In this assignment, we'll let <b>&gamma; = 0.1</b>.  Notice how based on the denominator of the exponent, ranges which are further have a higher probability density for the same deviation.  This is another way of seeing that the noise is worse for large distances.
										</p>
										<p>
											Assuming that the angles are all independent of each other, the joint probability of all of the angles in a single scan can be written as the product of all such probabilities at each angle.  
										</p>

										<p>
											Let's think about which states would have a high probability given particular measurements.  Below is an image showing an observed scan with its ground truth position indicated as a blue dot on the map, as well as two ideal scans at different positions shown at their state locations as orange and green dots on the map
										</p>

										<p>
											The orange dot is very close to the measured location, so its scan looks pretty similar.  The green dot is far and in a region that looks nothing like the region where the measurement was taken, so its scan is very different.  We can see the differences in the plot of the scans.  Each angle contributes a probability as in the above equation.  Because of the negative exponential which contains <b>-(x<SUB>i</SUB> - r<SUB>i</SUB>)<SUP>2</SUP></b>, the probability density at a particular angle peaks when <b>x<SUB>i</SUB> = r<SUB>i</SUB></b>, and it falls off the further apart they are. So we will incur large penalties in the product for angles that have very different range measurements compared to what should be seen at a state under consideration.  These bad angle measurements will drag all of the rest down in the product.
										</p>

										<img src = "ScanExamples.svg">


										<p>
											<b>Your Task:</b> Create a method that takes in a ground truth scan array, a measured scan array, and the noise value &alpha;, and which returns the probability density that a set of measured angles jointly occurred at a particular location.  For example, let's say you called this method <code>get_measurement_prob</code> and you ran the <a href = "#gettingstarted">initialization sequence above</a> with <b>&alpha;=0.1</b>.  Then running the following cell
										</p>

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											K = len(state_scans)
											# Compute the measurement probability of the scan at each location
											meas_probs = np.zeros(K)
											idx = 55
											for i in range(K):
												meas_probs[i] = get_measurement_prob(observed_scans[idx], state_scans[i], alpha)
											# Plot the measurement probabilities on the map
											env.plot_probabilities(meas_probs, p=1e-2, show_max=False)
											# Plot the ground truth location from the trajectory as an green dot
											plt.scatter([X[idx, 0]], X[idx, 1], c='C2') 
											plt.legend(["Ground Truth Location"])
										</script> 

										<p>
											should generate the following plot
										</p>

										<img src = "MeasurementProbs.svg">

										<p>
											The brighter the cell, the higher the probability is relative to cells at other locations.  You'll notice here that for this particular location (shown in green), it has a high probability around the true location, but it also has a high probability towards the right side of similarly shaped narrow horizontal hallways.  As you will see in the tasks below, this kind of confusion can get resolved by tracking measurements over time.
										</p>



										<h3><a name = "filtering">Task 2: Online Position Tracking (15 Points)</a></h3>
										<p>
											You now have all of the ingredients you need to do an online tracking of the robot positions as new measurements come in!  You can assume the following two things:
											<ul>
												<li>The robot is equally likely to transition to any of its neighbors (which, as you recall, you can look up in <code>env.neighbors</code>)</li>
												<li>The robot is equally likely to start at any position on the grid.  This implies that the initial probabilities for each state are <b>1/K</b>, where <b>K</b> is the number of states.</li>
											</ul>
											<b>Your Task: </b>Given all of this information and the measurement model from before, implement <a href = "../../ClassExercises/Week6_HMM/index.html#bayespseudocode">Bayes filtering</a>  to track the position over time.  <b>Be sure to normalize the probabilities at each step so that they all sum to 1!</b>  Also, when summing up the transition probabilities from previous states, <b>only consider states which are neighbors of the state you're transitioning to</b>.  Otherwise, the program will have <b>O(TK<SUP>2</SUP>)</b> complexity instead of <b>O(TK)</b> complexity, and your code will go <b>very slowly</b>.  
										</p>
										<p>
											To look at the results, it's helpful to plot the probabilities as you're filtering over time.  Matplotlib has some good <a href = "https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.ArtistAnimation.html">built in tools to help with this</a>.  Assuming that you have a list or numpy array called <code>f</code> that holds the filtered probabilities of each cell at time <code>n</code>, here's how you would make a video of your filtering results
										</p>

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											frames = [] # for storing the generated images
											fig = plt.figure(figsize=(6, 6))
											K = len(state_scans) # Number of states
											T = X.shape[0] # Number of timesteps
											f = np.zeros(K) # The probabilities across all states at each time
											## TODO: Setup initial probabilities, etc
											for i in range(T): # Filter every measurement that comes in
												## TODO: Bayes filtering for this frame index
												plot = env.plot_probabilities(f, p=1e-2)
												plot.append(plt.scatter([X[i, 0]], [X[i, 1]], c='C0'))
												frames.append(plot)
											ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True, repeat_delay=1000)
											ipd.HTML(ani.to_html5_video())
										</script> 

										<p>
											<b><a name = "gifsave">NOTE: </a></b> You may get an error about ffmpeg if you try to run the last line in the above code.  If this is the case, you can instead save the animation to a gif in the same file that you're notebook is being run using the line 

											<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											ani.save("result.gif")
											</script> 
										</p>

										<p>
											Regardless of how you generate the animation, if you've implemented filtering properly, here's what you should see on the trajectory example we've been looking at with <b>&alpha;=4</b> (the actual position is shown as a blue dot, while the maximum probability is shown as a <span style="color:green">X</span>)
										</p>

										<img src = "Maze1_Tracking.gif">

										<p>
											You'll notice some cool things here, how the probability is distributed over all of the cells at first, but how it quickly hones in on the actual location of the robot.  You'll also see it hedging its bets between different long and narrow hallways when it happens to be in one, but as soon as it turns a corner or passes a fork in the road, the ambiguity is resolved.
										</p>

										<p>
											Here's another neat example where you see it gets very confused and estimates it's at the top row instead of the second to the bottom row for a while because of how similar all of the hallways look.  The ambiguity is mostly resolved once the robot turns the corner
										</p>

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											env = Environment("Maze2.png")
											res = 50
											X = env.simulate_trajectory([[10, 45], [120, 45], [126, 100]])
											alpha = 4
											np.random.seed(0)
											observed_scans = []
											for i in range(X.shape[0]):
												observed_scans.append(env.get_range_scan(X[i, :], res, alpha))
											# These scans will take a second to generate on this map, so 
											# be sure to just run this cell once at the beginning
											state_scans = env.get_state_scans(res)
											## TODO: Make a new filtering model and try it out
										</script>

										<img src = "Maze2_Tracking.gif">


										<h3><a name = "viterbi">Task 3: The Viterbi Algorithm (15 Points)</a></h3>

										<p>
											As you can see, filtering can get confused at individual steps, and we know if we can do things <i>offline</i> (i.e. we have access to <i>all measurements</i> over time), we can maximize the <i>joint probability</i> over the whole trajectory using the <a href = "../../ClassExercises/Week6_HMM/index.html#viterbipseudocode">Viterbi algorithm</a>.  This will fix things up so the trajectory doesn't jitter around as much locally, since it enforces consistency of the whole trajectory.  
										</p>
										<p>
											<b>Your task: </b> Implement the Viterbi algorithm to compute an optimal sequence of state indices given an array of measurements.  <b>Make a separate notebook to show this to keep organized</b>.  Since you'll be accumulating over many states, be sure to add <b>log likelihoods</b>, as <a href = "../../ClassExercises/Week6_HMM/#viterbi">explained in the notes</a> rather than multiplying probabilities.
										</p>

										<p>
											<a name = "logmeasnote">NOTE: </a>
											If you use the same measurement probability function as you did in the last section and then you take the log, you might suffer from underflow and get complaints about taking the log of 0.  Instead, you should create a different version of your measurement function where you use a different analytical expression to compute the log probability directly.  You can use the following three facts about logs:
											<div style="width:100px;">
												<ul>
													<li>
														\[ \log(AB) = \log(A) + \log(B) \]
													</li>
													<li>
														\[ \log \left( \frac{1}{x} \right) = -\log(x) \]
													</li>
													<li>
														\[ e^{\log(x)} = x,  \log(e^x) = x \]
													</li>
												</ul> 
											</div>

											Taken together, this means that
										</p>

										<div style="width:300px;">
											<h3>
												\[ \log \left(  \left( \frac{1}{\sqrt{2 \pi }(\alpha r_i + \gamma)} \right) e^{ -\frac{(x_i-r_i)^2}{2(\alpha r_i + \gamma)^2}} \right) = -\log \left( \sqrt{2 \pi }(\alpha r_i + \gamma) \right) -\frac{(x_i-r_i)^2}{2(\alpha r_i + \gamma)^2} \]
											</h3>
										</div>
										

										<p>
											Let's say your Viterbi code outputs a list of state indices called <code>states</code>.  Then the following code will extract and plot the coordinates of the trajectory you estimated on top of the "ground truth" (correct) trajectory
										</p>

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											states = np.array(states, dtype=int)
											Y = env.X[states, :]
											plt.figure()
											plt.plot(X[:, 0], X[:, 1], 'k', linewidth=4)
											plt.plot(Y[:, 0], Y[:, 1], 'C1', linestyle='--')
											plt.legend(["Ground Truth", "Estimated"])
											plt.axis("equal")
											plt.title("Estimated Trajectory, $\\alpha={:.3f}$".format(alpha))
										</script> 

										<p>
											First try to make sure you can handle the case where &alpha; = 0.1, then crank up the noise and see how much noise the algorithm can take and still give a good result.  If your code works properly, you should get perfect results with &alpha;=4 on Maze1.png
										</p>

										<img src = "Est_Maze1_VeryNoisy.svg">

										<p>
											You should also get nearly perfect results with &alpha; = 4 for the trajectory on the second map
										</p>

										<img src = "Est_Maze2_VeryNoisy.svg">

										<h3><a name = "yourown">Task 4: Make Your Own Map</a> (5 Points)</h3>
										<p>
											Make your own map and trajectory on it and show the results of filtering!  You can use a program like MS paint, <a href = "https://www.gimp.org/">gimp</a>, or photoshop.  The map should be all black except for where, such as <a href = "HW4_RobotLocalization/Maze1.png">the example maze that was given</a>.   We'll make a class gallery
										</p>


											<h3>For The Bored...</h3>
											<p>
												If you finish this early, here are a few things you can try
											</p>
											<ul>
												<li>
													In addition to modeling the position, allow the robot to rotate.  How would you change your state space?  How would you update your observations to handle a rotation?
												</li>
												<li>
													The above is going to blow up the state space in memory.  Think of how you could use a <a href = "https://web.mit.edu/16.412j/www/html/Advanced%20lectures/Slides/Hsaio_plinval_miller_ParticleFiltersPrint.pdf">particle filter</a> to address this.
												</li>
												<li>
													Think about how you might store some "second order" information about where the robot has been like velocity.  If we assume the law of inertia, the robot is more likely to continue moving in the direction of its velocity than it is to make a sudden turn, so you can use non-uniform transition probabilities to neighbors.
												</li>
											</ul>


                                    
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#readings">Readings</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
													<ul>
														<li><a href = "../../Assignments/HW2_RushHour/competition.html">Competition Results</a> </li>
													</ul>
												</li>
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<li>
													<a href = "../../Assignments/HW4_RobotLocalization">HW4: Bayesian Robot Localization</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5a_3DShapeCluster">HW5a: 3D Shape Clustering</a>
												</li>
												<li>
													<a href = "../../Assignments/HW5b_Unsupervised">HW5b: NMF for Music Component Separation</a>
												</li>
												<li>
													<a href = "../../Assignments/HW6_LogisticRegression">HW6: Logistic Regression on Movie Reviews</a>
												</li>
												<li>
													<a href = "../../Assignments/HW7_DeepLearning">HW7: (Deep) Neural Networks on Images</a>
												</li>
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_WhatIsAI">Week 1: What Is AI?</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_COVID">Week 1: Monte Carlo COVID Simulation</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_COVID/solution.html">Solution</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/Module2/Video1">Week 5: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Exercise / Theory of Bayesian Classifiers</a>
													<ul>
														<li><a href = "../../ClassExercises/Week5_BagOfWords#exercise">Text Classification Exercise</a></li>
														<li><a href = "../../ClassExercises/Week5_BagOfWords#theory">Naive Bayes Theory</a></li>
													</ul>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/Module3/Exercise0">Week 5: Bayes Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_GradSchoolAdmissions">Week 6: Gaussian Naive Bayes And Grad School Admissions</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2021.github.io/Modules/MatrixModule/Video1">Week 8: Matrix Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week8_NMF">Week 8: Nonnegative Matrix Factorization</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week9_KMeans3DShapes">Week 9/10: KMeans Clustering, Applications To Image Processing</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_VisualBOW/index.html">Week 10: Visual Bag of Words</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_LogisticRegression/index.html">Week 10/11: Logistic Regression And Gradient Descent</a>
												</li>
											</ul>
										</li>
										<li><a href = "../../Ethics/index.html">Ethics Reading / Discussions</a></li>
										<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
