<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Homework 6: Build Your Own Multilayer Perceptron (BYOMLP)</h2>
										<h3>Chris Tralie</h3>
									</header>
									
									<h3>
										Table of Contents
										<ul>
											<li><a href = "#objectives">Learning Objectives</a></li>
											<li>
												<a href = "#programming">Programming Tasks</a>
												<ul>
													<li>
														<a href = "#backprop">Backpropagation (15 Points)</a>
													</li>
													<li>
														<a href = "#digits">Digits Classification (10 Points)</a>
													</li>
												</ul>

											</li>
										</ul>

									</h3>

									<div id="page-content">

                                        <p>
                                            <h2><a name = "objectives">Learning Objectives</a></h2>
                                            <ul>
												<li>
													Implement backpropagation for densely connected neural networks
												</li>
												<li>
													Implement stochastic gradient descent
												</li>
												<li>
													Explore different architectures of MLPs
												</li>
                                            </ul>
										</p>

										<h2><a name = "logistics">Logistics / Getting Started</a></h2>

										<p>
											In this assignment, you will implement your own neural network solver from scratch in Python. 
										</p>

										
										
									<h2><a name = "programming">Programming Tasks</a></h2>

									

									<h3>
										<a name = "backprop">Backpropagation (15 Points)</a>
									</h3>

									<p>
										<h4>Your Task</h4> Study the class I've setup in <code>neuralnet.py</code>, and then fill in the method <code>backprop_descent</code>.  This consists of two steps:
										<ol>
											<li>
												Use backpropagation to compute the gradients of all of the weights and biases
											</li>
											<li>
												Subtract the learning factor <b>&alpha;</b> times these gradients from all of the parameters.
											</li>
										</ol>
									</p>

									<p>
										The <code>backprop_descent</code> method accepts as input a single training example and its target output, and it updates the weights right away.  This is referred to as <a name = "stochastic">stochastic gradient descent</a>, and it contrasts to the <a href = "../HW5_LogisticRegression/index.html#batch">batch gradient descent</a> we used on the last homework where we waited until accumulating gradients from all training samples before updating the weights/bias.
									</p>

									<p>
										I have already provided a method <code>forward</code> to evaluate each layer of the neural network and to store the inputs <b>a</b> to each activation function and the outputs <b>h</b> of the activation function.  I have also provided a method <code>add_layer</code> to randomly initialize all of the weights and biases (this simply boils down to calling <a href = "https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html">numpy's randn</a> to generate random matrices).  So once you finish <code>backprop_descent</code>, you'll have a complete system to train arbitrary feedforward neural networks.
									</p>

									<p>
										To help you test your system, I've provided the notebook <code>DiscTest.ipynb</code> which will learn a network to separate the points on the inside of a disc from the points on the outside of the disc, which <i>is not something that we could do with 2D logistic regression</i>!  Dataset shown below:
									</p>

									<img src = "DiscData.svg">

									<p>
										
										To do the separation, we use a network with the following three layers:
										<ol>
											<li>
												100 neurons with a leaky ReLU
											</li>
											<li>
												2 neurons with a leaky ReLU
											</li>
											<li>
												The final output neuron with a logistic activation, using the logistic loss
											</li>
										</ol>
									</p>
									<p>
										Using our little API, we can define this network with the following code
									</p>
									<script type="syntaxhighlighter" class="brush: python"><![CDATA[
										nn = NeuralNet(2, logistic_est_loss_deriv) # Input is in 2 dimensions, and we want to use logistic loss
										nn.add_layer(100,  leaky_relu, leaky_relu_deriv) # First layer is 100 dimensions with a leaky ReLU
										nn.add_layer(2, leaky_relu, leaky_relu_deriv) # Second layer is 2 dimensions with a leaky ReLU
										nn.add_layer(1, logistic, None) # Last layer is the logistic function.  Its derivative is handled separately
									</script>  

									<p>
										Then, since our API is designed to update one example at a time with backpropagation, we continuously feed it random permutations of our data one at a time.  This is all in the notebook already and you can just run it.  If it works properly, you should see an animation like this, where the left plot shows the 2 coordinates of the output of layer 2 as well as the linear separator inferred from the final layer, and the right plot shows the loss over time.  As you can see, after a few iterations, the orange points get moved to the other side of the final linear separator from the blue points
									</p>

									<img src = "DiscResult.gif" width=1000>


									<h3>
										<a name = "digits">Digits Classification (10 Points)</a>
									</h3>

									<p>
										We're now ready to apply our MLPs to digit classification!  The last step is that we need to define a loss function over the output of the softmax, compared to ground truth.  For this, define ground truth as an an "indicator array," or also referred to as a <b>one-hot vector</b>.  Here are all of the possible 1-hot vectors based on ground truth 

										<table style="width:400px;">
											<tr><td>Ground truth Value</td><td>1-Hot Vector <b>y</b></td></tr>
											<tr>
												<td>0</td>
												<td>
													[ <b>1</b>, 0, 0, 0, 0, 0, 0, 0, 0, 0]
												</td>
											</tr>
											<tr>
												<td>1</td>
												<td>
													[0,  <b>1</b>, 0, 0, 0, 0, 0, 0, 0, 0]
												</td>
											</tr>
											<tr>
												<td>2</td>
												<td>
													[0, 0,  <b>1</b>, 0, 0, 0, 0, 0, 0, 0]
												</td>
											</tr>
											<tr>
												<td>3</td>
												<td>
													[0, 0, 0,  <b>1</b>, 0, 0, 0, 0, 0, 0]
												</td>
											</tr>
											<tr>
												<td>4</td>
												<td>
													[0, 0, 0, 0,  <b>1</b>, 0, 0, 0, 0, 0]
												</td>
											</tr>
											<tr>
												<td>5</td>
												<td>
													[0, 0, 0, 0, 0,  <b>1</b>, 0, 0, 0, 0]
												</td>
											</tr>
											<tr>
												<td>6</td>
												<td>
													[0, 0, 0, 0, 0, 0,  <b>1</b>, 0, 0, 0]
												</td>
											</tr>
											<tr>
												<td>7</td>
												<td>
													[0, 0, 0, 0, 0, 0, 0,  <b>1</b>, 0, 0]
												</td>
											</tr>
											<tr>
												<td>8</td>
												<td>
													[0, 0, 0, 0, 0, 0, 0, 0, <b>1</b>, 0]
												</td>
											</tr>
											<tr>
												<td>9</td>
												<td>
													[0, 0, 0, 0, 0, 0, 0, 0, 0, <b>1</b>]
												</td>
											</tr>
										</table>
									</p>
									<p>
										Based on this, we can define a loss function which is a generalization of the <a href = "../../ClassExercises/Week10_LogisticRegression/index.html#logisticloss">logistic loss</a> to multivariate output.  Given the output <b>y<SUB>est</SUB></b> of the softmax function and the ground truth 1-hot vectors <b>y</b>, we can define the <b>multivariate cross-entropy logistic loss</b> as 
									</p>

									<div style="width:100px;">
									<h3>
										\[ L(y, y_{\text{est}}) = -\sum_{i=1}^N  y[i] \log(y_{\text{est}}[i]) \]
									</h3>
									</div>

									<p>
										where in our digits problem <b>N = 10</b>.  We need to take the gradient of this loss with respect to the input to the logistic function, but this is actually incredibly simple and exactly like the single variable logistic loss case.  In particular, the gradient of the <b>i<SUP>th</SUP></b> component to the input of the softmax function is simply
										<b>y<SUB>est</SUB>[i] - y[i]</b>
										<h3>
											<b>y<SUB>est</SUB>[i] - y[i]</b>
										</h3>
										
										This should look pretty familiar! For details, look back to the <a href = "../HW5_LogisticRegression/index.html#logisticbbow">update rules</a> in the last assignment, and refer to my <a href = "../../../Modules/Softmax/Video1">derivation in the softmax module</a>.  To save time, I've already implemented this for you.  The loss function and its derivatives are the methods <code>softmax_est_crossentropy_loss</code> and <code>softmax_est_crossentropy_deriv</code>, respectively, in the <code>losses.py</code> file.
									</p>

									<p>
										<h4>Your Task</h4> Put everything together in a Jupyter notebook to train a neural network in our little API that classifies digits in the MNist dataset.  Train a network using <b><a href = "#stochastic">stochastic gradient descent</a> on the training set for 60 iterations</b>, and report the accuracy on all of the images in the test set.  You should compare two networks: the first network should have a single hidden layer with 20 neurons and a softmax output, which you can define like this:

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											nn = NeuralNet(x_train.shape[1], softmax_est_crossentropy_deriv)
											nn.add_layer(20, leaky_relu, leaky_relu_deriv)
											nn.add_layer(10, softmax, None)
										</script>  
										
										The second network should also start with a hidden layer with 20 neurons, but you should add an additional ReLU layer with 40 neurons.  <b>Indicate clearly in your notebook which network works better and why you think this is</b>.
									</p>


									<h4>Loading data</h4>
									
									
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#debugging">Debugging Principles</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading / Deadlines Policy</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "../../../Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
												</li>
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<li>
													<a href = "../../Assignments/HW4_FundamentalFreq">HW4: Fundamental Frequency Tracking And Pitch-Based Audio Effects</a>
													<ul>
														<li>
															<a href = "../../Assignments/HW4_FundamentalFreq/statements.html">Musical statements</a>
														</li>
													</ul>
												</li>
												<li>
													<a href = "../../Assignments/HW5_LogisticRegression">HW5: Logistic Regression on Movie Reviews</a>
												</li>
												<!--
												<li>
													<a href = "../../Assignments/HW7_DeepLearning">HW7: (Deep) Neural Networks on Images</a>
												</li>
												!-->
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_Bandit/index.html">Week 1: Introduction To Reinforcement Learning</a>
													<ul>
														<li>
															<a href = "../../ClassExercises/Week1_Bandit/jsbandit/index.html">The Multi-Armed Bandit Game</a>
														</li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../Materials/MazeExplorer">Week 2: Maze Searching Game</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/assignments/186957">Week 3: An Admissible But Not Consistent Heuristic</a>
												</li>
												<li>
													<a href = "../../../Modules/Module2/Video1">Week 4: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_MarkovText">Week 4: Markov Chains for Document Representations</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/quizzes/24127">Week 4: Bayes Rule Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_NaiveBayes">Week 5: Bayes Rule And Naive Bayes Classifiers</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Naive Bayes Exercise</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 5/6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_RobotLocalization">Week 5/6: Robot Localization</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2023.github.io/Modules/HMM/Video1">Week 6: HMM Module</a>
												</li>
												<li>
													<a href = "https://github.com/ursinus-cs477-f2023/Week6_MDP">Week 6: Markov Decision Processes And Pong AI</a>
												</li>
												<li>
													<a href = "../../../Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN/index.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_KMeans">Week 7: KMeans Clustering And Visual Bag of Words</a>
												</li>
												<li>
													<a href = "../../../Modules/MatrixModule/Video1">Week 7: Matrix Module</a>
												</li>
												<li>
													<a href = "ClassExercises/Week7_PCA/Week7_PCA/UnsupervisedDigits.html">Week 7: PCA on MNIST Digits</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_LogisticRegression/index.html">Week 8: Logistic Regression And Gradient Descent</a>
												</li>
												<li>
													<a href = "../../../Modules/LogisticRegression/Video1">Week 8: Neural Networks Module 1</a>
												</li>
												<li>
													<a href = "../../../Modules/Softmax/Video1">Week 9: Softmax Module</a>
												</li>
												<li><a href = "../../../Modules/NeuralNets/Video0">Week 9/10: Multi-Class Logistic Regression And Feedforward Neural Nets Module</a></li>
												<li>
													<a href = "../../ClassExercises/Week10_Backprop/index.html">Week 10: Backpropagation on Multilayer Perceptrons</a>
												</li>
												
												
											</ul>
										</li>
										<li><a href = "../../Ethics/index.html">Ethics Reading / Discussions</a></li>
										<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
