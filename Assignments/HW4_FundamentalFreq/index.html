<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 477: Artificial Intelligence And Machine Learning, Fall 2023</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Homework 4: Fundamental Frequency Tracking And Pitch-Based Audio Effects (35 Points)</h2>
										<h3>Chris Tralie</h3>
									</header>
									
									<h3>Table of Contents</h3>
									<ul>
										<li><a href = "#objectives">Learning Objectives</a></li>
										<li><a href = "#overview">Background</a></li>
										<ul>
											<li><a href = "#digitalaudio">Digital Audio Fundamentals</a></li>
											<li><a href = "#yin">The YIN Algorithm</a></li>
											<li><a href = "#pyin">The Probabilistic YIN (pYIN) Algorithm</a>
											<ul>
												<li><a href = "#states">States</a></li>
												<li><a href = "#observations">Observation Probabilities</a></li>
												<li><a href = "#transition">Transition Probabilities</a></li>
											</ul>
										</li>
										</ul>
										<li><a href = "#programming">Programming Tasks</a>
										<ul>
											<li>
												<a href = "#naive_freqs">Task 1: Naive Greedy Frequency Choices (5 Points)</a>
											</li>
											<li>
												<a href = "#naive_freqs">Task 2: PYin HMM-Based Frequency Estimation (25 Points)</a>
											</li>
											<li>
												<a href = "#statement">Task 3: Mandatory Musical Statement (5 Points)</a>
											</li>
										</ul></li>
									</ul>

									<p>
										The purpose of this assignment is to get students practice with <a href = "../../../Modules/HMM/Video1">Hidden Markov Models (HMMs)</a> and the <a href = "../../../Modules/HMM/Video4">Viterbi Algorithm</a> with a fun application involving digital music audio.  Students will create a system that uses a Bayesian framework to teach a computer how to listen to the musical notes that a signer is singing.  Implementing such a system unlocks a slew of fun applications, such as <a href = "#fmsynth">singing instruments</a> and <a href = "#autotune">autotuning effects</a>.
									</p>


									<div id="page-content">

                                        <p>
                                            <h2><a name = "objectives">Learning Objectives</a></h2>
                                            <ul>
												<li>
													Implement observation probabilities and transition probabilites for a hidden markov model
												</li>
												<li>
													Use the viterbi algorithm to estimate a clean sequence of states given outlier observations
												</li>
												<li>
													Have fun with some cool audio effects
												</li>
                                            </ul>
										</p>

										<h2><a name = "overview">Background</a></h2>

										<h3><a name = "digitalaudio">Digital Audio Fundamentals</a></h3>

										<p>
											Sound is carried by a <a href = "https://www.youtube.com/watch?v=Bcqp6t4ybxU">modulated pressure wave</a> through the air.  When it reaches a microphone, the microphone <b>samples</b> the audio by taking measurements of the pressure in regular intervals, and putting them all into one big array of <b>audio samples</b>.  A typical <b>sample rate</b>, which we'll use in this assignment, is <b>44100</b> samples per second.  As an example, let's suppose we have the following clip of Marvin Gaye singing "<a href = "https://www.udiscovermusic.com/stories/marvin-gaye-i-heard-it-through-the-grapevine-song/">I Heard It Through The Grapevine</a>" (<a href = "HW4_FundamentalFreq/marvin.wav">marvin.wav</a>)
										</p>

										<audio controls>
											<source src="Examples/marvin.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 

										<p>
											Here's some code we can use to look at a 4410 sample chunk of the above clip, which corresponds to 100 milliseconds of audio
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											x_audio, sr = load_audio("marvin.wav")
											t = np.arange(x_audio.size) / sr # Time in seconds
											plt.plot(t[sr:sr+sr//10], x_audio[sr:sr+sr//10])
											plt.xlabel("Time (Sec)")
											plt.ylabel("Normalized Pressure")
										</script>

										<p>
											The resulting plot looks like this
										</p>

										<img src = "Examples/Marvin100ms.svg" width="50%">

										<p>
											In general, the pressure is normalized to the range <b>[-1, 1]</b>.  Pressures closer to these limits are perceived as louder to us, so we see the above audio is not as loud as it could be.
										</p>
										<p>
											Another thing you might notice is that the audio <b>oscillates</b> back and forth in a regular pattern, which is a common occurrence in vibrational systems in nature such as human vocal tracts.  As it turns out, humans perceive such <b>periodic</b> signals as a <b><a href = "https://ursinus-cs372-s2023.github.io/CoursePage/ClassExercises/Week2/Week2_Harmonicity/index.html">pitch</a></b>, or a note.  The higher the <b>frequency</b> (number of repetitions per second) of the vibration, the higher the note we perceive.
										</p>

										<p>
											An ideal model of a periodic sound is a <b>pure tune</b>, which can be modeled with a cosine as follows:
										</p>

										<h3>
											\[ y(t) = \cos(2 \pi f t) \]
										</h3>
										<p>
											where <b>f</b> is the frequency in <b>cycles/second (hz)</b>, and <b>t</b> is the time in seconds.  We can synthesize such a pure tone at 440hz in numpy as follows:
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											freq = 440 # Frequency, in units of samples/second (hz)
											sr = 44100 # Audio sample rate, in units of samples/second (hz)
											t = np.arange(sr*2)/sr # Samples for 2 seconds of audio, in units of seconds
											y = np.cos(2*np.pi*freq*t) # Pure tone cosine at this frequency
											save_audio(y, sr, "puretone{}.wav".format(freq))
										</script>

										<p>
											This is "concert A," or the note that orchestras tune to.  The resulting audio sounds like this
										</p>
										<audio controls>
											<source src="Examples/puretone440.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 
										
										<p>
											If we change the frequency to 660hz instead of 440hz, we get the following
										</p>

										<audio controls>
											<source src="Examples/puretone660.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 

										<p>
											As you can hear, since the frequency went up, the note that we perceive also went up in pitch.
										</p>

										<p>
											In the above, we knew exactly what frequency we were making, but when we're analyzing data, we don't necessarily know this.  The problem of <b>fundamental frequency tracking</b> is to estimate the frequency <i>over time</i> in existing audio data.  In other words, we're teaching a computer how to listen for notes.  In what follows, I will explain some excellent models for doing this in practice.
										</p>


										<h3><a name = "yin">The YIN Algorithm</a></h3>

										<p>
											The "YIN" algorithm was a breakthrough in fundamental frequency estimation.  <a href = "http://audition.ens.fr/adc/pdf/2002_JASA_YIN.pdf">Click here</a> to see the original 2002 paper on this technique by Alan de CheveigneÂ´ and Hideki Kawahara.  I will give a brief overview of the main idea so you know enough to use the output from this algorithm in the next stage of the assignment.
										</p>

										<p>
											YIN is like an improved version of <a href = "https://ursinus-cs372-s2023.github.io/Modules/Module16/Video2">autocorrelation</a> for period estimation (for those who may have seen autocorrelation in my <a href = "https://ursinus-cs372-s2023.github.io/CoursePage">music class</a>).  The idea is to take a chunk of the signal, called a <b>frame</b>, and to slide the first half of that signal, called the <b>window</b>, past itself in the frame.  Then, we compute the sum of the squared differences where the window overlaps with the corresponding chunk in the frame.  YIN then does one more step after this, which is to <b>normalize</b> the values so that they are generally in the scale from 0 to 1, and to de-emphasize very small shifts.  The animation below shows the process:
										</p>

										<img src = "YINExample.gif" width="90%">

										<p>
											What you'll notice is that YIN dips down close to zero every time the signal overlaps well with itself, which happens once every cycle (I've marked these as dots in the bottom plot).  We refer to the shifts that happen as the <b>period</b>, which is the length of a cycle, and which is inversely proportional to the frequency.  In the above example, the period is 108, which corresponds to a frequency of 44100/108 &approx; 408hz, which is a slightly flat A flat.
										</p>

										<p>
											Since we're interested in the fundamental frequency <b>over time</b>, YIN keeps moving the frame along in time and redoing this process, returning all of the "troughs" where a min occurs, along with the normalized distance of that trough, referred to as that trough's <b>threshold</b>.  Below is code showing how to obtain these troughs in the Marvin Gaye clip using the provided <code><a href = "https://github.com/ursinus-cs477-f2023/HW4_FundamentalFreq/blob/7d7df7202275991042bd40a16c843c3ee34ad9dd/audiotools.py#L74">get_yin_freqs</a></code> method, which returns a list of lists of <code>[frequency, threshold]</code> values over time:
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											filename = "marvin.wav"
											x_audio, sr = load_audio(filename)
											
											fmin = 100
											fmax = 2000
											
											frame_length = 4096
											win_length = frame_length//2
											hop_length = frame_length//4
											all_freqs = get_yin_freqs(x_audio, frame_length, sr) # List of list of [frequency, threshold]
											
											
											X = []
											for t, freqs_t in enumerate(all_freqs):
												for (f, thresh) in freqs_t:
													X.append([t, f, thresh])
											X = np.array(X)
											plt.figure(figsize=(15, 6))
											plt.scatter(X[:, 0]*hop_length/sr, X[:, 1], s=10, c=X[:, 2], cmap='magma')
											plt.colorbar()
											plt.xlabel("Time (Seconds)")
											plt.ylabel("Frequency (hz)")
											plt.title(filename)
											
											plt.savefig("{}.png".format(filename[0:-4]), bbox_inches='tight')
										</script>

										<p>
											The plot of all of the mins over time looks like this, colored by their threshold value:
										</p>

										<img src = "Examples/marvin.png" width="90%">
										<p>
											Overall, the estimates seem pretty stable.  One issue, though, is that YIN returns the fundamental period, but also all integer multiples of the period.  In frequency terms, this means we'll see a track of frequencies that seems relatively stable, but we'll see a copy at half the frequency, a copy at 1/3 of the frequency, a copy at 1/4 of the frequency, etc.  So it's possible to get confused as to which one is the right one.  Also, there are clearly moments where there are a lot of noise in the estimate.
										</p>

										<p>
											We could be greedy and simply take the frequency with the minimum trough threshold at each time.  This is what you'll do in the <a href = "#naive_freqs">first task</a>, implementing the <code>naive_freqs</code> method.  The code below shows how to get the results and to modulate a cosine to match the estimated frequencies using the method <code><a href = "https://github.com/ursinus-cs477-f2023/HW4_FundamentalFreq/blob/7d7df7202275991042bd40a16c843c3ee34ad9dd/effects.py#L23">sonify_pure_tone</a></code> (if you're curious, <a href = "https://ursinus-cs372-s2023.github.io/Modules/Module4/Video1">click here</a> to learn more about how to turn instantaneous frequency estimates into pure tone pitches)
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											freqs = naive_freqs(all_freqs)
											plt.figure(figsize=(12, 4))
											plt.scatter(np.arange(freqs.size)*hop_length/sr, freqs, s=10)
											y = sonify_pure_tone(x_audio, freqs, frame_length//4, sr)
											plt.xlabel("Time (Sec)")
											plt.ylabel("Frequency (hz)")
											save_audio(y, sr, "marvin_greedy.wav")
										</script>

										<p>
											which are as follows:
										</p>

										<audio controls>
											<source src="Examples/marvin_greedy.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 

										<img src = "Examples/marvin_greedy.png" width="90%">

										<p>
											This is fairly decent, though we see a few outliers.  
										</p>

										<p>
											If, on the other hand, we do the same for vocals from Cher's <a href = "https://www.thrillist.com/entertainment/nation/chers-believe-auto-tune">Believe</a> (<a href = "HW4_FundamentalFreq/cher.wav">cher.wav</a>)
										</p>
										<audio controls>
											<source src="Examples/cher.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio>

										<p>
											then the YIN estimates are much messier:
										</p>


										<img src = "Examples/cher.png" width="90%">

										<p>
											And the greedy method has much more noticeable outliers:
										</p>

										<audio controls>
											<source src="Examples/cher_greedy.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 

										<img src = "Examples/cher_greedy.png">

										<p>
											The primary goal in this assignment will be to fix this problem and to create much more stable fundamental frequency estimates
										</p>


										<h3><a name = "pyin">The Probabilistic YIN (pYIN) Algorithm</a></h3>
										<p>
											There was an awesome followup paper in 2014 by Matthias Mauch and Simon Dixon called <a href = "https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/6040/MAUCHpYINFundamental2014Accepted.pdf">pYIN: A fundamental frequency estimator using probabalistic threshold distributions</a> (<a href = "https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/6040/MAUCHpYINFundamental2014Accepted.pdf">click here</a> to read the paper).  In a nutshell, the authors add a Hidden Markov Model on top of YIN and use the <a href = "../../../Modules/HMM/Video4">Viterbi Algorithm</a> to find the best sequence of fundamental frequencies.  The effect is to match the estimates from YIN as well as possible (the observation probabilities), but not to let the frequencies jump around too much (the transition probabilities).
										</p>
										<p>
											I'll now present a slightly simplified<SUP><a href = "#embellishments">*</a></SUP> version of pYIN suitable for this assignment:
										</p>

										<h3><a name = "states">States</a></h3>
										<p>
											The Viterbi algorithm is specialized to a discrete state space, but the estimated frequencies are real numbers (floats).  To deal with this, we'll come up with a discrete set of possible frequencies from some lower limit <code>fmin</code> to some upper limit <code>fmax</code>.  Adjacent frequencies are related by a <b>multiplicative ratio</b> known as the  <code>spacing</code> (the frequencies get spaced further apart as they increase, because humans perceive pitch logarithmically in frequency).  For example, with <code>fmin=100, fmax=2000, spacing=2**(1/120)</code>, then the first 10 frequencies would be:
										</p>

										<code>100.0,  100.6,  101.2,  101.7,  102.3,  102.9,  103.5,  104.1,  104.7,  105.3,  105.9,  106.6,  107.2,  107.8,  108.4,  109.1,  109.7,  110.3,  111.0,  111.6,   ...</code>

										<p>
											In this example, there would be <code>520</code> discrete frequencies states total (each possible note is divided into 10 intervals in this example).  In practice, we round the observations to the nearest state, so, for instance, a frequency of 101 would be state index 2 in the above example.
										</p>

										<p>
											In addition to the <b>K</b> frequency states, there is one more "dummy state" used to refer to <b>unvoiced</b> segments, and we allow ourselves to transition to this state when we're not confident enough about our frequency estimates and/or we think the person might not be singing.  So, in total, with the <b>K</b> frequencies and the unvoiced state, there are <b>K+1</b> states total.
										</p>

										<h3><a name = "observations">Observation Probabilities</a></h3>
										<p>
											The observation probabilities at each frame are related to the frequency estimates and the thresholds returned from YIN in that frame, as obtained from <code>get_yin_freqs</code>.  For example, let's say we ran the following code:
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											x_audio, sr = load_audio("marvin.wav")
											frame_length = 2048
											all_freqs = get_yin_freqs(x_audio, frame_length, sr)
											t = 0
											print(all_freqs[t])
										</script>

										<p>
											then we would get 
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											[(364.576912533348, 0.0320662716995428), 
											(182.76438405329742, 0.10031922414709707), 
											(122.25050168933892, 0.19591546020561615)]
										</script>

										<p>
											In other words, at time 0, there were 3 possible frequency estimate, each with different thresholds.  Intuitively, a lower threshold closer to 0 means we're more confident about that frequency.  To make turn this idea into a precise observation model, let's suppose there are <b>N</b> frequencies total, indexed as <b>(f<SUB>i</SUB>, t<SUB>i</SUB>)</b>, where <b>f<SUB>i</SUB></b> is a frequency and <b>t<SUB>i</SUB></b> is its corresponding threshold. Let's suppose that <b>j</b> is the state with the nearest frequency to <b>f<SUB>i</SUB></b></u>.  The we let the observation probability for state <b>j</b> be 
										</p>

										<h3>
											\[ p_o(j) = 2^{-t_i/\mu} / N \]
										</h3>

										<p>
											where <b>&mu;</b> is some hyperparameter specified ahead of time; smaller <b>&mu;</b> values are more strict at requiring lower thresholds.
										</p>

										<p>
											We also need an observation probability for the unvoiced state.  This is simply what's leftover, assuming all possible disjoint observations sum to a probability of 1:
										</p>

										<h3>
											\[ p_o(\text{unvoiced}) = 1 - \sum_{i=1}^N 2^{-t_i/\mu} / N \]
										</h3>

										<p>
											<b>NOTE: </b> When you run the Viterbi algorithm to update accumulated probabilities at time <code>t</code>, <b>you only need to worry about making updates for states corresponding to the <code>N</code> frequencies in <code>all_freqs[t]</code> and the unvoiced state</b>.  You can assume that all other frequencies have a probability of 0.  This is part of what makes the algorithm efficient in practice.
										</p>
										

										<h3><a name = "transition">Transition Probabilities</a></h3>
										<p>
											Finally, we need to specify the probability of transitioning from one frequency state to the next.  In all specifications below, <b>p<SUB>s</SUB></b> is the probability of remaining in a frequency state or remaining unvoiced (we take <b>p<SUB>s</SUB>=0.9999</b> in our examples) 
										</p>

										<h4><a name = "fre2freq">Case 1: Frequency To Frequency Transitions</a></h4>
										<p>	
											We don't want to allow the frequency estimates to jump around very much, so we will use lower transition probabilities for two adjacent states that are further in frequency.  Let the states be in ascending order of frequency.  Then, supposing we're at frequency state <b>j</b>, the probability of having transitioned from state <b>k</b> is 
										</p>

										<h3>
											\[ pt(x_t = j | x_{t-1} = k) = \left\{ \begin{array}{cc} \frac{p_s}{df} - \frac{p_s|j-k|}{df^2} & k > j-df, k < j+df \\ 0 & \text{otherwise} \end{array} \right\} \]
										</h3>

										<p>
											The image below shows this "triangular weight distribution" (which is what the paper calls it):
										</p>

										<img src = "TransitionProb.svg" width="60%">

										<p>
											<b>NOTE: </b> Transition probabilities outside of this range are 0, so you don't need to consider coming from any <b>k</b> outside of this range.  Together with the sparse observations, the limited possible transitions help to keep the algorithm very efficient.
										</p>



										<h4><a name = "unvoiced2freq">Case 2: Unvoiced State To Frequency State</a></h4>
										<p>
											We also have to consider the frequency of transitioning from the unvoiced state to the frequency state <b>j</b>.  We let this be:
										</p>

										<h3>
											\[ pt(x_t = j | x_{t-1} = \text{unvoiced}) = (1-p_s)/K\]
										</h3>

										<p>
											where <b>K</b> is the total number of frequency states.  
										</p>

										<h4><a name = "unvoiced2unvoiced">Case 3: Unvoiced To Unvoiced</a></h4>

										<p>
											Below is the probability of staying unvoiced given that we're unvoiced:
										</p>

										<h3>
											\[ pt(x_t = \text{unvoiced} | x_{t-1} = \text{unvoiced}) = p_s/K^2\]
										</h3>

										<p>
											(NOTE: I really had to hack this to get it to work with only one unvoiced state<SUP><a href = "#embellishments">*</a></SUP>)
										</p>

										

										<h4><a name = "freq2unvoiced">Case 4: Frequency To Unvoiced</a></h4>

										<p>
											The probability of jumping from a frequency state to the unvoiced state, for each possible frequency state <b>k</b>, is
										</p>

										<h3>
											\[ pt(x_t = \text{unvoiced} | x_{t-1} = k) = 1-p_s\]
										</h3>


										<h4><a name = "alltogethertransitions">Putting It All Together</a></h4>

										

										<p>
										Below are figures showing showing where to go from previous states if state <b>j</b> is a frequency state (left: case 1 and case 2 together) or if it is the unvoiced state (right: case 3 and case 4)
										</p>

										<div style="display: inline-block;width:40%;">
											<img src = "IntoFreqTransitions.svg" width="70%">
										</div>

										<div style="display: inline-block;width:40%;vertical-align: top;">
											<img src = "IntoUnvoicedTransitions.svg" width="70%">
										</div>

										

										<p></p>

										

										<h4>Running Viterbi</h4>
										<p>
											Using YIN and the above observation and transition probabilities, the Viterbi algorithm does quite well at recovering a stable pitch trajectory!  Here's the greedy estimate again for Cher:
										</p>

										<audio controls>
											<source src="Examples/cher_greedy.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 

										<p>
											And here's the result of pYIN:
										</p>

										<audio controls>
											<source src="Examples/cher_pyin.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 

										<p>
											<img src = "Examples/cher_greedy_vs_pyin.svg" width="90%">
										</p>

										<p>
											Your goal in <a href = "#naive_freqs">task 2</a> is to implement this pYIN to replicate these results.
										</p>

										<HR>
										<h2><a name = "programming">Programming Tasks</a></h2>

										<p>
											<a href = "https://github.com/ursinus-cs477-f2023/HW4_FundamentalFreq/archive/refs/heads/main.zip">Click here</a> to download the starter code for this assignment.  You will be editing <code>freqs.py</code>, which currently just holds skeleton definitions of the two methods <code>naive_freqs</code> and <code>pyin_freqs</code> that you'll be filling in.
										</p>

										<p>
											Feel free to use the notebook <code>freqs_tester.ipynb</code> to test things, or, if you don't like jupyter notebooks, you can save audio files with the <code>save_audio</code> method in <code>audiotools.py</code>
										</p>

										<h3><a name = "naive_freqs">Task 1: Naive Greedy Frequency Choices (5 Points)</a></h3>

										<p>
											Fill in the method <code>naive_freqs</code> to implement the method that chooses the frequency with the minimum YIN threshold at each time.  
										</p>

										<p>
											You can test this in the notebook, or by running the following code snippet:
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											x_audio, sr = load_audio("cher.wav")
											frame_length = 4096
											win_length = frame_length//2
											hop_length = frame_length//4
											all_freqs = get_yin_freqs(x_audio, frame_length, sr)
											freqs = naive_freqs(all_freqs)
											y = sonify_pure_tone(x_audio, freqs, frame_length//4, sr)
											save_audio(y, sr, "cher_naive.wav")
										</script>
										<p></p>

										<HR>
										<h3><a name = "naive_freqs">Task 2: PYin HMM-Based Frequency Estimation (25 Points)</a></h3>

										<p>
											Fill in the method <code>get_pyin_freqs</code> in <code>freqs.py</code> to implement the HMM-based method for frequency tracking.  You should review the notes <a href = "../../../Modules/HMM/Video4">at this link</a> on how we applied the Viterbi algorithm to robot localization.  The code in this section will be quite similar in structure to the robot localization code; the only thing that's changed is what exactly the states are, what the observation probabilities are, and what the transition probabilities are.  Refer to the <a href = "#pyin">notes above</a> for the specifics.  
										</p>
										
										<p>
											When you are backtracing through the states, if you happen to cross through an unvoiced state, make that frequency <code>np.nan</code>
										</p>


										<p>
											You can test this in the notebook, or by running the following code snippet:
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											x_audio, sr = load_audio("cher.wav")
											frame_length = 4096
											win_length = frame_length//2
											hop_length = frame_length//4
											all_freqs = get_yin_freqs(x_audio, frame_length, sr)
											freqs = pyin_freqs(all_freqs)
											y = sonify_pure_tone(x_audio, freqs, frame_length//4, sr)
											save_audio(y, sr, "cher_pyin.wav")
										</script>

										<p>
											In which case you should get the following audio clip:
										</p>
										
										<audio controls>
											<source src="Examples/cher_pyin.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										  </audio> 

										<p>
											<b>NOTE: </b> Be sure to add log probabilities in your transition model instead of multiplying the original probabilities!  Otherwise, your accumulated probabilities will quickly underflow.
										</p>

										<h4>Hints</h4>

										<p>
											It's helpful if you initialize <code>L</code> to be all <code>-np.inf</code>.  If you have <code>T</code> time instants and <code>K</code> frequency states, you can do this quickly with 
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											L = -np.inf*np.ones((K+1, T))
										</script>

										<p>
											If your code is working properly, here's what <code>L</code> and <code>B</code> should look like for Cher and Marvin 
										</p>

										<img src = "Examples/BLmarvin.png" width="90%">

										<p></p>
										<img src = "Examples/BLcher.png" width="90%">

										<p>
											Code to generate plots like this is as follows:
										</p>
										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											plt.figure(figsize=(12, 12))
											plt.subplot(211)
											plt.imshow(L, aspect='auto', interpolation='none', cmap='magma')
											plt.xlim([0, L.shape[1]])
											plt.ylim([L.shape[0]+20, -20])
											plt.xlabel("Time Index")
											plt.ylabel("State Index")
											plt.title("L for cher.wav")
											plt.colorbar()
											plt.subplot(212)
											plt.imshow(B, aspect='auto', interpolation='none', cmap='magma')
											plt.xlim([0, B.shape[1]])
											plt.ylim([B.shape[0]+20, -20])
											plt.xlabel("Time Index")
											plt.ylabel("State Index")
											plt.title("B for cher.wav")
											plt.colorbar()
											plt.savefig("cher.png", bbox_inches='tight')
										</script>

										<h4><a name = "pseudocode">Pseudocode</a></h4>
										Below is some pseudocode to help you piece the different cases together
									</p>

										<ol>
											<li>
												Figure out how many frequency states there are.  Let this be K
											</li>
											<li>
												Let the number of times T be len(all_freqs)
											</li>
											<li>
												Initialize a (K+1)xT matrix L of all -infinity.  This will hold the best accumulated log probabilities to each state
											</li>
											<li>
												Initialize a (K+1)xT matrix B of all 0's.  This will hold the backpointers to the previous state that maximized the probability, which we will use to reconstruct a solution
											</li>
											<li>
												For each time <b>t</b>, do the following:
												<ul>
													<li>
														<b>5a.</b> First, compute the best frequency probabilities for this time.  We only have to worry about the states closest to frequencies that we actually observed in <code>all_freqs[t]</code>; no other states will be likely enough to happen at this time.
														<ul>
															<li>
																<p>
																	For each frequency <code>f</code> and threshold <code>thresh</code> in <code>all_freqs[t]</code>, find the nearest frequency state index <code>j</code>.  For example, if we happened to have 5 frequency states [100, 102, 105, 110, 120] and we observed a frequency 103.2, that would be <code>j=2</code>, since it's closest to 105.
																</p>
																<p>
																	Now we need to consider all previous frequency states that it could have come from
																	<ul>
																		<li>
																			For each state <code>k = j-df+1 to j+df-1</code>, inclusive, compute 
																			<p>
																				<code>L[k, t-1] + log(transition probability from k to j) + log(observation probability of j)</code>
																			</p>

																			<code>L[j, t]</code> should store the maximum such probability, and <code>B[j, t]</code> should store the <code>k</code> that led to this maximum.  The observation probability can be computed from thresh, as explained above
																		</li>
																		<li>
																			Check to see if the unvoiced state <code>-1</code> (the last state index) beats any of the frequency states.  So compute 

																			<p>
																				<code>L[-1, t-1] + log(transition probability from unvoiced to j) + log(observation probability of j)</code>
																			</p>
																			<p>
																				and update <code>L[j, t]</code> and <code>B[j, t]</code> if this is better
																			</p>
																		</li>
																	</ul>
																</p>
															</li>
														</ul>
													</li>
													<li>
														<b>5b.</b> Now, we have to compute <code>L[-1, t]</code> and <code>B[-1, t]</code> for the unvoiced state.  The observation probability for the unvoiced state is
														
														<p>
															<code>1 - the sum of the observation probabilities of all of the <code>j</code>'s in the above step</code>
														</p>
														
														To figure out the best transition, we have to check all possible previous states, including all frequency states and the unvoiced state, and compute 
														<p>
															<code>L[k, t-1] + log(transition probability from k to unvoiced) + log(observation probability of unvoiced)</code>
														</p>
														As before, store the maximum such value in <code>L[-1, t]</code> and the <code>k</code> that achieved this maximum value in <code>B[-1, t]</code>
													</li>
												</ul>
											</li>
											<li>
												Now that <code>L</code> and <code>B</code> are filled in, we can do backtracing to find the best frequency sequence.  Start with the state <code>j</code> that maximizes <code>L</code> in the last column <code>t</code>, then repeatedly do 
												<ul>
													<li>
														<code>j = B[j, t]</code>
													</li>
													<li>
														<code>t = t - 1</code>
													</li>
												</ul>
												To backtrace and collect the sequence of best frequency states <code>j</code>.  As in A* search, you should then reverse this list.  Finally, convert from state index <code>j</code> to the actual frequency number.
											</li>
										</ol>

										
										<p></p>
										<HR>
										<h3><a name = "statement">Task 3: Mandatory Musical Statement (5 Points)</a></h3>
										<p>
											You just implemented a pretty powerful digital music processing tool, so put it to use and do something cool!  Below are a few examples of capabilities built into the assignment that you can play with once pyin is working
										</p>

										<p>
											<b>NOTE: </b> The codebase here is only setup to process .wav files.  If you have a file in another format, you can convert it to wav using <a href = "https://www.audacityteam.org/download/">Audacity</a>.  I'm also happy to do the conversion for you if you're having trouble.  I want to help you with your crazy ideas!
										</p>

										<h4><a name = "fmsynth">FM Synthesis</a></h4>
										<p>
											Pure cosine tones are pretty boring, but there's a surprisingly simple way to spice them up by putting a sine wave inside of them in a process known as <a href = "https://ursinus-cs372-s2023.github.io/Modules/Module6/Video1">FM Synthesis</a>.  I've implemented a simple version of this for you in <code>effects.py</code>.  For example, if you call
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											
											y_fm = fm_synth(x_audio, freqs, frame_length//4, sr, ratio=1, I=8)
											
										</script>

										<p>
											you'll get an "electronic trumpet" sound
										</p>

										<audio controls>
											<source src="Examples/cher_trumpet.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 

										<p>
											and if you call
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[

											y_fm = fm_synth(x_audio, freqs, frame_length//4, sr, ratio=1.4, I=8)
											#
										</script>

										<p>
											you'll get an "electronic bell" sound
										</p>

										<audio controls>
											<source src="Examples/cher_bell.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 

										<p>
											In general, the higher the <b>I</b> parameter is, the "scratchier" the sound is.  If you're going for ordinary "<a href = "https://ursinus-cs372-s2023.github.io/Modules/Module6/Video0">harmonic</a>" instruments, the ratio should be an integer, but for inharmonic instruments like bells, it's not.
										</p>

										<h4><a name = "autotune">Autotuning</a></h4>
										<p>
											Another thing we can do once we have the fundamental frequency is to pitch shift things slightly and round to the nearest "allowed note."  If we do this with <code>cher.wav</code> and the frequency estimates we have, nothing really significant changes (the original purpose of autotune was actually not to be noticed, and Cher is singing pretty on key anyway)
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											allowed_notes = get_scale("G Flat", major=True)
											notes, yauto = autotune(x_audio, freqs, hop_length, allowed_notes)
											save_audio(yauto, sr, "cher_autotune_gflat.wav")
										</script>

										<audio controls>
											<source src="Examples/cher_autotune_gflat.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 
										<p></p>
										<p>
											However, if we "wiggle" the target notes around a little bit around each note transition, we get the effect that Cher pioneered which is now quite common in hip hop 
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											allowed_notes = get_scale("G Flat", major=True)
											notes, yauto = autotune(x_audio, freqs, hop_length, allowed_notes, wiggle_amt=1)
											save_audio(yauto, sr, "cher_autotune_gflat_wiggle.wav")
										</script>

										<audio controls>
											<source src="Examples/cher_autotune_gflat_wiggle.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 
										<p></p>

										<p>
											We can do even weirder stuff, like only letting cher sing a single note the whole time
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											allowed_notes = [440*2**(-3/12)] # Only allowed to sing a G Flat
											notes, yauto = autotune(x_audio, freqs, hop_length, allowed_notes, wiggle_amt=0)
											save_audio(yauto, sr, "cher_autotune_constant_note.wav")
										</script>

										<audio controls>
											<source src="Examples/cher_autotune_constant_note.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 


                                    
										<HR>
											<h4><a name = "embellishments">*My Embellishments (for those curious)</a></h4>
										<p>
											The two main things I did to simplify pYIN for this assignment are:
											<ul>
												<li>
													<p>
														I used a simpler observation probability.  The pYIN paper used the <a href = "https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af">beta distribution</a> instead of my simpler exponential distribution, though I didn't find this to make much of a difference in the examples I looked at, and the simple exponential is much easier to code
													</p>
												</li>
												<li>
													<p>
														I used a simplified state space.  The original pYIN paper actually has an unvoiced state for every single note, for <b>2K</b> states total.  Though I tried this and it does work better, the unvoiced to unvoiced transitions would be difficult for students to implement efficiently without using numba or numpy tricks.
													</p>
													<p>
														With the version in this assignment using only a single unvoiced state, we do take a hit on the ability to determine when things are unvoiced, but it makes it much easier to code up, and it still does a very good job tracking the frequencies. 
													</p>
													<p>
														To make this work properly, I found it was crucial to make the transition probability of staying in the unvoiced state very small (p<SUB>s</SUB>/K<SUP>2</SUP>), and I also had to make the probability of staying in a voiced state very high (0.9999).
													</p>

													<p>
														For those who are curious, here is what the <code>B</code> and <code>L</code> matrices look like for the state space with a voiced/unvoiced state for each frequency:
													</p>
													<img src = "Examples/BLcher_2K.png">
													<p>
														The problem is, they are no longer "sparse"; most of the entries of <code>L</code> in the unvoiced state are filled in, so it takes some tricks to do this efficiently in python.
													</p>
													<p>
														Regardless, here's what the tracking results are like.  It's basically the same as before, except there are slightly more unvoiced gaps.  So this will work better if there are long regions of silence or unreliable frequency estimates
													</p>

													<audio controls>
														<source src="Examples/cher_pyin_2k.mp3" type="audio/mpeg">
													  Your browser does not support the audio element.
													</audio> 
			
													<p>
														<img src = "Examples/cher_greedy_vs_pyin_2K.svg" width="90%">
													</p>

												</li>
											</ul>
										</p>

                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a></li>
												<li><a href = "../../index.html#debugging">Debugging Principles</a></li>
												<li><a href = "../../index.html#schedule">Schedule</a></li>
												<li><a href = "../../index.html#grading">Grading / Deadlines Policy</a></li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "../../../Modules/Module1/Video1">HW0: Python Self Study Module</a>
												</li>
												<li>
													<a href = "../../Assignments/HW1_WelcomeToCS477">HW1: Welcome To CS 477</a>
												</li>
												
												<li>
													<a href = "../../Assignments/HW2_RushHour">HW2: The Rush Hour Problem</a>
												</li>
												<li>
													<a href = "../../Assignments/HW3_Markov">HW3: Markov Chains for Text Processing</a>
												</li>
												<li>
													<a href = "../../Assignments/HW4_FundamentalFreq">HW4: Fundamental Frequency Tracking And Pitch-Based Audio Effects</a>
													<ul>
														<li>
															<a href = "../../Assignments/HW4_FundamentalFreq/statements.html">Musical statements</a>
														</li>
													</ul>
												</li>
												<li>
													<a href = "../../Assignments/HW5_LogisticRegression">HW5: Logistic Regression on Movie Reviews</a>
												</li>
												<!--
												<li>
													<a href = "../../Assignments/HW7_DeepLearning">HW7: (Deep) Neural Networks on Images</a>
												</li>
												!-->
											</ul>
										</li>
                                        <li>
											<span class="opener">Class Exercises / Notes</span>
											<ul>
												<li>
													<a href = "../../ClassExercises/Week1_Bandit/index.html">Week 1: Introduction To Reinforcement Learning</a>
													<ul>
														<li>
															<a href = "../../ClassExercises/Week1_Bandit/jsbandit/index.html">The Multi-Armed Bandit Game</a>
														</li>
													</ul>
												</li>
												<li>
													<a href = "../../ClassExercises/Week1_Adventure">Week 1: Choose Your Own Adventure</a>
													<ul>
														<li><a href = "../../ClassExercises/Week1_Adventure/index.html#student">Student Adventures</a></li>
													</ul>
												</li>
												<li>
													<a href = "../../Materials/MazeExplorer">Week 2: Maze Searching Game</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_BasicSearch">Week 2: Blind Maze Searching</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week2_8Puzzle">Week 2: 8 Puzzle</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week3_PrioritySearch">Week 3: Uniform Cost, Greedy Best-First, and A* Search</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/assignments/186957">Week 3: An Admissible But Not Consistent Heuristic</a>
												</li>
												<li>
													<a href = "../../../Modules/Module2/Video1">Week 4: Probability Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_Markov">Week 4: Markov Chains of Characters</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week4_MarkovText">Week 4: Markov Chains for Document Representations</a>
												</li>
												<li>
													<a href = "https://ursinus.instructure.com/courses/16260/quizzes/24127">Week 4: Bayes Rule Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_NaiveBayes">Week 5: Bayes Rule And Naive Bayes Classifiers</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_BagOfWords">Week 5: Bag of Words Naive Bayes Exercise</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week6_HMM">Week 5/6: Hidden Markov Models / Bayes Filtering / Viterbi Notes</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_RobotLocalization">Week 5/6: Robot Localization</a>
												</li>
												<li>
													<a href = "https://ursinus-cs477-f2023.github.io/Modules/HMM/Video1">Week 6: HMM Module</a>
												</li>
												<li>
													<a href = "https://github.com/ursinus-cs477-f2023/Week6_MDP">Week 6: Markov Decision Processes And Pong AI</a>
												</li>
												<li>
													<a href = "../../../Modules/VectorModule/Video1">Week 7: Euclidean Vectors / Data Vectorization Module</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week7_DigitsNN/index.html">Week 7: K-Nearest Neighbors And Digits Classification</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week5_KMeans">Week 7: KMeans Clustering And Visual Bag of Words</a>
												</li>
												<li>
													<a href = "../../../Modules/MatrixModule/Video1">Week 7: Matrix Module</a>
												</li>
												<li>
													<a href = "ClassExercises/Week7_PCA/Week7_PCA/UnsupervisedDigits.html">Week 7: PCA on MNIST Digits</a>
												</li>
												<li>
													<a href = "../../ClassExercises/Week10_LogisticRegression/index.html">Week 8: Logistic Regression And Gradient Descent</a>
												</li>
												<li>
													<a href = "../../../Modules/LogisticRegression/Video1">Week 8: Neural Networks Module 1</a>
												</li>
												<li>
													<a href = "../../../Modules/Softmax/Video1">Week 9: Softmax Module</a>
												</li>
												<li><a href = "../../../Modules/NeuralNets/Video0">Week 9/10: Multi-Class Logistic Regression And Feedforward Neural Nets Module</a></li>
												
												
											</ul>
										</li>
										<li><a href = "../../Ethics/index.html">Ethics Reading / Discussions</a></li>
										<li><a href = "../../FinalProject/index.html">Final Ethics Project</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
